{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本周只有一个代码实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码课用来加载 预先训练好的模型,你需要只需要修改模型的存放路径即可（第二行代码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T11:49:29.630739Z",
     "start_time": "2020-01-07T11:49:25.927240Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "# from torch.utils.data import DataLoader,Dataset\n",
    "import os\n",
    "if not os.path.exists(\"DialoGPT\"):\n",
    " !git clone https://github.com/microsoft/DialoGPT.git "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步 使用以下链接下载相应预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://convaisharables.blob.core.windows.net/lsp/multiref/small_ft.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T11:49:38.119490Z",
     "start_time": "2020-01-07T11:49:30.457521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "weights = torch.load('small_ft.pkl')\n",
    "medium_config = GPT2Config(n_embd = 768,n_layer = 12, n_head = 12)\n",
    "model = GPT2LMHeadModel(medium_config)\n",
    "\n",
    "weights['lm_head.weight'] = weights['lm_head.decoder.weight']\n",
    "weights.pop('lm_head.decoder.weight',None)\n",
    "\n",
    "model.load_state_dict(weights)\n",
    "# model.train()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你需要写一个推理函数，这个函数接收一个英文句子为输入，输出一个回应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试语句 \n",
    "一轮对话   \n",
    "1. Does money buy happiness ?   \n",
    "2. What is the best way to buy happiness?   \n",
    "\n",
    "一轮对话   \n",
    "1. what is the meaning of a godd life ?   \n",
    "2. How to be a good person ?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T12:09:10.576484Z",
     "start_time": "2020-01-07T12:09:10.570937Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_p_filtering(logits, top_p = 0.9, filter_value = -float('Inf')): # nucleus filtering\n",
    "    assert logits.dim() == 1\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending = True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim = -1), dim = -1)\n",
    "\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    \n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[indices_to_remove] = filter_value\n",
    "    return logits \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T12:09:10.919223Z",
     "start_time": "2020-01-07T12:09:10.914092Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_n_filtering(logits, top_n = 10, filter_value = -float('Inf')):\n",
    "    assert logits.dim() == 1\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending = True)\n",
    "\n",
    "    sorted_indices_to_remove = sorted_indices >= top_n\n",
    "    sorted_indices_to_remove[..., :top_n] = 0\n",
    "    sorted_indices_to_remove[..., top_n:] = 1\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[indices_to_remove] = filter_value\n",
    "    return logits \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T12:10:06.451233Z",
     "start_time": "2020-01-07T12:10:06.445929Z"
    }
   },
   "outputs": [],
   "source": [
    "def recalc():\n",
    "    global conditioned_tokens\n",
    "    global generated_tokens\n",
    "    \n",
    "    indexed_tokens = conditioned_tokens + generated_tokens\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]) # \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    logits = predictions[0, -1, :]\n",
    "\n",
    "    filtered_logits = top_n_filtering(logits)\n",
    "\n",
    "    probabilities = F.softmax(filtered_logits, dim = -1)\n",
    "\n",
    "    next_token = torch.multinomial(probabilities, 1)\n",
    "\n",
    "    generated_tokens.append(next_token.item())\n",
    "    return next_token.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T12:08:53.942781Z",
     "start_time": "2020-01-07T12:08:53.936158Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate():\n",
    "    global conditioned_tokens\n",
    "    global generated_tokens\n",
    "    \n",
    "    if len(tokenizer.decode(conditioned_tokens)) > 320:\n",
    "        dc = tokenizer.decode(conditioned_tokens)\n",
    "        dc = dc[len(dc) - 320:]\n",
    "        idx = dc.find('<|endoftext|>')\n",
    "        if idx != -1:\n",
    "            dc = dc[idx + len('<|endoftext|>'):]\n",
    "            \n",
    "        conditioned_tokens = tokenizer.encode(dc)\n",
    "        \n",
    "    while True:\n",
    "        result = recalc()\n",
    "        \n",
    "        if result == 50256:\n",
    "            # end of text: 50256\n",
    "            \n",
    "            decoded_reply = tokenizer.decode(generated_tokens)\n",
    "            \n",
    "            to_print = decoded_reply\n",
    "            if to_print.endswith('<|endoftext|>'):\n",
    "                to_print = to_print[:-len('<|endoftext|>')]\n",
    "            print(to_print)\n",
    "            \n",
    "            conditioned_tokens += (tokenizer.encode(decoded_reply))\n",
    "            \n",
    "            generated_tokens = []\n",
    "            \n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T13:32:19.084035Z",
     "start_time": "2020-01-07T13:32:19.073921Z"
    }
   },
   "outputs": [],
   "source": [
    "def recalc_beam_search(n_beam = 10):\n",
    "    global conditioned_tokens\n",
    "    global generated_tokens\n",
    "    \n",
    "    indexed_tokens = generated_tokens #conditioned_tokens + generated_tokens\n",
    "\n",
    "    next_token = dict()\n",
    "    for _tokens in indexed_tokens:\n",
    "        tokens_tensor = torch.tensor([_tokens]) # \n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens_tensor)\n",
    "            predictions = outputs[0]\n",
    "\n",
    "        logits = predictions[0, -1, :]\n",
    "    \n",
    "        probabilities = F.softmax(logits, dim = -1)\n",
    "\n",
    "        sorted_probabilities, sorted_indices = torch.sort(probabilities, descending = True)\n",
    "        for i in sorted_indices[:n_beam]: #probabilities.shape[1]:\n",
    "            next_token[tuple(_tokens + [i.item()])] = probabilities[..., i,None]\n",
    "\n",
    "    next_token2 = sorted(next_token.items(), key = lambda x: x[1], reverse=True)\n",
    "    next_token2 = next_token2[:n_beam] \n",
    "\n",
    "    generated_tokens = [list(x[0]) for x in next_token2]\n",
    "    return [x[0][-1] for x in next_token2] # return end of text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T13:33:32.906715Z",
     "start_time": "2020-01-07T13:33:32.899016Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_beam_search():\n",
    "    global conditioned_tokens\n",
    "    global generated_tokens\n",
    "    \n",
    "    if len(tokenizer.decode(conditioned_tokens[0])) > 320:\n",
    "        dc = tokenizer.decode(conditioned_tokens[0])\n",
    "        dc = dc[len(dc) - 320:]\n",
    "        idx = dc.find('<|endoftext|>')\n",
    "        if idx != -1:\n",
    "            dc = dc[idx + len('<|endoftext|>'):]\n",
    "            \n",
    "        conditioned_tokens = [tokenizer.encode(dc)]\n",
    "    \n",
    "    generated_tokens = conditioned_tokens\n",
    "\n",
    "    while True:\n",
    "        result = recalc_beam_search()\n",
    "        if 50256 in result:\n",
    "            # end of text: 50256\n",
    "            \n",
    "            decoded_reply = tokenizer.decode(generated_tokens[0])\n",
    "            \n",
    "            to_print = decoded_reply\n",
    "            if to_print.endswith('<|endoftext|>'):\n",
    "                to_print = to_print[:-len('<|endoftext|>')]\n",
    "            \n",
    "            print_start = to_print.find('<|endoftext|>') + len('<|endoftext|>')\n",
    "            to_print = to_print[print_start:]\n",
    "            print(to_print)\n",
    "            \n",
    "            conditioned_tokens += [tokenizer.encode(to_print)]\n",
    "            \n",
    "            generated_tokens = [] \n",
    "            \n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T13:40:09.801276Z",
     "start_time": "2020-01-07T13:40:07.289924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-sampling:\n",
      "\n",
      "What's your favorite thing about life?\n",
      "#################\n",
      "\n",
      "Beam search:\n",
      "\n",
      "The meaning of life\n",
      "#################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conditioned_tokens = []\n",
    "generated_tokens = []\n",
    "## k-sampling\n",
    "first_text = \"What is the meaning of life?\"\n",
    "conditioned_tokens += tokenizer.encode('\\t' + first_text) + [50256]\n",
    "print('K-sampling:\\n')\n",
    "generate()\n",
    "print('#################\\n')\n",
    "\n",
    "## beam search \n",
    "conditioned_tokens = []\n",
    "generated_tokens = []\n",
    "conditioned_tokens += [tokenizer.encode('\\t' + first_text) + [50256]]\n",
    "print('Beam search:\\n')\n",
    "generate_beam_search()\n",
    "print('#################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T13:41:07.058871Z",
     "start_time": "2020-01-07T13:41:05.042813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-sampling:\n",
      "\n",
      "Money makes you happy\n",
      "#################\n",
      "\n",
      "Beam search:\n",
      "\n",
      "If money buys happiness\n",
      "#################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conditioned_tokens = []\n",
    "generated_tokens = []\n",
    "\n",
    "first_text = \"Does money buy happiness?\"\n",
    "conditioned_tokens += tokenizer.encode('\\t' + first_text) + [50256]\n",
    "print('K-sampling:\\n')\n",
    "generate()\n",
    "print('#################\\n')\n",
    "\n",
    "## beam search \n",
    "conditioned_tokens = []\n",
    "generated_tokens = []\n",
    "conditioned_tokens += [tokenizer.encode('\\t' + first_text) + [50256]]\n",
    "print('Beam search:\\n')\n",
    "generate_beam_search()\n",
    "print('#################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T13:41:56.504441Z",
     "start_time": "2020-01-07T13:41:53.517344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-sampling:\n",
      "\n",
      "So, how are you coping with life in the end, exactly?\n",
      "#################\n",
      "\n",
      "Beam search:\n",
      "\n",
      "What is the meaning\n",
      "#################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conditioned_tokens = []\n",
    "generated_tokens = []\n",
    "\n",
    "first_text = \"what is the meaning of a good life?\"\n",
    "conditioned_tokens += tokenizer.encode('\\t' + first_text) + [50256]\n",
    "print('K-sampling:\\n')\n",
    "generate()\n",
    "print('#################\\n')\n",
    "\n",
    "## beam search \n",
    "conditioned_tokens = []\n",
    "generated_tokens = []\n",
    "conditioned_tokens += [tokenizer.encode('\\t' + first_text) + [50256]]\n",
    "print('Beam search:\\n')\n",
    "generate_beam_search()\n",
    "print('#################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T13:42:41.902086Z",
     "start_time": "2020-01-07T13:42:38.172889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-sampling:\n",
      "\n",
      "how to be a good person\n",
      "#################\n",
      "\n",
      "Beam search:\n",
      "\n",
      "what's the book?\n",
      "#################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conditioned_tokens = []\n",
    "generated_tokens = []\n",
    "\n",
    "first_text = \"How to be a good person?\"\n",
    "conditioned_tokens += tokenizer.encode('\\t' + first_text) + [50256]\n",
    "print('K-sampling:\\n')\n",
    "generate()\n",
    "print('#################\\n')\n",
    "\n",
    "## beam search \n",
    "conditioned_tokens = []\n",
    "generated_tokens = []\n",
    "conditioned_tokens += [tokenizer.encode('\\t' + first_text) + [50256]]\n",
    "print('Beam search:\\n')\n",
    "generate_beam_search()\n",
    "print('#################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
