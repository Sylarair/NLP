{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:56:37.988307Z",
     "start_time": "2019-10-21T04:56:36.431010Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:56:38.318374Z",
     "start_time": "2019-10-21T04:56:37.991479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11792a5f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "random_data = np.random.random((100, 2))\n",
    "X = random_data[:, 0]\n",
    "Y = random_data[:,1]\n",
    "\n",
    "plt.scatter(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27562764, 0.22034597, 0.25514153, 0.69712036, 0.71691096,\n",
       "       0.2810688 , 0.43902753, 0.98358776, 0.95182416, 0.02096077,\n",
       "       0.22886871, 0.72289816, 0.11070981, 0.36337449, 0.17486422,\n",
       "       0.2390066 , 0.70808349, 0.00598323, 0.693187  , 0.56925749,\n",
       "       0.75757204, 0.51099406, 0.83212528, 0.0686582 , 0.1243046 ,\n",
       "       0.59043164, 0.08595381, 0.55634456, 0.26308404, 0.56462231,\n",
       "       0.74954674, 0.80537543, 0.09183093, 0.01076709, 0.15913681,\n",
       "       0.17008466, 0.15995242, 0.97023528, 0.11850541, 0.26097673,\n",
       "       0.24841922, 0.80794452, 0.81857853, 0.87562925, 0.68719228,\n",
       "       0.46276489, 0.44356609, 0.00116953, 0.31659746, 0.7821273 ,\n",
       "       0.15654886, 0.19739148, 0.82443744, 0.6721557 , 0.53465554,\n",
       "       0.44589374, 0.63118401, 0.05397527, 0.07762672, 0.24921999,\n",
       "       0.70430466, 0.69994603, 0.89253373, 0.68577908, 0.76480384,\n",
       "       0.95962383, 0.40662374, 0.41510059, 0.57066511, 0.75194815,\n",
       "       0.5591256 , 0.46661846, 0.80418233, 0.97476963, 0.36217928,\n",
       "       0.72041293, 0.49115869, 0.39218595, 0.45176646, 0.79258619,\n",
       "       0.809126  , 0.19661102, 0.79530497, 0.16061704, 0.74188212,\n",
       "       0.99902041, 0.70749594, 0.35716587, 0.80431329, 0.59112497,\n",
       "       0.6250892 , 0.72740972, 0.1680261 , 0.72214882, 0.44184364,\n",
       "       0.76758418, 0.35365954, 0.29007976, 0.57337602, 0.86321054])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:57:23.174252Z",
     "start_time": "2019-10-21T04:57:22.888018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11ace65f8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = [20 * x + 5 + random.randint(-10, 10) for x in X]\n",
    "\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:56:46.361870Z",
     "start_time": "2019-10-21T04:56:46.353574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40506536, 0.84234079, 0.42946785, 0.74721226, 0.25177723,\n",
       "       0.03210442, 0.62706441, 0.32543574, 0.57257701, 0.64427731])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.reshape(-1, 1)\n",
    "X[:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "$$ y = k * x + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:57:29.565405Z",
     "start_time": "2019-10-21T04:57:29.560876Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(X.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:57:32.028842Z",
     "start_time": "2019-10-21T04:57:32.022263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回归线的k值是: [19.41666004]\n",
      "回归线的截距b是: 4.337110680351273\n",
      "回归线的R^2是: 0.449044748995828\n"
     ]
    }
   ],
   "source": [
    "print('回归线的k值是: {0}'.format(reg.coef_))\n",
    "print('回归线的截距b是: {0}'.format(reg.intercept_))\n",
    "print('回归线的R^2是: {0}'.format(reg.score(X.reshape(-1, 1), y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:57:34.427412Z",
     "start_time": "2019-10-21T04:57:34.424581Z"
    }
   },
   "outputs": [],
   "source": [
    "### function of predicting new data\n",
    "def regression_value(x):\n",
    "    return reg.coef_ * x + reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:57:39.138251Z",
     "start_time": "2019-10-21T04:57:38.829746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1096c7ac8>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, regression_value(X), color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:57:42.464710Z",
     "start_time": "2019-10-21T04:57:42.104531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11aeba860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_random_X(n): # n means the number of randomed data\n",
    "    return [np.random.random((50, 2)) for i in range(n)]\n",
    "\n",
    "new_dataset_X = generate_random_X(100)\n",
    "\n",
    "## plot\n",
    "plt.scatter(new_dataset_X, regression_value(new_dataset_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:57:47.596189Z",
     "start_time": "2019-10-21T04:57:47.592142Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def model(X, y):\n",
    "    return [(Xi, yi) for Xi, yi in zip(X, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:57:48.674550Z",
     "start_time": "2019-10-21T04:57:48.671367Z"
    }
   },
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    return cosine(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:57:50.266872Z",
     "start_time": "2019-10-21T04:57:50.261881Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def predict(x, k=5):\n",
    "    most_similars = sorted(model(X, y), key=lambda xi: distance(xi[0], x))[:k]\n",
    "#     most_similars = sorted(model(X, y), key=lambda xi: xi[0])[:k] # the cos distance between two numbers is 0.\n",
    "    # 已经获得了最相似的数据集\n",
    "#     most_similars.Couner().most_common()\n",
    "#     cosine_distance = sorted([distance(xi[0], x) for xi in model(X, y)])\n",
    "#     return model(most_similars, cosine_distance)\n",
    "    return np.mean([i[1] for i in most_similars]) # get the mean of the most similar sets' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:57:56.394062Z",
     "start_time": "2019-10-21T04:57:56.383849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.176234836823316"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(distance([1,2,3], [4,5,6]))\n",
    "# print(distance([1,2,3], [6,5,9]))\n",
    "# X = np.random.random((50, 2))\n",
    "# Y = np.random.random((25, 1))\n",
    "# model(X, y)\n",
    "# list(zip(X, y))\n",
    "# print(cosine((0.3633744887636341, 11.267489775272683), (0.892533725055378, 30.85067450110756)))\n",
    "# print(cosine((0.892533725055378, 30.85067450110756), (0.892533725055378, 30.85067450110756)))\n",
    "# print(cosine(0.3633744887636341, 0.892533725055378))\n",
    "# print(Counter(X.tolist()).most_common())\n",
    "predict(0.892533725055378)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:58:04.128386Z",
     "start_time": "2019-10-21T04:58:04.123605Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def entropy(inlist):\n",
    "#     frequency_dict = defaultdict(list)\n",
    "#     frequency = []\n",
    "    Entropy = 0\n",
    "    for i in set(inlist):\n",
    "#         print(i)\n",
    "        single_frequency = inlist.count(i) / len(inlist)\n",
    "#         frequency_dict[i] = single_frequency\n",
    "#         print(single_frequency)\n",
    "        Entropy += single_frequency * np.log(single_frequency)\n",
    "        \n",
    "    return -1 * Entropy\n",
    "#     counter = Counter(inlist)\n",
    "#     probs = [counter[c] / len(inlist) for c in set(inlist)]\n",
    "#     return - sum(p * np.log(p) for p in probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T04:58:06.513482Z",
     "start_time": "2019-10-21T04:58:06.508655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5623351446188083"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entropy(range(4))\n",
    "entropy([1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:16:27.336913Z",
     "start_time": "2019-10-21T16:16:27.329476Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# mock_data = {\n",
    "#     'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "#     'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "#     'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "#     'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "# }\n",
    "\n",
    "# mock_data = {\n",
    "#     'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'M'],\n",
    "#     'income': ['+10', '-10', '+10', '+10', '-10', '-10', '-10', '-10', '+10', '+10', '+10', '-10', '+10', '+10','-10','-10'],\n",
    "#     'family_number': [1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2],\n",
    "#     'loan': [0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1],\n",
    "#     'bought': [1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T00:56:01.389488Z",
     "start_time": "2019-10-22T00:56:01.381568Z"
    }
   },
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "\tdataSet = [[-10, 2, 0, 'F', 0],\t\t\t\t\t\t#数据集\n",
    "\t\t\t[-10, 2, 1, 'F', 0],\n",
    "\t\t\t[-10, 2, 1, 'M', 1],\n",
    "\t\t\t[-10, 1, 0, 'M', 1],\n",
    "\t\t\t[-10, 2, 0, 'F', 0],\n",
    "\t\t\t[10, 2, 0, 'F', 0],\n",
    "\t\t\t[10, 2, 1, 'F', 0],\n",
    "\t\t\t[10, 1, 1, 'M', 1],\n",
    "\t\t\t[10, 1, 2, 'F', 1],\n",
    "\t\t\t[10, 1, 2, 'F', 1],\n",
    "\t\t\t[20, 1, 2, 'F', 1],\n",
    "\t\t\t[20, 1, 1, 'F', 1],\n",
    "\t\t\t[20, 2, 1, 'M', 1],\n",
    "\t\t\t[20, 2, 2, 'M', 1],\n",
    "\t\t\t[20, 2, 0, 'F', 0]]\n",
    "\tlabels = ['income', 'family_number', 'loan', 'gender', 'bought']\t\t#特征标签\n",
    "\treturn dataSet \t\t\t\t\t\t\t#返回数据集和分类属性\n",
    "\n",
    "mock_data = createDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T00:56:02.834634Z",
     "start_time": "2019-10-22T00:56:02.821795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>loan</th>\n",
       "      <th>gender</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    income  family_number  loan gender  bought\n",
       "0      -10              2     0      F       0\n",
       "1      -10              2     1      F       0\n",
       "2      -10              2     1      M       1\n",
       "3      -10              1     0      M       1\n",
       "4      -10              2     0      F       0\n",
       "5       10              2     0      F       0\n",
       "6       10              2     1      F       0\n",
       "7       10              1     1      M       1\n",
       "8       10              1     2      F       1\n",
       "9       10              1     2      F       1\n",
       "10      20              1     2      F       1\n",
       "11      20              1     1      F       1\n",
       "12      20              2     1      M       1\n",
       "13      20              2     2      M       1\n",
       "14      20              2     0      F       0"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame.from_dict(mock_data)\n",
    "dataset.columns = ['income', 'family_number', 'loan', 'gender', 'bought']\n",
    "type(dataset)\n",
    "# dataset.values.tolist()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:19:03.335341Z",
     "start_time": "2019-10-21T16:19:03.329667Z"
    }
   },
   "outputs": [],
   "source": [
    "family_number_set1 = dataset[dataset['family_number']==1]\n",
    "family_number_set2 = dataset[dataset['family_number']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:19:04.444962Z",
     "start_time": "2019-10-21T16:19:04.439423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3862943611198906"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(family_number_set1['bought'].tolist()) + entropy(family_number_set2['bought'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:19:05.797448Z",
     "start_time": "2019-10-21T16:19:05.789596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.369869681297795"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_set1 = dataset[dataset['gender']=='F']\n",
    "gender_set2 = dataset[dataset['gender']!='F']\n",
    "entropy(gender_set1['bought'].tolist()) + entropy(gender_set2['bought'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T00:42:55.025967Z",
     "start_time": "2019-10-22T00:42:53.768889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 2)\n",
      "the min entropy is: 0.6365141682948128\n",
      "{'income', 'loan', 'family_number', 'gender'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_feature_selection(training_data, target):\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    # Warning! the set is unordered, so using set to remove 'bought' would change the order of colnames\n",
    "    spliter = None\n",
    "    min_entropy = float('inf') # set initialized minimal entropy as Inf, then adjust it in the following steps.\n",
    "    \n",
    "    for f in x_fields:\n",
    "#         ic(f)\n",
    "        values = set(training_data[f])\n",
    "#         ic(values)\n",
    "        for v in values:\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "\n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "\n",
    "            entropy_sum = entropy_1 + entropy_2\n",
    "#             ic(entropy_sum)\n",
    "            \n",
    "            if entropy_sum <= min_entropy:\n",
    "                min_entropy = entropy_sum\n",
    "                spliter = (f, v)\n",
    "    \n",
    "    print('spliter is: {}'.format(spliter))\n",
    "    print('the min entropy is: {}'.format(min_entropy))\n",
    "    print(x_fields)\n",
    "    return list(x_fields).index(spliter[0]) #+ (min_entropy)\n",
    " \n",
    "best_feature_selection(dataset, target = 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T15:36:14.863101Z",
     "start_time": "2019-10-21T15:36:14.857871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# int(dataset.loc[1, 'bought'])\n",
    "type(list(dataset))\n",
    "# list(dataset)\n",
    "# dataset\n",
    "# dataset.shape\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T13:33:55.337042Z",
     "start_time": "2019-10-21T13:33:55.329562Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_most_label(classList):\n",
    "    classDict = Counter(classList).most_common()\n",
    "    return list(classDict)[0][0] # retrieve the most frequent element\n",
    "# get_most_label([1, 1, 1, 2, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:09:49.846990Z",
     "start_time": "2019-10-21T16:09:49.841827Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(dataframe, feature, _value):\n",
    "    dataframe2 = dataframe[dataframe[feature]==_value]\n",
    "    dataframe3 = dataframe2.drop(feature, 1)\n",
    "    \n",
    "    return dataframe3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T15:56:20.889858Z",
     "start_time": "2019-10-21T15:56:20.879861Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_most_choice_in_training_set(training_data, target):\n",
    "    return list(Counter(training_data[target].values.tolist()).most_common())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T01:00:36.659226Z",
     "start_time": "2019-10-22T01:00:36.649266Z"
    }
   },
   "outputs": [],
   "source": [
    "def createDataSet2():\n",
    "\tdataSet = [[0, 0, 0, 0, 'no'],\t\t\t\t\t\t#数据集\n",
    "\t\t\t[0, 0, 0, 1, 'no'],\n",
    "\t\t\t[0, 1, 0, 1, 'yes'],\n",
    "\t\t\t[0, 1, 1, 0, 'yes'],\n",
    "\t\t\t[0, 0, 0, 0, 'no'],\n",
    "\t\t\t[1, 0, 0, 0, 'no'],\n",
    "\t\t\t[1, 0, 0, 1, 'no'],\n",
    "\t\t\t[1, 1, 1, 1, 'yes'],\n",
    "\t\t\t[1, 0, 1, 2, 'yes'],\n",
    "\t\t\t[1, 0, 1, 2, 'yes'],\n",
    "\t\t\t[2, 0, 1, 2, 'yes'],\n",
    "\t\t\t[2, 0, 1, 1, 'yes'],\n",
    "\t\t\t[2, 1, 0, 1, 'yes'],\n",
    "\t\t\t[2, 1, 0, 2, 'yes'],\n",
    "\t\t\t[2, 0, 0, 0, 'no']]\n",
    "\tlabels = ['年龄', '有工作', '有自己的房子', '信贷情况']\t\t#特征标签\n",
    "\treturn dataSet, labels \t\t\t\t\t\t\t#返回数据集和分类属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-22T01:05:08.668Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 2)\n",
      "the min entropy is: 0.6365141682948128\n",
      "{'income', 'loan', 'family_number', 'gender'}\n",
      "2\n",
      "    income  loan  family_number gender  bought\n",
      "0      -10     0              2      F       0\n",
      "1      -10     1              2      F       0\n",
      "2      -10     1              2      M       1\n",
      "3      -10     0              1      M       1\n",
      "4      -10     0              2      F       0\n",
      "5       10     0              2      F       0\n",
      "6       10     1              2      F       0\n",
      "7       10     1              1      M       1\n",
      "8       10     2              1      F       1\n",
      "9       10     2              1      F       1\n",
      "10      20     2              1      F       1\n",
      "11      20     1              1      F       1\n",
      "12      20     1              2      M       1\n",
      "13      20     2              2      M       1\n",
      "14      20     0              2      F       0\n",
      "spliter is: ('gender', 'F')\n",
      "the min entropy is: -0.0\n",
      "{'income', 'loan', 'gender'}\n",
      "2\n",
      "    income  loan gender  bought\n",
      "0      -10     0      F       0\n",
      "1      -10     1      F       0\n",
      "2      -10     1      M       1\n",
      "4      -10     0      F       0\n",
      "5       10     0      F       0\n",
      "6       10     1      F       0\n",
      "12      20     1      M       1\n",
      "13      20     2      M       1\n",
      "14      20     0      F       0\n"
     ]
    }
   ],
   "source": [
    "def continous_best_feature_selection(training_data, target, labels, featLabels):\n",
    "#     x_fields = set(training_data.columns.tolist()) - {target}\n",
    "\n",
    "#     step = 1\n",
    "    feature_chosen_list = []\n",
    "    feature_entropy_dict = {}\n",
    "    \n",
    "    ClassList = training_data.loc[:, 'bought'].values.tolist()\n",
    "#     return ClassList\n",
    "#     print(ClassList)\n",
    "#     print(labels)\n",
    "    if len(ClassList) == 0:\n",
    "        return get_most_choice_in_training_set(dataset, target)\n",
    "    elif len(ClassList) == ClassList.count(ClassList[0]):\n",
    "        return ClassList[0]\n",
    "    elif len(labels) == 0 or training_data.shape[1] == 1:\n",
    "        return get_most_label(ClassList)\n",
    "    \n",
    "    best_feature_index = best_feature_selection(training_data, target)\n",
    "    best_feature = labels[best_feature_index]\n",
    "    \n",
    "    featLabels.append(best_feature)\n",
    "    myTree = {best_feature:{}}\n",
    "    del labels[best_feature_index]\n",
    "    print(best_feature_index)\n",
    "    print(training_data)\n",
    "    unique_feature_values = set([i[best_feature_index] for i in training_data.values.tolist()])\n",
    "    for value in unique_feature_values:\n",
    "            subLabels = labels[:]\n",
    "#             print(best_feature, value)\n",
    "#             print(split_data(training_data, best_feature, value))\n",
    "            myTree[best_feature][value] = continous_best_feature_selection(split_data(training_data, best_feature, value), target, subLabels, featLabels)\n",
    "    return myTree\n",
    "\n",
    "# print(dataset)\n",
    "Labels = list(set(dataset.columns.tolist()) - {'bought'}) #[i for i in dataset.columns.tolist() if i != 'bought'] #\n",
    "# print(Labels)\n",
    "dataset2 = dataset.loc[:, Labels + ['bought']]\n",
    "# print(dataset2)\n",
    "Tree_model = continous_best_feature_selection(dataset2, 'bought', Labels, [])\n",
    "# dataset\n",
    "\n",
    "# # Another dataSet\n",
    "# dataSet, labels = createDataSet2()\n",
    "# dataSet = pd.DataFrame(dataSet)\n",
    "# dataSet.columns = labels + ['bought']\n",
    "# # print(dataSet)\n",
    "# continous_best_feature_selection(dataSet, 'bought', labels, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def continous_optimal_feature_selection(training_data, target):\n",
    "#     x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    \n",
    "#     training_data1 = training_data\n",
    "#     training_data2 = training_data\n",
    "#     ## 1st step\n",
    "#     best_feature = best_feature_selection(training_data, target)[0] #('family_number', 2)\n",
    "#     cutting_value = best_feature_selection(training_data, target)[1]\n",
    "#     first_step_entropy = best_feature_selection(training_data, target)[2]\n",
    "    \n",
    "#     for i in range(1, len(x_fields)):        \n",
    "#         training_data1 = training_data1[training_data[best_feature]==cutting_value]\n",
    "#         training_data2 = training_data2[training_data[best_feature]!=cutting_value]\n",
    "#         training_data1 = training_data1.drop(best_feature, 1) # remove used feature column\n",
    "#         training_data2 = training_data2.drop(best_feature, 1) # remove used feature column\n",
    "        \n",
    "#         best_feature_list1 = best_feature_selection(training_data1)\n",
    "#         best_feature_list2 = best_feature_selection(training_data1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T05:20:42.080100Z",
     "start_time": "2019-10-21T05:20:42.077068Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T05:20:47.372732Z",
     "start_time": "2019-10-21T05:20:47.368824Z"
    }
   },
   "outputs": [],
   "source": [
    "X = [random.randint(0, 100) for i in range(100)]\n",
    "Y = [random.randint(0, 100) for i in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T05:21:02.543054Z",
     "start_time": "2019-10-21T05:21:02.297781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11ffd8550>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T05:09:40.466391Z",
     "start_time": "2019-10-21T05:09:40.462964Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data = [[i, j] for i, j in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T05:10:40.887167Z",
     "start_time": "2019-10-21T05:10:40.883378Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T05:11:30.460757Z",
     "start_time": "2019-10-21T05:11:30.399975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "       n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T05:11:47.444086Z",
     "start_time": "2019-10-21T05:11:47.439095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[80.82352941, 15.76470588],\n",
       "       [24.39130435, 61.39130435],\n",
       "       [76.14285714, 76.14285714],\n",
       "       [33.85185185, 23.2962963 ],\n",
       "       [17.25      , 91.83333333]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T05:30:42.958184Z",
     "start_time": "2019-10-21T05:30:42.952732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 4, 3, 2, 0, 3, 3, 1, 3, 2, 0, 2, 4, 1, 1, 0, 3, 2, 0, 3,\n",
       "       3, 3, 2, 4, 0, 1, 3, 3, 3, 1, 0, 0, 0, 0, 3, 4, 1, 1, 2, 3, 0, 3,\n",
       "       1, 4, 3, 1, 1, 3, 2, 2, 2, 1, 1, 1, 0, 2, 1, 2, 4, 2, 1, 3, 4, 4,\n",
       "       1, 1, 1, 2, 3, 3, 3, 2, 0, 1, 2, 4, 2, 3, 0, 2, 0, 3, 3, 3, 2, 4,\n",
       "       2, 1, 1, 3, 0, 2, 4, 3, 4, 2, 1, 3], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T05:37:08.925431Z",
     "start_time": "2019-10-21T05:37:08.261877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "cluster_label = defaultdict(list)\n",
    "\n",
    "for label, location in zip(cluster.labels_, training_data):\n",
    "    cluster_label[label].append(location)\n",
    "\n",
    "### plot\n",
    "color = ['red', 'green', 'grey', 'black', 'yellow', 'orange']\n",
    "for label, center in enumerate(cluster_label):\n",
    "#     print(label)\n",
    "    for location in cluster_label[center]:\n",
    "#         print(location)\n",
    "        plt.scatter(*location, c = color[label])\n",
    "\n",
    "for i in range(len(cluster.cluster_centers_)):\n",
    "    location = cluster.cluster_centers_[i]\n",
    "    plt.scatter(*location, c = 'purple', s=100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: $$ Precision = \\frac{TP}{TP + FP} $$\n",
    "     $$ Recall = \\frac{TP}{TP+FN} $$\n",
    "     $$ F1-score = \\frac{2*Precision*Recall}{Precision + Recall} $$\n",
    "     $$ F2-score = \\frac{5*Precision*Recall}{4*Precision + Recall} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "   1. **Precision** means how accurate a machine learning model is, namely the true positive divided by all predicted positive samples number.\n",
    "     \n",
    "   2. **Recall** measures the fraction of true positive that are successfully retrieved. Also, it can be understood that the threshold that seperates the true positive and false negative.\n",
    "     \n",
    "   3. **AUC**: Area under curve. This term means the performance of a model.\n",
    "     \n",
    "   4. **F1-score**: Because the precision competes with recall, and it is often hard to measure which model is better just by precision or recall. To balance them, a F1-score is used to give precision and recall same weight in some conditions, so we can judge which model is better.\n",
    "     \n",
    "   5. **F2-score**: Sometimes, the weight of precision and recall is different for we would prefer someone. For example, we prefer recall rather than precision in the epidemic disease detection, while precision is more important than recall in the court trial. Under this circumstance, we add a weight parameter \\beta to adjust the importance of precision and recall. When more importance is on precision, the \\beta would be less than 1, otherwise, it is greater than 1. It is **F2-Score when the \\beta is 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ F-score = (1 + \\beta^2) * \\frac{Precision*Recall}{\\beta^2*Precision + Recall} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: **Machine learning** is the scientific study that computers use to perform tasks under algorithms and statistical models without using clear instructions. It can undermine underlying mechanisms of data, which can be then used to predict new results with other new data. Without doubt, it is a part of Artificial Intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: I think it's correct. Through a universal and correct evaluation system and method, we know what a model needs, and we can know a model's performance including precision, recall, etc, under different conditions. From that, the problem that which model is better can be solved.\n",
    "\n",
    "Also, we can understand the advantages and disadvantages, which provides basis of choice when solving problems and should be the direction that we try to move forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'family_number': {1: 1, 2: {'gender': {'M': 1, 'F': 0}}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(DT_model, feature_labels, test_var):\n",
    "    firstStr = next(iter(DT_model))\n",
    "    secondDict = DT_model[firstStr]\n",
    "    \n",
    "    feature_label_index = feature_labels.index(firstStr)\n",
    "    for _key in secondDict.keys():\n",
    "        if test_var[feature_label_index] == _key:\n",
    "            if type(secondDict[_key]).__name__ == 'dict':\n",
    "                predict_result = predict(secondDict[_key], feature_labels, test_var)\n",
    "            else:\n",
    "                predict_result = secondDict[_key]\n",
    "    return str(predict_result) #'feature list is ' + ' '.join(np.array(test_var, dtype=str).tolist()) + '. The predicted result is: ' + str(predict_result)\n",
    "\n",
    "# dataset2\n",
    "# income  loan  family_number gender  bought\n",
    "# -10     0              2      F       0\n",
    "# -10     1              2      F       0\n",
    "# -10     1              2      M       1\n",
    "# -10     0              1      M       1\n",
    "# -10     0              2      F       0\n",
    "# 10     0              2      F       0\n",
    "# 10     1              2      F       0\n",
    "# 10     1              1      M       1\n",
    "# 10     2              1      F       1\n",
    "# 10     2              1      F       1\n",
    "# 20     2              1      F       1\n",
    "# 20     1              1      F       1\n",
    "# 20     1              2      M       1\n",
    "# 20     2              2      M       1\n",
    "# 20     0              2      F       0\n",
    "# 20     2      M       1\n",
    "\n",
    "feature_labels = ['income', 'loan', 'family_number', 'gender']\n",
    "print(Tree_model)\n",
    "predict(Tree_model, feature_labels, [10, 0, 1, 'M'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10 0 1 M. Predicted result:1\n",
      "-10 0 1 F. Predicted result:1\n",
      "-10 0 2 M. Predicted result:1\n",
      "-10 0 2 F. Predicted result:0\n",
      "-10 1 1 M. Predicted result:1\n",
      "-10 1 1 F. Predicted result:1\n",
      "-10 1 2 M. Predicted result:1\n",
      "-10 1 2 F. Predicted result:0\n",
      "-10 2 1 M. Predicted result:1\n",
      "-10 2 1 F. Predicted result:1\n",
      "-10 2 2 M. Predicted result:1\n",
      "-10 2 2 F. Predicted result:0\n",
      "10 0 1 M. Predicted result:1\n",
      "10 0 1 F. Predicted result:1\n",
      "10 0 2 M. Predicted result:1\n",
      "10 0 2 F. Predicted result:0\n",
      "10 1 1 M. Predicted result:1\n",
      "10 1 1 F. Predicted result:1\n",
      "10 1 2 M. Predicted result:1\n",
      "10 1 2 F. Predicted result:0\n",
      "10 2 1 M. Predicted result:1\n",
      "10 2 1 F. Predicted result:1\n",
      "10 2 2 M. Predicted result:1\n",
      "10 2 2 F. Predicted result:0\n",
      "20 0 1 M. Predicted result:1\n",
      "20 0 1 F. Predicted result:1\n",
      "20 0 2 M. Predicted result:1\n",
      "20 0 2 F. Predicted result:0\n",
      "20 1 1 M. Predicted result:1\n",
      "20 1 1 F. Predicted result:1\n",
      "20 1 2 M. Predicted result:1\n",
      "20 1 2 F. Predicted result:0\n",
      "20 2 1 M. Predicted result:1\n",
      "20 2 1 F. Predicted result:1\n",
      "20 2 2 M. Predicted result:1\n",
      "20 2 2 F. Predicted result:0\n"
     ]
    }
   ],
   "source": [
    "income_list = [-10, 10, 20]\n",
    "loan_list = [0, 1, 2]\n",
    "family_number_list = [1, 2]\n",
    "gender_list = ['M', 'F']\n",
    "\n",
    "generated_testset_list = [[i, j, k, m] for i in income_list for j in loan_list for k in family_number_list for m in gender_list]\n",
    "\n",
    "for _item in generated_testset_list:\n",
    "    print(' '.join(np.array(_item, dtype=str).tolist()) + '. Predicted result:' + predict(Tree_model, feature_labels, _item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:36:11.217196Z",
     "start_time": "2019-10-21T06:36:11.211205Z"
    }
   },
   "source": [
    "#### Loss fucntion (L1 norm)\n",
    "$$ loss = \\frac{1}{n} \\sum{\\lvert{(y_i - \\hat{y_i})}\\rvert} $$\n",
    "$$ loss = \\frac{1}{n} \\sum{\\lvert{(y_i - (k*x_i + b_i))}\\rvert} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:53:01.378181Z",
     "start_time": "2019-10-21T06:53:01.374535Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    return sum(np.abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y), list(y_hat))) / len(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:44:26.347721Z",
     "start_time": "2019-10-21T06:44:26.341509Z"
    }
   },
   "source": [
    "#### Define partial derivative\n",
    "$$ if ({y_i - \\hat{y_i}}\\neq{0}) $$\n",
    "$$ \\frac{\\partial{loss}}{\\partial{k}} = -\\frac{2}{n} \\sum(\\frac{y_i - \\hat{y_i}}{|y_i - \\hat{y_i}|})x_i $$\n",
    "$$ \\frac{\\partial{loss}}{\\partial{b}} = -\\frac{2}{n} \\sum(\\frac{y_i - \\hat{y_i}}{|y_i - \\hat{y_i}|}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ if (y_i - \\hat{y_i} = 0), suppose: $$\n",
    "$$ \\frac{\\partial{loss}}{\\partial{k}} = 0 $$\n",
    "$$ \\frac{\\partial{loss}}{\\partial{b}} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:53:03.682940Z",
     "start_time": "2019-10-21T06:53:03.674185Z"
    }
   },
   "outputs": [],
   "source": [
    "def partial_derivative_for_k(x, y, y_hat):\n",
    "    partial_derivative_k = 0\n",
    "    \n",
    "    for i in range(len(list(y))):\n",
    "        if list(y)[i] - list(y_hat)[i] != 0:\n",
    "            partial_derivative_k += (list(y)[i] - list(y_hat)[i]) * list(x)[i]\n",
    "        else:\n",
    "            partial_derivative_k += 0 # when it's not derivatived in 0, so remain the accumulatid partial derivative value\n",
    "        \n",
    "    return -2 * partial_derivative_k / len(y)\n",
    "\n",
    "def partial_derivative_for_b(y, y_hat):\n",
    "    partial_derivative_b = 0\n",
    "    \n",
    "    for i in range(len(list(y))):\n",
    "        if list(y)[i] - list(y_hat)[i] != 0:\n",
    "            partial_derivative_b += (list(y)[i] - list(y_hat)[i])\n",
    "        else:\n",
    "            partial_derivative_b += 0#  when it's not derivatived in 0, so remain the accumulatid partial derivative value\n",
    "        \n",
    "    return -2 * partial_derivative_b / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:53:10.092762Z",
     "start_time": "2019-10-21T06:53:10.089493Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_function(k, b, x):\n",
    "    return k * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:56:57.418043Z",
     "start_time": "2019-10-21T06:56:57.413973Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:54:47.478237Z",
     "start_time": "2019-10-21T06:54:47.459684Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_boston()\n",
    "x, y = dataset['data'], dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:56:58.152850Z",
     "start_time": "2019-10-21T06:56:57.915921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1229a3908>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x[:, 5], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Startting iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:56:57.411249Z",
     "start_time": "2019-10-21T06:54:51.898071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "This is the 1 time's iteration. The loss is 587.9728213298042. The k is -77.39581548329122, and b is -79.03561158226839\n",
      "This is the 2 time's iteration. The loss is 106.39781312911892. The k is -2.6395889763646636, and b is -67.2761551556723\n",
      "This is the 3 time's iteration. The loss is 19.5338939884237. The k is 10.849539694499596, and b is -65.14819889308993\n",
      "This is the 4 time's iteration. The loss is 6.117463692584493. The k is 13.282776923824487, and b is -64.75828659376572\n",
      "This is the 5 time's iteration. The loss is 5.042671964948471. The k is 13.720934760473847, and b is -64.682012667708\n",
      "This is the 6 time's iteration. The loss is 4.974598554821456. The k is 13.799072803246284, and b is -64.6623374563173\n",
      "This is the 7 time's iteration. The loss is 4.966610204292151. The k is 13.812244064698174, and b is -64.65287712976578\n",
      "This is the 8 time's iteration. The loss is 4.96502311654154. The k is 13.813692035287328, and b is -64.6452615409982\n",
      "This is the 9 time's iteration. The loss is 4.964530395252399. The k is 13.813024714737525, and b is -64.63798026332111\n",
      "This is the 10 time's iteration. The loss is 4.96422350545196. The k is 13.811975904787806, and b is -64.63076073388406\n",
      "This is the 11 time's iteration. The loss is 4.963950202100565. The k is 13.810858476809312, and b is -64.62355376729424\n",
      "This is the 12 time's iteration. The loss is 4.963683011962238. The k is 13.809728889632595, and b is -64.61635048751023\n",
      "This is the 13 time's iteration. The loss is 4.963416977369577. The k is 13.80859733142468, and b is -64.60914929247362\n",
      "This is the 14 time's iteration. The loss is 4.963151203679177. The k is 13.807465640627198, and b is -64.60194989274524\n",
      "This is the 15 time's iteration. The loss is 4.962885529436485. The k is 13.80633414894061, and b is -64.5947522357534\n",
      "This is the 16 time's iteration. The loss is 4.9626199254937715. The k is 13.805202916167955, and b is -64.58755631167016\n",
      "This is the 17 time's iteration. The loss is 4.962354386578645. The k is 13.804071953047302, and b is -64.5803621183809\n",
      "This is the 18 time's iteration. The loss is 4.96208891172708. The k is 13.802941261462774, and b is -64.57316965516306\n",
      "This is the 19 time's iteration. The loss is 4.9618235007525335. The k is 13.80181084170077, and b is -64.5659789215453\n",
      "This is the 20 time's iteration. The loss is 4.961558153608766. The k is 13.80068069375939, and b is -64.55878991710172\n",
      "This is the 21 time's iteration. The loss is 4.961292870274855. The k is 13.799550817584722, and b is -64.55160264141472\n",
      "This is the 22 time's iteration. The loss is 4.961027650734451. The k is 13.798421213113482, and b is -64.54441709406824\n",
      "This is the 23 time's iteration. The loss is 4.960762494972033. The k is 13.797291880280703, and b is -64.53723327464661\n",
      "This is the 24 time's iteration. The loss is 4.960497402972222. The k is 13.79616281902113, and b is -64.5300511827343\n",
      "This is the 25 time's iteration. The loss is 4.960232374719683. The k is 13.795034029269473, and b is -64.52287081791589\n",
      "This is the 26 time's iteration. The loss is 4.959967410199094. The k is 13.793905510960442, and b is -64.51569217977605\n",
      "This is the 27 time's iteration. The loss is 4.959702509395117. The k is 13.792777264028764, and b is -64.50851526789958\n",
      "This is the 28 time's iteration. The loss is 4.959437672292447. The k is 13.791649288409179, and b is -64.50134008187136\n",
      "This is the 29 time's iteration. The loss is 4.959172898875744. The k is 13.790521584036444, and b is -64.49416662127636\n",
      "This is the 30 time's iteration. The loss is 4.958908189129712. The k is 13.789394150845332, and b is -64.48699488569967\n",
      "This is the 31 time's iteration. The loss is 4.958643543039024. The k is 13.788266988770632, and b is -64.47982487472646\n",
      "This is the 32 time's iteration. The loss is 4.958378960588389. The k is 13.787140097747146, and b is -64.47265658794203\n",
      "This is the 33 time's iteration. The loss is 4.958114441762489. The k is 13.786013477709696, and b is -64.46549002493174\n",
      "This is the 34 time's iteration. The loss is 4.957849986546026. The k is 13.784887128593118, and b is -64.45832518528108\n",
      "This is the 35 time's iteration. The loss is 4.957585594923714. The k is 13.783761050332261, and b is -64.45116206857563\n",
      "This is the 36 time's iteration. The loss is 4.957321266880252. The k is 13.782635242861991, and b is -64.44400067440107\n",
      "This is the 37 time's iteration. The loss is 4.957057002400357. The k is 13.781509706117193, and b is -64.43684100234317\n",
      "This is the 38 time's iteration. The loss is 4.956792801468739. The k is 13.780384440032762, and b is -64.42968305198782\n",
      "This is the 39 time's iteration. The loss is 4.956528664070111. The k is 13.779259444543614, and b is -64.422526822921\n",
      "This is the 40 time's iteration. The loss is 4.956264590189203. The k is 13.77813471958468, and b is -64.41537231472878\n",
      "This is the 41 time's iteration. The loss is 4.956000579810738. The k is 13.777010265090901, and b is -64.40821952699734\n",
      "This is the 42 time's iteration. The loss is 4.9557366329194465. The k is 13.77588608099724, and b is -64.40106845931295\n",
      "This is the 43 time's iteration. The loss is 4.955472749500066. The k is 13.774762167238672, and b is -64.393919111262\n",
      "This is the 44 time's iteration. The loss is 4.955208929537333. The k is 13.773638523750188, and b is -64.38677148243094\n",
      "This is the 45 time's iteration. The loss is 4.954945173015966. The k is 13.772515150466798, and b is -64.37962557240638\n",
      "This is the 46 time's iteration. The loss is 4.954681479920744. The k is 13.771392047323525, and b is -64.37248138077499\n",
      "This is the 47 time's iteration. The loss is 4.954417850236386. The k is 13.770269214255407, and b is -64.36533890712352\n",
      "This is the 48 time's iteration. The loss is 4.954154283947659. The k is 13.769146651197499, and b is -64.35819815103885\n",
      "This is the 49 time's iteration. The loss is 4.953890781039308. The k is 13.768024358084869, and b is -64.35105911210796\n",
      "This is the 50 time's iteration. The loss is 4.953627341496118. The k is 13.766902334852606, and b is -64.34392178991793\n",
      "This is the 51 time's iteration. The loss is 4.9533639653028185. The k is 13.765780581435807, and b is -64.33678618405592\n",
      "This is the 52 time's iteration. The loss is 4.953100652444182. The k is 13.764659097769595, and b is -64.32965229410921\n",
      "This is the 53 time's iteration. The loss is 4.952837402904986. The k is 13.763537883789096, and b is -64.32252011966516\n",
      "This is the 54 time's iteration. The loss is 4.9525742166700075. The k is 13.762416939429462, and b is -64.31538966031124\n",
      "This is the 55 time's iteration. The loss is 4.952311093724019. The k is 13.761296264625855, and b is -64.30826091563502\n",
      "This is the 56 time's iteration. The loss is 4.952048034051801. The k is 13.760175859313454, and b is -64.30113388522417\n",
      "This is the 57 time's iteration. The loss is 4.951785037638138. The k is 13.759055723427457, and b is -64.29400856866646\n",
      "This is the 58 time's iteration. The loss is 4.951522104467819. The k is 13.75793585690307, and b is -64.28688496554975\n",
      "This is the 59 time's iteration. The loss is 4.951259234525631. The k is 13.75681625967552, and b is -64.279763075462\n",
      "This is the 60 time's iteration. The loss is 4.95099642779638. The k is 13.755696931680053, and b is -64.27264289799129\n",
      "This is the 61 time's iteration. The loss is 4.950733684264843. The k is 13.754577872851922, and b is -64.26552443272577\n",
      "This is the 62 time's iteration. The loss is 4.950471003915857. The k is 13.7534590831264, and b is -64.25840767925371\n",
      "This is the 63 time's iteration. The loss is 4.950208386734197. The k is 13.752340562438775, and b is -64.25129263716346\n",
      "This is the 64 time's iteration. The loss is 4.949945832704686. The k is 13.751222310724351, and b is -64.24417930604349\n",
      "This is the 65 time's iteration. The loss is 4.949683341812141. The k is 13.75010432791845, and b is -64.23706768548236\n",
      "This is the 66 time's iteration. The loss is 4.949420914041366. The k is 13.748986613956404, and b is -64.22995777506871\n",
      "This is the 67 time's iteration. The loss is 4.949158549377197. The k is 13.747869168773565, and b is -64.22284957439132\n",
      "This is the 68 time's iteration. The loss is 4.948896247804456. The k is 13.746751992305297, and b is -64.21574308303903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 69 time's iteration. The loss is 4.948634009307963. The k is 13.745635084486983, and b is -64.20863830060081\n",
      "This is the 70 time's iteration. The loss is 4.948371833872563. The k is 13.744518445254021, and b is -64.2015352266657\n",
      "This is the 71 time's iteration. The loss is 4.94810972148307. The k is 13.743402074541823, and b is -64.19443386082285\n",
      "This is the 72 time's iteration. The loss is 4.94784767212435. The k is 13.742285972285817, and b is -64.18733420266153\n",
      "This is the 73 time's iteration. The loss is 4.94758568578122. The k is 13.741170138421447, and b is -64.18023625177108\n",
      "This is the 74 time's iteration. The loss is 4.947323762438552. The k is 13.740054572884173, and b is -64.17314000774094\n",
      "This is the 75 time's iteration. The loss is 4.947061902081176. The k is 13.738939275609468, and b is -64.16604547016067\n",
      "This is the 76 time's iteration. The loss is 4.9468001046939545. The k is 13.737824246532822, and b is -64.15895263861991\n",
      "This is the 77 time's iteration. The loss is 4.946538370261736. The k is 13.736709485589744, and b is -64.15186151270841\n",
      "This is the 78 time's iteration. The loss is 4.946276698769393. The k is 13.735594992715754, and b is -64.144772092016\n",
      "This is the 79 time's iteration. The loss is 4.946015090201789. The k is 13.734480767846387, and b is -64.13768437613263\n",
      "This is the 80 time's iteration. The loss is 4.945753544543786. The k is 13.733366810917195, and b is -64.13059836464834\n",
      "This is the 81 time's iteration. The loss is 4.945492061780261. The k is 13.732253121863748, and b is -64.12351405715327\n",
      "This is the 82 time's iteration. The loss is 4.945230641896096. The k is 13.731139700621627, and b is -64.11643145323767\n",
      "This is the 83 time's iteration. The loss is 4.9449692848761515. The k is 13.730026547126434, and b is -64.10935055249186\n",
      "This is the 84 time's iteration. The loss is 4.9447079907053215. The k is 13.728913661313783, and b is -64.10227135450629\n",
      "This is the 85 time's iteration. The loss is 4.9444467593685015. The k is 13.727801043119301, and b is -64.09519385887147\n",
      "This is the 86 time's iteration. The loss is 4.944185590850561. The k is 13.726688692478636, and b is -64.08811806517805\n",
      "This is the 87 time's iteration. The loss is 4.943924485136417. The k is 13.725576609327446, and b is -64.08104397301676\n",
      "This is the 88 time's iteration. The loss is 4.943663442210947. The k is 13.72446479360141, and b is -64.07397158197843\n",
      "This is the 89 time's iteration. The loss is 4.943402462059062. The k is 13.723353245236218, and b is -64.06690089165397\n",
      "This is the 90 time's iteration. The loss is 4.943141544665667. The k is 13.722241964167576, and b is -64.05983190163442\n",
      "This is the 91 time's iteration. The loss is 4.942880690015658. The k is 13.72113095033121, and b is -64.05276461151091\n",
      "This is the 92 time's iteration. The loss is 4.942619898093965. The k is 13.720020203662855, and b is -64.04569902087465\n",
      "This is the 93 time's iteration. The loss is 4.942359168885501. The k is 13.718909724098268, and b is -64.03863512931696\n",
      "This is the 94 time's iteration. The loss is 4.9420985023751705. The k is 13.717799511573212, and b is -64.03157293642926\n",
      "This is the 95 time's iteration. The loss is 4.94183789854791. The k is 13.716689566023476, and b is -64.02451244180307\n",
      "This is the 96 time's iteration. The loss is 4.9415773573886455. The k is 13.715579887384859, and b is -64.01745364503\n",
      "This is the 97 time's iteration. The loss is 4.941316878882298. The k is 13.714470475593176, and b is -64.01039654570178\n",
      "This is the 98 time's iteration. The loss is 4.941056463013813. The k is 13.713361330584258, and b is -64.0033411434102\n",
      "This is the 99 time's iteration. The loss is 4.940796109768114. The k is 13.71225245229395, and b is -63.99628743774718\n",
      "This is the 100 time's iteration. The loss is 4.940535819130153. The k is 13.711143840658114, and b is -63.989235428304724\n",
      "This is the 101 time's iteration. The loss is 4.940275591084868. The k is 13.710035495612628, and b is -63.98218511467495\n",
      "This is the 102 time's iteration. The loss is 4.940015425617212. The k is 13.708927417093385, and b is -63.97513649645005\n",
      "This is the 103 time's iteration. The loss is 4.939755738744998. The k is 13.70781960503629, and b is -63.96808957322233\n",
      "This is the 104 time's iteration. The loss is 4.939499585224782. The k is 13.70671205937727, and b is -63.96104434458419\n",
      "This is the 105 time's iteration. The loss is 4.939243493302383. The k is 13.70560478005226, and b is -63.95400081012812\n",
      "This is the 106 time's iteration. The loss is 4.938987462962989. The k is 13.704497766997216, and b is -63.94695896944673\n",
      "This is the 107 time's iteration. The loss is 4.938731494191791. The k is 13.703391020148109, and b is -63.93991882213271\n",
      "This is the 108 time's iteration. The loss is 4.938475586973979. The k is 13.70228453944092, and b is -63.93288036777885\n",
      "This is the 109 time's iteration. The loss is 4.938219741294763. The k is 13.70117832481165, and b is -63.92584360597804\n",
      "This is the 110 time's iteration. The loss is 4.937963957139327. The k is 13.700072376196317, and b is -63.91880853632327\n",
      "This is the 111 time's iteration. The loss is 4.937708234492899. The k is 13.698966693530952, and b is -63.91177515840762\n",
      "This is the 112 time's iteration. The loss is 4.937452573340657. The k is 13.6978612767516, and b is -63.90474347182428\n",
      "This is the 113 time's iteration. The loss is 4.9371969736678505. The k is 13.696756125794323, and b is -63.89771347616653\n",
      "This is the 114 time's iteration. The loss is 4.93694143545966. The k is 13.695651240595199, and b is -63.89068517102775\n",
      "This is the 115 time's iteration. The loss is 4.936685958701331. The k is 13.694546621090318, and b is -63.883658556001414\n",
      "This is the 116 time's iteration. The loss is 4.936430543378071. The k is 13.693442267215792, and b is -63.8766336306811\n",
      "This is the 117 time's iteration. The loss is 4.936175189475117. The k is 13.69233817890774, and b is -63.86961039466048\n",
      "This is the 118 time's iteration. The loss is 4.935919896977688. The k is 13.691234356102303, and b is -63.86258884753332\n",
      "This is the 119 time's iteration. The loss is 4.935664665871019. The k is 13.690130798735636, and b is -63.85556898889349\n",
      "This is the 120 time's iteration. The loss is 4.935409496140364. The k is 13.689027506743905, and b is -63.84855081833496\n",
      "This is the 121 time's iteration. The loss is 4.935154387770943. The k is 13.687924480063296, and b is -63.84153433545178\n",
      "This is the 122 time's iteration. The loss is 4.934899340748015. The k is 13.686821718630013, and b is -63.83451953983813\n",
      "This is the 123 time's iteration. The loss is 4.934644355056825. The k is 13.685719222380264, and b is -63.82750643108827\n",
      "This is the 124 time's iteration. The loss is 4.9343894306826055. The k is 13.684616991250287, and b is -63.820495008796534\n",
      "This is the 125 time's iteration. The loss is 4.934134567610637. The k is 13.683515025176323, and b is -63.813485272557394\n",
      "This is the 126 time's iteration. The loss is 4.933879765826171. The k is 13.682413324094636, and b is -63.80647722196539\n",
      "This is the 127 time's iteration. The loss is 4.933625025314468. The k is 13.681311887941503, and b is -63.79947085661517\n",
      "This is the 128 time's iteration. The loss is 4.933370346060791. The k is 13.680210716653214, and b is -63.79246617610149\n",
      "This is the 129 time's iteration. The loss is 4.93311572805041. The k is 13.679109810166077, and b is -63.78546318001919\n",
      "This is the 130 time's iteration. The loss is 4.932861171268596. The k is 13.678009168416414, and b is -63.77846186796321\n",
      "This is the 131 time's iteration. The loss is 4.932606675700623. The k is 13.676908791340564, and b is -63.77146223952859\n",
      "This is the 132 time's iteration. The loss is 4.932352241331789. The k is 13.675808678874882, and b is -63.76446429431046\n",
      "This is the 133 time's iteration. The loss is 4.932097868147353. The k is 13.674708830955733, and b is -63.757468031904054\n",
      "This is the 134 time's iteration. The loss is 4.931843556132617. The k is 13.673609247519504, and b is -63.75047345190471\n",
      "This is the 135 time's iteration. The loss is 4.931589305272865. The k is 13.672509928502592, and b is -63.74348055390785\n",
      "This is the 136 time's iteration. The loss is 4.931335115553388. The k is 13.671410873841413, and b is -63.736489337509\n",
      "This is the 137 time's iteration. The loss is 4.931080986959492. The k is 13.670312083472396, and b is -63.72949980230378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 138 time's iteration. The loss is 4.9308269194764724. The k is 13.669213557331986, and b is -63.72251194788791\n",
      "This is the 139 time's iteration. The loss is 4.930572913089644. The k is 13.668115295356642, and b is -63.71552577385721\n",
      "This is the 140 time's iteration. The loss is 4.930318967784301. The k is 13.667017297482843, and b is -63.7085412798076\n",
      "This is the 141 time's iteration. The loss is 4.930065083545753. The k is 13.665919563647078, and b is -63.701558465335076\n",
      "This is the 142 time's iteration. The loss is 4.9298112603593385. The k is 13.664822093785853, and b is -63.694577330035756\n",
      "This is the 143 time's iteration. The loss is 4.929557498210343. The k is 13.66372488783569, and b is -63.687597873505844\n",
      "This is the 144 time's iteration. The loss is 4.929303797084113. The k is 13.662627945733124, and b is -63.680620095341645\n",
      "This is the 145 time's iteration. The loss is 4.929050156965978. The k is 13.661531267414711, and b is -63.67364399513956\n",
      "This is the 146 time's iteration. The loss is 4.928796577841241. The k is 13.660434852817014, and b is -63.666669572496076\n",
      "This is the 147 time's iteration. The loss is 4.928543059695255. The k is 13.659338701876617, and b is -63.65969682700779\n",
      "This is the 148 time's iteration. The loss is 4.928289602513355. The k is 13.65824281453012, and b is -63.6527257582714\n",
      "This is the 149 time's iteration. The loss is 4.928036206280871. The k is 13.657147190714133, and b is -63.645756365883685\n",
      "This is the 150 time's iteration. The loss is 4.927782870983158. The k is 13.656051830365284, and b is -63.63878864944153\n",
      "This is the 151 time's iteration. The loss is 4.927529596605555. The k is 13.654956733420216, and b is -63.631822608541924\n",
      "This is the 152 time's iteration. The loss is 4.927276383133413. The k is 13.65386189981559, and b is -63.62485824278194\n",
      "This is the 153 time's iteration. The loss is 4.927023230552091. The k is 13.65276732948808, and b is -63.61789555175876\n",
      "This is the 154 time's iteration. The loss is 4.92677013884694. The k is 13.651673022374375, and b is -63.61093453506964\n",
      "This is the 155 time's iteration. The loss is 4.92651710800333. The k is 13.650578978411177, and b is -63.603975192311964\n",
      "This is the 156 time's iteration. The loss is 4.926264138006618. The k is 13.649485197535208, and b is -63.597017523083196\n",
      "This is the 157 time's iteration. The loss is 4.926011228842175. The k is 13.648391679683202, and b is -63.5900615269809\n",
      "This is the 158 time's iteration. The loss is 4.9257583804953615. The k is 13.647298424791911, and b is -63.58310720360272\n",
      "This is the 159 time's iteration. The loss is 4.9255055929515725. The k is 13.646205432798096, and b is -63.576154552546434\n",
      "This is the 160 time's iteration. The loss is 4.925252866196168. The k is 13.64511270363854, and b is -63.56920357340989\n",
      "This is the 161 time's iteration. The loss is 4.9250002002145505. The k is 13.64402023725004, and b is -63.562254265791026\n",
      "This is the 162 time's iteration. The loss is 4.924747594992085. The k is 13.642928033569405, and b is -63.5553066292879\n",
      "This is the 163 time's iteration. The loss is 4.924495050514168. The k is 13.64183609253346, and b is -63.54836066349864\n",
      "This is the 164 time's iteration. The loss is 4.924242566766201. The k is 13.640744414079048, and b is -63.5414163680215\n",
      "This is the 165 time's iteration. The loss is 4.923990143733566. The k is 13.639652998143026, and b is -63.534473742454814\n",
      "This is the 166 time's iteration. The loss is 4.923737781401668. The k is 13.638561844662263, and b is -63.52753278639701\n",
      "This is the 167 time's iteration. The loss is 4.923485479755915. The k is 13.637470953573647, and b is -63.520593499446626\n",
      "This is the 168 time's iteration. The loss is 4.923233238781708. The k is 13.636380324814082, and b is -63.51365588120228\n",
      "This is the 169 time's iteration. The loss is 4.922981058464457. The k is 13.635289958320485, and b is -63.5067199312627\n",
      "This is the 170 time's iteration. The loss is 4.922728938789578. The k is 13.634199854029784, and b is -63.4997856492267\n",
      "This is the 171 time's iteration. The loss is 4.922476879742497. The k is 13.63311001187893, and b is -63.4928530346932\n",
      "This is the 172 time's iteration. The loss is 4.92222488130861. The k is 13.632020431804886, and b is -63.4859220872612\n",
      "This is the 173 time's iteration. The loss is 4.921972943473369. The k is 13.630931113744628, and b is -63.478992806529824\n",
      "This is the 174 time's iteration. The loss is 4.921721066222188. The k is 13.62984205763515, and b is -63.47206519209827\n",
      "This is the 175 time's iteration. The loss is 4.921469249540494. The k is 13.62875326341346, and b is -63.46513924356584\n",
      "This is the 176 time's iteration. The loss is 4.921217493413728. The k is 13.62766473101658, and b is -63.458214960531926\n",
      "This is the 177 time's iteration. The loss is 4.920965797827335. The k is 13.62657646038155, and b is -63.451292342596034\n",
      "This is the 178 time's iteration. The loss is 4.920714162766742. The k is 13.625488451445422, and b is -63.44437138935774\n",
      "This is the 179 time's iteration. The loss is 4.920462588217402. The k is 13.624400704145266, and b is -63.43745210041674\n",
      "This is the 180 time's iteration. The loss is 4.920211074164777. The k is 13.623313218418167, and b is -63.430534475372816\n",
      "This is the 181 time's iteration. The loss is 4.9199596205942875. The k is 13.622225994201221, and b is -63.42361851382584\n",
      "This is the 182 time's iteration. The loss is 4.919708991082624. The k is 13.621139031431545, and b is -63.4167042153758\n",
      "This is the 183 time's iteration. The loss is 4.919460253278192. The k is 13.620052330046265, and b is -63.409791579622755\n",
      "This is the 184 time's iteration. The loss is 4.9192115752882914. The k is 13.618965889982528, and b is -63.402880606166875\n",
      "This is the 185 time's iteration. The loss is 4.918962957098552. The k is 13.61787971117749, and b is -63.39597129460843\n",
      "This is the 186 time's iteration. The loss is 4.918714398694601. The k is 13.61679379356833, and b is -63.38906364454777\n",
      "This is the 187 time's iteration. The loss is 4.918465900062046. The k is 13.615708137092236, and b is -63.38215765558535\n",
      "This is the 188 time's iteration. The loss is 4.918217461186517. The k is 13.61462274168641, and b is -63.37525332732174\n",
      "This is the 189 time's iteration. The loss is 4.917969082053653. The k is 13.613537607288075, and b is -63.36835065935757\n",
      "This is the 190 time's iteration. The loss is 4.917720762649076. The k is 13.612452733834466, and b is -63.36144965129359\n",
      "This is the 191 time's iteration. The loss is 4.917472502958437. The k is 13.611368121262831, and b is -63.354550302730644\n",
      "This is the 192 time's iteration. The loss is 4.917224302967359. The k is 13.610283769510438, and b is -63.34765261326966\n",
      "This is the 193 time's iteration. The loss is 4.9169761626614985. The k is 13.609199678514564, and b is -63.34075658251167\n",
      "This is the 194 time's iteration. The loss is 4.916728082026491. The k is 13.608115848212506, and b is -63.333862210057816\n",
      "This is the 195 time's iteration. The loss is 4.916480061048005. The k is 13.607032278541574, and b is -63.32696949550931\n",
      "This is the 196 time's iteration. The loss is 4.9162320997116815. The k is 13.605948969439094, and b is -63.32007843846747\n",
      "This is the 197 time's iteration. The loss is 4.915984198003183. The k is 13.604865920842405, and b is -63.313189038533714\n",
      "This is the 198 time's iteration. The loss is 4.91573635590817. The k is 13.603783132688864, and b is -63.30630129530956\n",
      "This is the 199 time's iteration. The loss is 4.915488573412306. The k is 13.60270060491584, and b is -63.2994152083966\n",
      "This is the 200 time's iteration. The loss is 4.915240850501254. The k is 13.601618337460723, and b is -63.29253077739656\n",
      "This is the 201 time's iteration. The loss is 4.914993187160701. The k is 13.600536330260908, and b is -63.28564800191122\n",
      "This is the 202 time's iteration. The loss is 4.914745583376311. The k is 13.599454583253815, and b is -63.27876688154248\n",
      "This is the 203 time's iteration. The loss is 4.914498039133757. The k is 13.598373096376873, and b is -63.27188741589233\n",
      "This is the 204 time's iteration. The loss is 4.914250554418729. The k is 13.597291869567528, and b is -63.265009604562856\n",
      "This is the 205 time's iteration. The loss is 4.914003129216911. The k is 13.596210902763241, and b is -63.25813344715624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 206 time's iteration. The loss is 4.913755763513998. The k is 13.595130195901488, and b is -63.25125894327476\n",
      "This is the 207 time's iteration. The loss is 4.913508457295665. The k is 13.594049748919762, and b is -63.24438609252079\n",
      "This is the 208 time's iteration. The loss is 4.9132612105476205. The k is 13.592969561755565, and b is -63.237514894496805\n",
      "This is the 209 time's iteration. The loss is 4.913014023255569. The k is 13.591889634346423, and b is -63.23064534880535\n",
      "This is the 210 time's iteration. The loss is 4.912766895405197. The k is 13.590809966629868, and b is -63.22377745504911\n",
      "This is the 211 time's iteration. The loss is 4.91251982698222. The k is 13.589730558543453, and b is -63.21691121283082\n",
      "This is the 212 time's iteration. The loss is 4.912272817972354. The k is 13.588651410024744, and b is -63.21004662175334\n",
      "This is the 213 time's iteration. The loss is 4.912025868361296. The k is 13.587572521011321, and b is -63.20318368141962\n",
      "This is the 214 time's iteration. The loss is 4.911778978134767. The k is 13.586493891440782, and b is -63.19632239143269\n",
      "This is the 215 time's iteration. The loss is 4.911532147278494. The k is 13.585415521250738, and b is -63.189462751395695\n",
      "This is the 216 time's iteration. The loss is 4.911285375778198. The k is 13.584337410378813, and b is -63.182604760911865\n",
      "This is the 217 time's iteration. The loss is 4.911038663619593. The k is 13.583259558762652, and b is -63.17574841958453\n",
      "This is the 218 time's iteration. The loss is 4.910792010788432. The k is 13.582181966339906, and b is -63.168893727017114\n",
      "This is the 219 time's iteration. The loss is 4.910545417270433. The k is 13.58110463304825, and b is -63.162040682813135\n",
      "This is the 220 time's iteration. The loss is 4.910298883051326. The k is 13.580027558825371, and b is -63.15518928657621\n",
      "This is the 221 time's iteration. The loss is 4.910052408116869. The k is 13.578950743608967, and b is -63.14833953791005\n",
      "This is the 222 time's iteration. The loss is 4.9098059924528. The k is 13.577874187336755, and b is -63.14149143641845\n",
      "This is the 223 time's iteration. The loss is 4.909559636044857. The k is 13.576797889946468, and b is -63.13464498170532\n",
      "This is the 224 time's iteration. The loss is 4.909313338878798. The k is 13.57572185137585, and b is -63.12780017337465\n",
      "This is the 225 time's iteration. The loss is 4.90906710094038. The k is 13.574646071562663, and b is -63.12095701103054\n",
      "This is the 226 time's iteration. The loss is 4.908820922215353. The k is 13.573570550444682, and b is -63.114115494277165\n",
      "This is the 227 time's iteration. The loss is 4.9085748026894835. The k is 13.5724952879597, and b is -63.10727562271881\n",
      "This is the 228 time's iteration. The loss is 4.908328742348532. The k is 13.571420284045521, and b is -63.10043739595985\n",
      "This is the 229 time's iteration. The loss is 4.908082741178261. The k is 13.570345538639968, and b is -63.09360081360476\n",
      "This is the 230 time's iteration. The loss is 4.907836799164459. The k is 13.569271051680873, and b is -63.0867658752581\n",
      "This is the 231 time's iteration. The loss is 4.907590916292891. The k is 13.56819682310609, and b is -63.07993258052454\n",
      "This is the 232 time's iteration. The loss is 4.907345092549327. The k is 13.567122852853485, and b is -63.07310092900883\n",
      "This is the 233 time's iteration. The loss is 4.907099327919558. The k is 13.566049140860937, and b is -63.06627092031582\n",
      "This is the 234 time's iteration. The loss is 4.906853622389363. The k is 13.564975687066342, and b is -63.05944255405047\n",
      "This is the 235 time's iteration. The loss is 4.9066079759445325. The k is 13.563902491407612, and b is -63.0526158298178\n",
      "This is the 236 time's iteration. The loss is 4.906362388570861. The k is 13.56282955382267, and b is -63.045790747222966\n",
      "This is the 237 time's iteration. The loss is 4.90611686025414. The k is 13.561756874249458, and b is -63.03896730587119\n",
      "This is the 238 time's iteration. The loss is 4.905871390980174. The k is 13.56068445262593, and b is -63.0321455053678\n",
      "This is the 239 time's iteration. The loss is 4.905625980734751. The k is 13.559612288890058, and b is -63.02532534531822\n",
      "This is the 240 time's iteration. The loss is 4.905380629503692. The k is 13.558540382979826, and b is -63.01850682532797\n",
      "This is the 241 time's iteration. The loss is 4.905135337272797. The k is 13.557468734833236, and b is -63.01168994500265\n",
      "This is the 242 time's iteration. The loss is 4.904890104027878. The k is 13.556397344388301, and b is -63.004874703947976\n",
      "This is the 243 time's iteration. The loss is 4.904644929754754. The k is 13.555326211583052, and b is -62.99806110176974\n",
      "This is the 244 time's iteration. The loss is 4.904399814439243. The k is 13.554255336355533, and b is -62.99124913807385\n",
      "This is the 245 time's iteration. The loss is 4.904154758067165. The k is 13.553184718643802, and b is -62.98443881246629\n",
      "This is the 246 time's iteration. The loss is 4.903909760624351. The k is 13.552114358385937, and b is -62.97763012455314\n",
      "This is the 247 time's iteration. The loss is 4.903664822096622. The k is 13.551044255520027, and b is -62.97082307394058\n",
      "This is the 248 time's iteration. The loss is 4.903419942469816. The k is 13.549974409984175, and b is -62.964017660234894\n",
      "This is the 249 time's iteration. The loss is 4.903175121729771. The k is 13.5489048217165, and b is -62.95721388304245\n",
      "This is the 250 time's iteration. The loss is 4.902930359862325. The k is 13.547835490655137, and b is -62.9504117419697\n",
      "This is the 251 time's iteration. The loss is 4.902685656853318. The k is 13.546766416738235, and b is -62.94361123662322\n",
      "This is the 252 time's iteration. The loss is 4.902441012688595. The k is 13.545697599903958, and b is -62.93681236660965\n",
      "This is the 253 time's iteration. The loss is 4.902196427354008. The k is 13.544629040090484, and b is -62.93001513153574\n",
      "This is the 254 time's iteration. The loss is 4.901951900835411. The k is 13.543560737236007, and b is -62.923219531008336\n",
      "This is the 255 time's iteration. The loss is 4.901707433118662. The k is 13.542492691278737, and b is -62.91642556463437\n",
      "This is the 256 time's iteration. The loss is 4.901463024189617. The k is 13.541424902156894, and b is -62.90963323202088\n",
      "This is the 257 time's iteration. The loss is 4.901218674034139. The k is 13.540357369808719, and b is -62.90284253277499\n",
      "This is the 258 time's iteration. The loss is 4.900974382638095. The k is 13.539290094172465, and b is -62.896053466503915\n",
      "This is the 259 time's iteration. The loss is 4.9007301499873535. The k is 13.538223075186398, and b is -62.88926603281497\n",
      "This is the 260 time's iteration. The loss is 4.9004859760678. The k is 13.5371563127888, and b is -62.882480231315576\n",
      "This is the 261 time's iteration. The loss is 4.900241860865295. The k is 13.536089806917973, and b is -62.875696061613226\n",
      "This is the 262 time's iteration. The loss is 4.899997804365727. The k is 13.535023557512227, and b is -62.86891352331552\n",
      "This is the 263 time's iteration. The loss is 4.899753806554979. The k is 13.533957564509889, and b is -62.86213261603015\n",
      "This is the 264 time's iteration. The loss is 4.899509867418937. The k is 13.5328918278493, and b is -62.85535333936491\n",
      "This is the 265 time's iteration. The loss is 4.899265986943485. The k is 13.531826347468819, and b is -62.84857569292767\n",
      "This is the 266 time's iteration. The loss is 4.899022165114523. The k is 13.530761123306815, and b is -62.84179967632641\n",
      "This is the 267 time's iteration. The loss is 4.898778401917956. The k is 13.529696155301679, and b is -62.8350252891692\n",
      "This is the 268 time's iteration. The loss is 4.898534697339672. The k is 13.52863144339181, and b is -62.82825253106421\n",
      "This is the 269 time's iteration. The loss is 4.898291051365584. The k is 13.527566987515623, and b is -62.821481401619685\n",
      "This is the 270 time's iteration. The loss is 4.898047463981593. The k is 13.52650278761155, and b is -62.81471190044399\n",
      "This is the 271 time's iteration. The loss is 4.897803935173602. The k is 13.525438843618037, and b is -62.80794402714556\n",
      "This is the 272 time's iteration. The loss is 4.897560464927548. The k is 13.524375155473544, and b is -62.801177781332946\n",
      "This is the 273 time's iteration. The loss is 4.897317053229332. The k is 13.523311723116548, and b is -62.794413162614774\n",
      "This is the 274 time's iteration. The loss is 4.897073700064873. The k is 13.522248546485535, and b is -62.787650170599775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 275 time's iteration. The loss is 4.896830405420103. The k is 13.521185625519015, and b is -62.780888804896776\n",
      "This is the 276 time's iteration. The loss is 4.896587169280948. The k is 13.520122960155506, and b is -62.77412906511469\n",
      "This is the 277 time's iteration. The loss is 4.896343991633332. The k is 13.519060550333542, and b is -62.767370950862535\n",
      "This is the 278 time's iteration. The loss is 4.896100872463206. The k is 13.517998395991672, and b is -62.76061446174941\n",
      "This is the 279 time's iteration. The loss is 4.89585781175649. The k is 13.516936497068462, and b is -62.75385959738452\n",
      "This is the 280 time's iteration. The loss is 4.895614809499139. The k is 13.515874853502488, and b is -62.74710635737715\n",
      "This is the 281 time's iteration. The loss is 4.89537186567708. The k is 13.514813465232347, and b is -62.74035474133669\n",
      "This is the 282 time's iteration. The loss is 4.895128980276278. The k is 13.513752332196646, and b is -62.73360474887262\n",
      "This is the 283 time's iteration. The loss is 4.894886153282679. The k is 13.512691454334007, and b is -62.72685637959452\n",
      "This is the 284 time's iteration. The loss is 4.894643384682233. The k is 13.51163083158307, and b is -62.72010963311206\n",
      "This is the 285 time's iteration. The loss is 4.894400674460905. The k is 13.510570463882486, and b is -62.713364509034996\n",
      "This is the 286 time's iteration. The loss is 4.894158022604653. The k is 13.509510351170924, and b is -62.70662100697319\n",
      "This is the 287 time's iteration. The loss is 4.893915429099439. The k is 13.508450493387066, and b is -62.69987912653658\n",
      "This is the 288 time's iteration. The loss is 4.893672893931238. The k is 13.507390890469607, and b is -62.69313886733522\n",
      "This is the 289 time's iteration. The loss is 4.893430417086017. The k is 13.506331542357259, and b is -62.68640022897925\n",
      "This is the 290 time's iteration. The loss is 4.893187998549751. The k is 13.505272448988752, and b is -62.6796632110789\n",
      "This is the 291 time's iteration. The loss is 4.892945638308419. The k is 13.504213610302823, and b is -62.6729278132445\n",
      "This is the 292 time's iteration. The loss is 4.89270333634801. The k is 13.50315502623823, and b is -62.666194035086455\n",
      "This is the 293 time's iteration. The loss is 4.8924610926545. The k is 13.502096696733743, and b is -62.65946187621529\n",
      "This is the 294 time's iteration. The loss is 4.8922189072138655. The k is 13.501038621728148, and b is -62.65273133624161\n",
      "This is the 295 time's iteration. The loss is 4.891976780012125. The k is 13.499980801160245, and b is -62.646002414776106\n",
      "This is the 296 time's iteration. The loss is 4.891734711035263. The k is 13.498923234968847, and b is -62.63927511142958\n",
      "This is the 297 time's iteration. The loss is 4.891492700269266. The k is 13.497865923092787, and b is -62.63254942581292\n",
      "This is the 298 time's iteration. The loss is 4.891250747700151. The k is 13.496808865470905, and b is -62.6258253575371\n",
      "This is the 299 time's iteration. The loss is 4.891008853313912. The k is 13.495752062042063, and b is -62.6191029062132\n",
      "This is the 300 time's iteration. The loss is 4.890767017096575. The k is 13.494695512745134, and b is -62.61238207145239\n",
      "This is the 301 time's iteration. The loss is 4.890525239034133. The k is 13.493639217519004, and b is -62.60566285286592\n",
      "This is the 302 time's iteration. The loss is 4.89028351911261. The k is 13.49258317630258, and b is -62.59894525006516\n",
      "This is the 303 time's iteration. The loss is 4.890041857318021. The k is 13.491527389034777, and b is -62.59222926266155\n",
      "This is the 304 time's iteration. The loss is 4.889800253636395. The k is 13.490471855654528, and b is -62.58551489026663\n",
      "This is the 305 time's iteration. The loss is 4.8895587080537535. The k is 13.48941657610078, and b is -62.578802132492044\n",
      "This is the 306 time's iteration. The loss is 4.889317220556123. The k is 13.488361550312497, and b is -62.57209098894951\n",
      "This is the 307 time's iteration. The loss is 4.889075791129539. The k is 13.487306778228652, and b is -62.56538145925086\n",
      "This is the 308 time's iteration. The loss is 4.88883441976003. The k is 13.486252259788237, and b is -62.558673543008\n",
      "This is the 309 time's iteration. The loss is 4.8885931064336505. The k is 13.485197994930259, and b is -62.55196723983294\n",
      "This is the 310 time's iteration. The loss is 4.888351851136425. The k is 13.484143983593738, and b is -62.545262549337785\n",
      "This is the 311 time's iteration. The loss is 4.888110653854409. The k is 13.483090225717708, and b is -62.53855947113473\n",
      "This is the 312 time's iteration. The loss is 4.887869514573647. The k is 13.48203672124122, and b is -62.53185800483607\n",
      "This is the 313 time's iteration. The loss is 4.8876284332802005. The k is 13.480983470103338, and b is -62.525158150054175\n",
      "This is the 314 time's iteration. The loss is 4.887387409960112. The k is 13.479930472243142, and b is -62.518459906401525\n",
      "This is the 315 time's iteration. The loss is 4.88714644459945. The k is 13.478877727599725, and b is -62.511763273490686\n",
      "This is the 316 time's iteration. The loss is 4.886905537184274. The k is 13.477825236112194, and b is -62.50506825093432\n",
      "This is the 317 time's iteration. The loss is 4.88666468770065. The k is 13.476772997719674, and b is -62.498374838345185\n",
      "This is the 318 time's iteration. The loss is 4.886423896134647. The k is 13.475721012361303, and b is -62.49168303533613\n",
      "This is the 319 time's iteration. The loss is 4.886183162472335. The k is 13.474669279976231, and b is -62.48499284152009\n",
      "This is the 320 time's iteration. The loss is 4.885942486699789. The k is 13.473617800503627, and b is -62.4783042565101\n",
      "This is the 321 time's iteration. The loss is 4.885701868803092. The k is 13.472566573882672, and b is -62.471617279919286\n",
      "This is the 322 time's iteration. The loss is 4.8854613087683285. The k is 13.471515600052562, and b is -62.46493191136087\n",
      "This is the 323 time's iteration. The loss is 4.885220806581574. The k is 13.470464878952509, and b is -62.45824815044816\n",
      "This is the 324 time's iteration. The loss is 4.884980362228933. The k is 13.469414410521734, and b is -62.45156599679457\n",
      "This is the 325 time's iteration. The loss is 4.88473997569648. The k is 13.468364194699483, and b is -62.444885450013594\n",
      "This is the 326 time's iteration. The loss is 4.884499646970331. The k is 13.467314231425007, and b is -62.438206509718825\n",
      "This is the 327 time's iteration. The loss is 4.884259376036576. The k is 13.466264520637576, and b is -62.43152917552395\n",
      "This is the 328 time's iteration. The loss is 4.884019162881312. The k is 13.465215062276476, and b is -62.424853447042736\n",
      "This is the 329 time's iteration. The loss is 4.88377900749065. The k is 13.464165856281001, and b is -62.41817932388906\n",
      "This is the 330 time's iteration. The loss is 4.883538909850692. The k is 13.463116902590468, and b is -62.41150680567689\n",
      "This is the 331 time's iteration. The loss is 4.883298869947556. The k is 13.462068201144204, and b is -62.40483589202028\n",
      "This is the 332 time's iteration. The loss is 4.883058887767364. The k is 13.46101975188155, and b is -62.39816658253338\n",
      "This is the 333 time's iteration. The loss is 4.882818963296234. The k is 13.459971554741863, and b is -62.391498876830426\n",
      "This is the 334 time's iteration. The loss is 4.882579096520288. The k is 13.458923609664515, and b is -62.384832774525755\n",
      "This is the 335 time's iteration. The loss is 4.882339287425638. The k is 13.457875916588893, and b is -62.37816827523379\n",
      "This is the 336 time's iteration. The loss is 4.882099535998429. The k is 13.456828475454396, and b is -62.37150537856906\n",
      "This is the 337 time's iteration. The loss is 4.881859842224784. The k is 13.45578128620044, and b is -62.36484408414617\n",
      "This is the 338 time's iteration. The loss is 4.881620206090847. The k is 13.454734348766452, and b is -62.35818439157983\n",
      "This is the 339 time's iteration. The loss is 4.881380627582751. The k is 13.45368766309188, and b is -62.35152630048484\n",
      "This is the 340 time's iteration. The loss is 4.881141106686644. The k is 13.452641229116182, and b is -62.344869810476084\n",
      "This is the 341 time's iteration. The loss is 4.880901643388673. The k is 13.451595046778829, and b is -62.33821492116854\n",
      "This is the 342 time's iteration. The loss is 4.880662237674972. The k is 13.450549116019312, and b is -62.3315616321773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 343 time's iteration. The loss is 4.880422889531711. The k is 13.449503436777132, and b is -62.32490994311752\n",
      "This is the 344 time's iteration. The loss is 4.880183598945041. The k is 13.448458008991807, and b is -62.31825985360447\n",
      "This is the 345 time's iteration. The loss is 4.87994436590113. The k is 13.447412832602867, and b is -62.31161136325349\n",
      "This is the 346 time's iteration. The loss is 4.879705190386116. The k is 13.44636790754986, and b is -62.30496447168004\n",
      "This is the 347 time's iteration. The loss is 4.879466072386177. The k is 13.445323233772344, and b is -62.298319178499646\n",
      "This is the 348 time's iteration. The loss is 4.8792270118874965. The k is 13.444278811209896, and b is -62.29167548332795\n",
      "This is the 349 time's iteration. The loss is 4.8789880088762265. The k is 13.443234639802109, and b is -62.28503338578067\n",
      "This is the 350 time's iteration. The loss is 4.878749063338557. The k is 13.44219071948858, and b is -62.27839288547362\n",
      "This is the 351 time's iteration. The loss is 4.878510175260657. The k is 13.441147050208935, and b is -62.271753982022716\n",
      "This is the 352 time's iteration. The loss is 4.878271344628721. The k is 13.440103631902803, and b is -62.26511667504395\n",
      "This is the 353 time's iteration. The loss is 4.878032571428918. The k is 13.439060464509833, and b is -62.258480964153414\n",
      "This is the 354 time's iteration. The loss is 4.87779385564745. The k is 13.438017547969686, and b is -62.2518468489673\n",
      "This is the 355 time's iteration. The loss is 4.877555197270516. The k is 13.43697488222204, and b is -62.24521432910188\n",
      "This is the 356 time's iteration. The loss is 4.8773165962842935. The k is 13.43593246720659, and b is -62.23858340417352\n",
      "This is the 357 time's iteration. The loss is 4.877078052674996. The k is 13.434890302863034, and b is -62.23195407379869\n",
      "This is the 358 time's iteration. The loss is 4.876839566428813. The k is 13.433848389131098, and b is -62.22532633759394\n",
      "This is the 359 time's iteration. The loss is 4.876601137531958. The k is 13.432806725950517, and b is -62.21870019517592\n",
      "This is the 360 time's iteration. The loss is 4.876362765970641. The k is 13.431765313261037, and b is -62.21207564616137\n",
      "This is the 361 time's iteration. The loss is 4.876124451731074. The k is 13.430724151002424, and b is -62.20545269016711\n",
      "This is the 362 time's iteration. The loss is 4.8758861947994685. The k is 13.429683239114457, and b is -62.198831326810065\n",
      "This is the 363 time's iteration. The loss is 4.8756479951620495. The k is 13.428642577536928, and b is -62.19221155570726\n",
      "This is the 364 time's iteration. The loss is 4.875409852805034. The k is 13.427602166209644, and b is -62.1855933764758\n",
      "This is the 365 time's iteration. The loss is 4.875171767714649. The k is 13.426562005072427, and b is -62.178976788732875\n",
      "This is the 366 time's iteration. The loss is 4.874933739877134. The k is 13.425522094065112, and b is -62.172361792095785\n",
      "This is the 367 time's iteration. The loss is 4.8746957692787065. The k is 13.424482433127551, and b is -62.16574838618191\n",
      "This is the 368 time's iteration. The loss is 4.874457855905601. The k is 13.42344302219961, and b is -62.15913657060872\n",
      "This is the 369 time's iteration. The loss is 4.874219999744074. The k is 13.422403861221166, and b is -62.152526344993795\n",
      "This is the 370 time's iteration. The loss is 4.873982200780355. The k is 13.421364950132117, and b is -62.145917708954784\n",
      "This is the 371 time's iteration. The loss is 4.87374445900068. The k is 13.420326288872369, and b is -62.13931066210944\n",
      "This is the 372 time's iteration. The loss is 4.873506774391315. The k is 13.419287877381844, and b is -62.132705204075606\n",
      "This is the 373 time's iteration. The loss is 4.873269146938509. The k is 13.418249715600481, and b is -62.126101334471215\n",
      "This is the 374 time's iteration. The loss is 4.873031576628508. The k is 13.417211803468232, and b is -62.1194990529143\n",
      "This is the 375 time's iteration. The loss is 4.872794063447591. The k is 13.416174140925062, and b is -62.11289835902297\n",
      "This is the 376 time's iteration. The loss is 4.872556607381997. The k is 13.415136727910953, and b is -62.10629925241545\n",
      "This is the 377 time's iteration. The loss is 4.872319208417998. The k is 13.4140995643659, and b is -62.099701732710024\n",
      "This is the 378 time's iteration. The loss is 4.872081866541868. The k is 13.41306265022991, and b is -62.0931057995251\n",
      "This is the 379 time's iteration. The loss is 4.8718445817398734. The k is 13.412025985443012, and b is -62.08651145247916\n",
      "This is the 380 time's iteration. The loss is 4.871607353998299. The k is 13.410989569945242, and b is -62.07991869119078\n",
      "This is the 381 time's iteration. The loss is 4.871370183303411. The k is 13.409953403676653, and b is -62.07332751527863\n",
      "This is the 382 time's iteration. The loss is 4.8711330696415. The k is 13.40891748657731, and b is -62.06673792436147\n",
      "This is the 383 time's iteration. The loss is 4.870896012998842. The k is 13.407881818587299, and b is -62.06014991805815\n",
      "This is the 384 time's iteration. The loss is 4.870659013361738. The k is 13.406846399646712, and b is -62.053563495987625\n",
      "This is the 385 time's iteration. The loss is 4.870422070716462. The k is 13.405811229695662, and b is -62.04697865776892\n",
      "This is the 386 time's iteration. The loss is 4.870185185049339. The k is 13.404776308674272, and b is -62.040395403021165\n",
      "This is the 387 time's iteration. The loss is 4.869948356346636. The k is 13.403741636522684, and b is -62.03381373136359\n",
      "This is the 388 time's iteration. The loss is 4.869711584594668. The k is 13.402707213181051, and b is -62.02723364241549\n",
      "This is the 389 time's iteration. The loss is 4.869474869779746. The k is 13.40167303858954, and b is -62.020655135796275\n",
      "This is the 390 time's iteration. The loss is 4.869238211888162. The k is 13.400639112688333, and b is -62.01407821112544\n",
      "This is the 391 time's iteration. The loss is 4.869001610906248. The k is 13.399605435417628, and b is -62.00750286802256\n",
      "This is the 392 time's iteration. The loss is 4.868765066820289. The k is 13.398572006717638, and b is -62.00092910610733\n",
      "This is the 393 time's iteration. The loss is 4.868528579616643. The k is 13.397538826528585, and b is -61.9943569249995\n",
      "This is the 394 time's iteration. The loss is 4.868292149281599. The k is 13.396505894790712, and b is -61.98778632431894\n",
      "This is the 395 time's iteration. The loss is 4.8680557758014915. The k is 13.395473211444271, and b is -61.9812173036856\n",
      "This is the 396 time's iteration. The loss is 4.867819459162658. The k is 13.394440776429532, and b is -61.97464986271952\n",
      "This is the 397 time's iteration. The loss is 4.867583199351414. The k is 13.393408589686777, and b is -61.96808400104083\n",
      "This is the 398 time's iteration. The loss is 4.867346996354104. The k is 13.392376651156304, and b is -61.96151971826976\n",
      "This is the 399 time's iteration. The loss is 4.867110850157067. The k is 13.391344960778428, and b is -61.95495701402663\n",
      "This is the 400 time's iteration. The loss is 4.8668747607466365. The k is 13.390313518493471, and b is -61.948395887931845\n",
      "This is the 401 time's iteration. The loss is 4.866638728109163. The k is 13.389282324241774, and b is -61.941836339605906\n",
      "This is the 402 time's iteration. The loss is 4.866402752230989. The k is 13.388251377963694, and b is -61.9352783686694\n",
      "This is the 403 time's iteration. The loss is 4.866166833098471. The k is 13.387220679599599, and b is -61.928721974743006\n",
      "This is the 404 time's iteration. The loss is 4.865930970697958. The k is 13.386190229089873, and b is -61.922167157447504\n",
      "This is the 405 time's iteration. The loss is 4.8656951650158105. The k is 13.385160026374914, and b is -61.91561391640375\n",
      "This is the 406 time's iteration. The loss is 4.865459416038391. The k is 13.384130071395132, and b is -61.909062251232704\n",
      "This is the 407 time's iteration. The loss is 4.865223723752064. The k is 13.383100364090957, and b is -61.902512161555414\n",
      "This is the 408 time's iteration. The loss is 4.8649880881431935. The k is 13.382070904402827, and b is -61.89596364699302\n",
      "This is the 409 time's iteration. The loss is 4.864752509198146. The k is 13.3810416922712, and b is -61.88941670716674\n",
      "This is the 410 time's iteration. The loss is 4.864516986903309. The k is 13.380012727636544, and b is -61.8828713416979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 411 time's iteration. The loss is 4.864281521245051. The k is 13.378984010439341, and b is -61.87632755020791\n",
      "This is the 412 time's iteration. The loss is 4.8640461122097465. The k is 13.377955540620093, and b is -61.86978533231827\n",
      "This is the 413 time's iteration. The loss is 4.863810759783792. The k is 13.376927318119309, and b is -61.86324468765058\n",
      "This is the 414 time's iteration. The loss is 4.863575463953559. The k is 13.375899342877517, and b is -61.85670561582651\n",
      "This is the 415 time's iteration. The loss is 4.863340224705454. The k is 13.374871614835259, and b is -61.85016811646785\n",
      "This is the 416 time's iteration. The loss is 4.8631050420258575. The k is 13.373844133933089, and b is -61.843632189196455\n",
      "This is the 417 time's iteration. The loss is 4.862869915901181. The k is 13.372816900111577, and b is -61.83709783363429\n",
      "This is the 418 time's iteration. The loss is 4.862635187203532. The k is 13.371789913311307, and b is -61.8305650494034\n",
      "This is the 419 time's iteration. The loss is 4.862402198740821. The k is 13.370763173472879, and b is -61.82403383612591\n",
      "This is the 420 time's iteration. The loss is 4.862169266305367. The k is 13.369736680536905, and b is -61.81750419342407\n",
      "This is the 421 time's iteration. The loss is 4.861936389883709. The k is 13.368710434444008, and b is -61.81097612092019\n",
      "This is the 422 time's iteration. The loss is 4.861703569462367. The k is 13.367684435134834, and b is -61.804449618236674\n",
      "This is the 423 time's iteration. The loss is 4.86147080502788. The k is 13.366658682550035, and b is -61.79792468499604\n",
      "This is the 424 time's iteration. The loss is 4.861238096566787. The k is 13.365633176630283, and b is -61.79140132082087\n",
      "This is the 425 time's iteration. The loss is 4.861005444065621. The k is 13.36460791731626, and b is -61.78487952533384\n",
      "This is the 426 time's iteration. The loss is 4.860772847510924. The k is 13.363582904548664, and b is -61.778359298157746\n",
      "This is the 427 time's iteration. The loss is 4.860540306889255. The k is 13.362558138268211, and b is -61.771840638915435\n",
      "This is the 428 time's iteration. The loss is 4.860307822187148. The k is 13.361533618415624, and b is -61.76532354722987\n",
      "This is the 429 time's iteration. The loss is 4.860075393391171. The k is 13.360509344931645, and b is -61.75880802272409\n",
      "This is the 430 time's iteration. The loss is 4.859843020487869. The k is 13.359485317757027, and b is -61.75229406502124\n",
      "This is the 431 time's iteration. The loss is 4.859610703463805. The k is 13.358461536832545, and b is -61.74578167374454\n",
      "This is the 432 time's iteration. The loss is 4.859378442305544. The k is 13.357438002098977, and b is -61.739270848517315\n",
      "This is the 433 time's iteration. The loss is 4.859146236999647. The k is 13.356414713497125, and b is -61.732761588962966\n",
      "This is the 434 time's iteration. The loss is 4.858914087532688. The k is 13.355391670967798, and b is -61.726253894704996\n",
      "This is the 435 time's iteration. The loss is 4.858681993891238. The k is 13.354368874451824, and b is -61.719747765366996\n",
      "This is the 436 time's iteration. The loss is 4.858449956061866. The k is 13.353346323890044, and b is -61.71324320057265\n",
      "This is the 437 time's iteration. The loss is 4.85821797403116. The k is 13.35232401922331, and b is -61.70674019994572\n",
      "This is the 438 time's iteration. The loss is 4.857986047785697. The k is 13.351301960392496, and b is -61.70023876311007\n",
      "This is the 439 time's iteration. The loss is 4.857754177312062. The k is 13.350280147338482, and b is -61.693738889689655\n",
      "This is the 440 time's iteration. The loss is 4.857522362596849. The k is 13.349258580002166, and b is -61.687240579308515\n",
      "This is the 441 time's iteration. The loss is 4.857290603626636. The k is 13.34823725832446, and b is -61.68074383159078\n",
      "This is the 442 time's iteration. The loss is 4.857058900388035. The k is 13.347216182246289, and b is -61.67424864616068\n",
      "This is the 443 time's iteration. The loss is 4.856827252867632. The k is 13.346195351708595, and b is -61.66775502264252\n",
      "This is the 444 time's iteration. The loss is 4.856595661052039. The k is 13.34517476665233, and b is -61.6612629606607\n",
      "This is the 445 time's iteration. The loss is 4.85636412492785. The k is 13.344154427018463, and b is -61.65477245983972\n",
      "This is the 446 time's iteration. The loss is 4.856132644481673. The k is 13.343134332747978, and b is -61.64828351980417\n",
      "This is the 447 time's iteration. The loss is 4.855901219700126. The k is 13.342114483781872, and b is -61.64179614017871\n",
      "This is the 448 time's iteration. The loss is 4.855669850569822. The k is 13.341094880061155, and b is -61.63531032058812\n",
      "This is the 449 time's iteration. The loss is 4.855438537077382. The k is 13.34007552152685, and b is -61.628826060657246\n",
      "This is the 450 time's iteration. The loss is 4.855207279209411. The k is 13.339056408120003, and b is -61.62234336001104\n",
      "This is the 451 time's iteration. The loss is 4.8549760769525445. The k is 13.338037539781661, and b is -61.61586221827452\n",
      "This is the 452 time's iteration. The loss is 4.854744930293404. The k is 13.337018916452896, and b is -61.60938263507283\n",
      "This is the 453 time's iteration. The loss is 4.854513839218638. The k is 13.336000538074789, and b is -61.60290461003118\n",
      "This is the 454 time's iteration. The loss is 4.854282803714851. The k is 13.334982404588434, and b is -61.596428142774876\n",
      "This is the 455 time's iteration. The loss is 4.854051823768702. The k is 13.333964515934944, and b is -61.58995323292931\n",
      "This is the 456 time's iteration. The loss is 4.853820899366822. The k is 13.332946872055443, and b is -61.583479880119974\n",
      "This is the 457 time's iteration. The loss is 4.853590030495857. The k is 13.331929472891069, and b is -61.57700808397244\n",
      "This is the 458 time's iteration. The loss is 4.853359217142457. The k is 13.330912318382973, and b is -61.57053784411237\n",
      "This is the 459 time's iteration. The loss is 4.853128459293258. The k is 13.329895408472327, and b is -61.56406916016553\n",
      "This is the 460 time's iteration. The loss is 4.852897756934919. The k is 13.328878743100306, and b is -61.55760203175776\n",
      "This is the 461 time's iteration. The loss is 4.852667110054105. The k is 13.32786232220811, and b is -61.551136458514996\n",
      "This is the 462 time's iteration. The loss is 4.852436518637467. The k is 13.326846145736948, and b is -61.544672440063266\n",
      "This is the 463 time's iteration. The loss is 4.852205982671669. The k is 13.32583021362804, and b is -61.53820997602869\n",
      "This is the 464 time's iteration. The loss is 4.851975502143377. The k is 13.324814525822628, and b is -61.53174906603746\n",
      "This is the 465 time's iteration. The loss is 4.8517450770392525. The k is 13.32379908226196, and b is -61.525289709715885\n",
      "This is the 466 time's iteration. The loss is 4.851514707345985. The k is 13.322783882887304, and b is -61.518831906690345\n",
      "This is the 467 time's iteration. The loss is 4.851284393050238. The k is 13.321768927639939, and b is -61.51237565658732\n",
      "This is the 468 time's iteration. The loss is 4.851054134138687. The k is 13.320754216461161, and b is -61.50592095903337\n",
      "This is the 469 time's iteration. The loss is 4.850823930598019. The k is 13.319739749292276, and b is -61.499467813655144\n",
      "This is the 470 time's iteration. The loss is 4.850593782414915. The k is 13.318725526074608, and b is -61.4930162200794\n",
      "This is the 471 time's iteration. The loss is 4.850363689576071. The k is 13.317711546749493, and b is -61.486566177932964\n",
      "This is the 472 time's iteration. The loss is 4.850133652068165. The k is 13.316697811258281, and b is -61.48011768684277\n",
      "This is the 473 time's iteration. The loss is 4.84990366987791. The k is 13.315684319542337, and b is -61.473670746435815\n",
      "This is the 474 time's iteration. The loss is 4.849673742991987. The k is 13.31467107154304, and b is -61.46722535633922\n",
      "This is the 475 time's iteration. The loss is 4.849443871397093. The k is 13.313658067201784, and b is -61.46078151618017\n",
      "This is the 476 time's iteration. The loss is 4.84921405507996. The k is 13.312645306459974, and b is -61.45433922558595\n",
      "This is the 477 time's iteration. The loss is 4.848984294027262. The k is 13.311632789259033, and b is -61.44789848418392\n",
      "This is the 478 time's iteration. The loss is 4.848754588225728. The k is 13.310620515540394, and b is -61.44145929160157\n",
      "This is the 479 time's iteration. The loss is 4.848524937662067. The k is 13.309608485245509, and b is -61.43502164746643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 480 time's iteration. The loss is 4.848295342323007. The k is 13.308596698315839, and b is -61.428585551406144\n",
      "This is the 481 time's iteration. The loss is 4.848065802195246. The k is 13.30758515469286, and b is -61.42215100304845\n",
      "This is the 482 time's iteration. The loss is 4.847836317265525. The k is 13.306573854318069, and b is -61.41571800202116\n",
      "This is the 483 time's iteration. The loss is 4.8476068875205645. The k is 13.305562797132966, and b is -61.40928654795219\n",
      "This is the 484 time's iteration. The loss is 4.847377512947094. The k is 13.304551983079074, and b is -61.402856640469544\n",
      "This is the 485 time's iteration. The loss is 4.847148193531854. The k is 13.303541412097927, and b is -61.3964282792013\n",
      "This is the 486 time's iteration. The loss is 4.84691892926156. The k is 13.30253108413107, and b is -61.39000146377565\n",
      "This is the 487 time's iteration. The loss is 4.846689720122968. The k is 13.301520999120067, and b is -61.38357619382084\n",
      "This is the 488 time's iteration. The loss is 4.846460566102811. The k is 13.300511157006492, and b is -61.37715246896525\n",
      "This is the 489 time's iteration. The loss is 4.846231467187846. The k is 13.299501557731935, and b is -61.37073028883731\n",
      "This is the 490 time's iteration. The loss is 4.846002423364817. The k is 13.298492201238002, and b is -61.36430965306556\n",
      "This is the 491 time's iteration. The loss is 4.845773434620473. The k is 13.29748308746631, and b is -61.35789056127863\n",
      "This is the 492 time's iteration. The loss is 4.84554450094157. The k is 13.296474216358488, and b is -61.351473013105235\n",
      "This is the 493 time's iteration. The loss is 4.845315622314871. The k is 13.295465587856189, and b is -61.34505700817417\n",
      "This is the 494 time's iteration. The loss is 4.845086798727136. The k is 13.294457201901066, and b is -61.33864254611434\n",
      "This is the 495 time's iteration. The loss is 4.8448625963376175. The k is 13.293449058434799, and b is -61.33222962655472\n",
      "This is the 496 time's iteration. The loss is 4.844647812979076. The k is 13.292441157399072, and b is -61.32581824912438\n",
      "This is the 497 time's iteration. The loss is 4.844434686567007. The k is 13.291433498735588, and b is -61.319408413452486\n",
      "This is the 498 time's iteration. The loss is 4.844221611405923. The k is 13.290426082386064, and b is -61.31300011916829\n",
      "This is the 499 time's iteration. The loss is 4.84400858748351. The k is 13.28941890829223, and b is -61.306593365901115\n",
      "This is the 500 time's iteration. The loss is 4.843795614787438. The k is 13.28841197639583, and b is -61.3001881532804\n",
      "This is the 501 time's iteration. The loss is 4.843582693305396. The k is 13.287405286638622, and b is -61.29378448093567\n",
      "This is the 502 time's iteration. The loss is 4.843369823025065. The k is 13.286398838962379, and b is -61.28738234849652\n",
      "This is the 503 time's iteration. The loss is 4.843157003934131. The k is 13.285392633308888, and b is -61.28098175559265\n",
      "This is the 504 time's iteration. The loss is 4.8429442360202835. The k is 13.284386669619947, and b is -61.274582701853845\n",
      "This is the 505 time's iteration. The loss is 4.84273151927122. The k is 13.28338094783737, and b is -61.268185186909975\n",
      "This is the 506 time's iteration. The loss is 4.842518853674634. The k is 13.28237546790299, and b is -61.26178921039101\n",
      "This is the 507 time's iteration. The loss is 4.8423062392182254. The k is 13.281370229758643, and b is -61.25539477192699\n",
      "This is the 508 time's iteration. The loss is 4.842093675889697. The k is 13.280365233346188, and b is -61.24900187114806\n",
      "This is the 509 time's iteration. The loss is 4.841881163676749. The k is 13.279360478607495, and b is -61.242610507684454\n",
      "This is the 510 time's iteration. The loss is 4.841668702567103. The k is 13.27835596548445, and b is -61.236220681166486\n",
      "This is the 511 time's iteration. The loss is 4.841456292548453. The k is 13.277351693918948, and b is -61.22983239122457\n",
      "This is the 512 time's iteration. The loss is 4.841243933608528. The k is 13.276347663852905, and b is -61.22344563748919\n",
      "This is the 513 time's iteration. The loss is 4.841031625735028. The k is 13.275343875228241, and b is -61.21706041959094\n",
      "This is the 514 time's iteration. The loss is 4.840819368915687. The k is 13.274340327986902, and b is -61.2106767371605\n",
      "This is the 515 time's iteration. The loss is 4.840607163138227. The k is 13.27333702207084, and b is -61.20429458982861\n",
      "This is the 516 time's iteration. The loss is 4.840395008390368. The k is 13.272333957422022, and b is -61.19791397722614\n",
      "This is the 517 time's iteration. The loss is 4.840182904659845. The k is 13.27133113398243, and b is -61.19153489898403\n",
      "This is the 518 time's iteration. The loss is 4.839970851934381. The k is 13.270328551694062, and b is -61.1851573547333\n",
      "This is the 519 time's iteration. The loss is 4.8397588502017195. The k is 13.269326210498926, and b is -61.17878134410507\n",
      "This is the 520 time's iteration. The loss is 4.839546899449589. The k is 13.268324110339044, and b is -61.17240686673055\n",
      "This is the 521 time's iteration. The loss is 4.839334999665746. The k is 13.267322251156457, and b is -61.166033922241034\n",
      "This is the 522 time's iteration. The loss is 4.839123150837916. The k is 13.266320632893216, and b is -61.15966251026791\n",
      "This is the 523 time's iteration. The loss is 4.838911352953855. The k is 13.265319255491386, and b is -61.153292630442635\n",
      "This is the 524 time's iteration. The loss is 4.83869960600131. The k is 13.264318118893046, and b is -61.14692428239679\n",
      "This is the 525 time's iteration. The loss is 4.838487909968034. The k is 13.263317223040291, and b is -61.14055746576201\n",
      "This is the 526 time's iteration. The loss is 4.838276264841783. The k is 13.262316567875226, and b is -61.13419218017004\n",
      "This is the 527 time's iteration. The loss is 4.838064670610313. The k is 13.261316153339974, and b is -61.1278284252527\n",
      "This is the 528 time's iteration. The loss is 4.837853127261392. The k is 13.260315979376669, and b is -61.12146620064191\n",
      "This is the 529 time's iteration. The loss is 4.837641634782769. The k is 13.259316045927463, and b is -61.11510550596968\n",
      "This is the 530 time's iteration. The loss is 4.837430193162231. The k is 13.258316352934514, and b is -61.108746340868095\n",
      "This is the 531 time's iteration. The loss is 4.83721880238753. The k is 13.257316900340005, and b is -61.102388704969336\n",
      "This is the 532 time's iteration. The loss is 4.837007462446454. The k is 13.256317688086122, and b is -61.09603259790567\n",
      "This is the 533 time's iteration. The loss is 4.836796173326774. The k is 13.255318716115072, and b is -61.08967801930945\n",
      "This is the 534 time's iteration. The loss is 4.836584935016259. The k is 13.254319984369072, and b is -61.08332496881314\n",
      "This is the 535 time's iteration. The loss is 4.836373747502704. The k is 13.253321492790358, and b is -61.07697344604926\n",
      "This is the 536 time's iteration. The loss is 4.836162610773887. The k is 13.252323241321172, and b is -61.07062345065044\n",
      "This is the 537 time's iteration. The loss is 4.835951524817596. The k is 13.251325229903777, and b is -61.06427498224938\n",
      "This is the 538 time's iteration. The loss is 4.835740489621621. The k is 13.250327458480447, and b is -61.05792804047889\n",
      "This is the 539 time's iteration. The loss is 4.8355295051737635. The k is 13.24932992699347, and b is -61.05158262497186\n",
      "This is the 540 time's iteration. The loss is 4.835318571461803. The k is 13.248332635385147, and b is -61.04523873536125\n",
      "This is the 541 time's iteration. The loss is 4.835107688473556. The k is 13.247335583597794, and b is -61.03889637128014\n",
      "This is the 542 time's iteration. The loss is 4.834896856196826. The k is 13.246338771573742, and b is -61.03255553236167\n",
      "This is the 543 time's iteration. The loss is 4.834686074619401. The k is 13.245342199255333, and b is -61.02621621823909\n",
      "This is the 544 time's iteration. The loss is 4.8344753437291. The k is 13.244345866584926, and b is -61.01987842854573\n",
      "This is the 545 time's iteration. The loss is 4.834264663513737. The k is 13.24334977350489, and b is -61.013542162915\n",
      "This is the 546 time's iteration. The loss is 4.834054033961122. The k is 13.242353919957614, and b is -61.007207420980414\n",
      "This is the 547 time's iteration. The loss is 4.833843455059066. The k is 13.241358305885493, and b is -61.000874202375556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 548 time's iteration. The loss is 4.8336329267954055. The k is 13.24036293123094, and b is -60.99454250673411\n",
      "This is the 549 time's iteration. The loss is 4.8334224491579425. The k is 13.239367795936387, and b is -60.98821233368985\n",
      "This is the 550 time's iteration. The loss is 4.8332120221345205. The k is 13.238372899944268, and b is -60.981883682876635\n",
      "This is the 551 time's iteration. The loss is 4.833001645712962. The k is 13.237378243197043, and b is -60.9755565539284\n",
      "This is the 552 time's iteration. The loss is 4.832791319881092. The k is 13.236383825637175, and b is -60.969230946479186\n",
      "This is the 553 time's iteration. The loss is 4.832581044626756. The k is 13.23538964720715, and b is -60.962906860163116\n",
      "This is the 554 time's iteration. The loss is 4.832370819937782. The k is 13.23439570784946, and b is -60.956584294614395\n",
      "This is the 555 time's iteration. The loss is 4.832160645802016. The k is 13.23340200750662, and b is -60.950263249467326\n",
      "This is the 556 time's iteration. The loss is 4.831950522207298. The k is 13.23240854612115, and b is -60.94394372435629\n",
      "This is the 557 time's iteration. The loss is 4.83174044914148. The k is 13.231415323635588, and b is -60.93762571891576\n",
      "This is the 558 time's iteration. The loss is 4.831530426592407. The k is 13.230422339992485, and b is -60.9313092327803\n",
      "This is the 559 time's iteration. The loss is 4.831320454547929. The k is 13.229429595134405, and b is -60.92499426558456\n",
      "This is the 560 time's iteration. The loss is 4.831110532995909. The k is 13.228437089003931, and b is -60.918680816963274\n",
      "This is the 561 time's iteration. The loss is 4.830900661924193. The k is 13.227444821543651, and b is -60.91236888655127\n",
      "This is the 562 time's iteration. The loss is 4.830690841320656. The k is 13.226452792696174, and b is -60.906058473983464\n",
      "This is the 563 time's iteration. The loss is 4.830481071173156. The k is 13.225461002404119, and b is -60.89974957889485\n",
      "This is the 564 time's iteration. The loss is 4.830271351469553. The k is 13.22446945061012, and b is -60.89344220092052\n",
      "This is the 565 time's iteration. The loss is 4.830061682197723. The k is 13.223478137256826, and b is -60.887136339695644\n",
      "This is the 566 time's iteration. The loss is 4.829852063345534. The k is 13.222487062286898, and b is -60.88083199485549\n",
      "This is the 567 time's iteration. The loss is 4.8296424949008685. The k is 13.221496225643012, and b is -60.874529166035416\n",
      "This is the 568 time's iteration. The loss is 4.829432976851601. The k is 13.220505627267856, and b is -60.86822785287085\n",
      "This is the 569 time's iteration. The loss is 4.829223509185614. The k is 13.219515267104134, and b is -60.86192805499733\n",
      "This is the 570 time's iteration. The loss is 4.829014091890789. The k is 13.218525145094562, and b is -60.855629772050456\n",
      "This is the 571 time's iteration. The loss is 4.828804724955019. The k is 13.21753526118187, and b is -60.84933300366594\n",
      "This is the 572 time's iteration. The loss is 4.828595408366183. The k is 13.216545615308805, and b is -60.84303774947957\n",
      "This is the 573 time's iteration. The loss is 4.828386142112187. The k is 13.215556207418121, and b is -60.83674400912723\n",
      "This is the 574 time's iteration. The loss is 4.828176926180911. The k is 13.214567037452593, and b is -60.83045178224488\n",
      "This is the 575 time's iteration. The loss is 4.827967760560278. The k is 13.213578105355007, and b is -60.82416106846856\n",
      "This is the 576 time's iteration. The loss is 4.827758645238159. The k is 13.212589411068159, and b is -60.817871867434434\n",
      "This is the 577 time's iteration. The loss is 4.827549580202491. The k is 13.211600954534864, and b is -60.81158417877871\n",
      "This is the 578 time's iteration. The loss is 4.827340565441159. The k is 13.21061273569795, and b is -60.80529800213771\n",
      "This is the 579 time's iteration. The loss is 4.8271316009420735. The k is 13.209624754500258, and b is -60.799013337147834\n",
      "This is the 580 time's iteration. The loss is 4.826922686693166. The k is 13.208637010884638, and b is -60.79273018344558\n",
      "This is the 581 time's iteration. The loss is 4.826713822682334. The k is 13.207649504793963, and b is -60.786448540667514\n",
      "This is the 582 time's iteration. The loss is 4.826505008897498. The k is 13.206662236171113, and b is -60.78016840845031\n",
      "This is the 583 time's iteration. The loss is 4.826296245326595. The k is 13.205675204958983, and b is -60.77388978643071\n",
      "This is the 584 time's iteration. The loss is 4.826087531957531. The k is 13.20468841110048, and b is -60.76761267424556\n",
      "This is the 585 time's iteration. The loss is 4.825878868778253. The k is 13.203701854538533, and b is -60.76133707153179\n",
      "This is the 586 time's iteration. The loss is 4.825670255776676. The k is 13.202715535216075, and b is -60.75506297792641\n",
      "This is the 587 time's iteration. The loss is 4.825461692940748. The k is 13.201729453076059, and b is -60.748790393066514\n",
      "This is the 588 time's iteration. The loss is 4.825253180258386. The k is 13.200743608061444, and b is -60.7425193165893\n",
      "This is the 589 time's iteration. The loss is 4.825044717717543. The k is 13.199758000115212, and b is -60.736249748132046\n",
      "This is the 590 time's iteration. The loss is 4.824836305306167. The k is 13.198772629180356, and b is -60.729981687332106\n",
      "This is the 591 time's iteration. The loss is 4.8246279430121914. The k is 13.197787495199876, and b is -60.723715133826936\n",
      "This is the 592 time's iteration. The loss is 4.824419630823564. The k is 13.196802598116795, and b is -60.71745008725407\n",
      "This is the 593 time's iteration. The loss is 4.824211368728247. The k is 13.195817937874146, and b is -60.71118654725114\n",
      "This is the 594 time's iteration. The loss is 4.824003156714178. The k is 13.194833514414974, and b is -60.70492451345585\n",
      "This is the 595 time's iteration. The loss is 4.823794994769326. The k is 13.19384932768234, and b is -60.698663985506\n",
      "This is the 596 time's iteration. The loss is 4.823586882881656. The k is 13.192865377619317, and b is -60.69240496303948\n",
      "This is the 597 time's iteration. The loss is 4.823378821039119. The k is 13.191881664168992, and b is -60.68614744569426\n",
      "This is the 598 time's iteration. The loss is 4.8231708092296826. The k is 13.190898187274469, and b is -60.6798914331084\n",
      "This is the 599 time's iteration. The loss is 4.82296284744131. The k is 13.18991494687886, and b is -60.67363692492005\n",
      "This is the 600 time's iteration. The loss is 4.822754935661997. The k is 13.188931942925294, and b is -60.667383920767435\n",
      "This is the 601 time's iteration. The loss is 4.8225470738796865. The k is 13.187949175356914, and b is -60.66113242028889\n",
      "This is the 602 time's iteration. The loss is 4.822339262082372. The k is 13.186966644116877, and b is -60.65488242312281\n",
      "This is the 603 time's iteration. The loss is 4.822131500258029. The k is 13.18598434914835, and b is -60.64863392890769\n",
      "This is the 604 time's iteration. The loss is 4.8219237883946455. The k is 13.185002290394518, and b is -60.642386937282126\n",
      "This is the 605 time's iteration. The loss is 4.821716126480201. The k is 13.184020467798577, and b is -60.636141447884775\n",
      "This is the 606 time's iteration. The loss is 4.821508514502691. The k is 13.183038881303737, and b is -60.6298974603544\n",
      "This is the 607 time's iteration. The loss is 4.821300952450102. The k is 13.182057530853225, and b is -60.62365497432984\n",
      "This is the 608 time's iteration. The loss is 4.821093440310427. The k is 13.181076416390278, and b is -60.61741398945002\n",
      "This is the 609 time's iteration. The loss is 4.820885978071666. The k is 13.180095537858145, and b is -60.611174505353965\n",
      "This is the 610 time's iteration. The loss is 4.820678565721817. The k is 13.179114895200094, and b is -60.60493652168077\n",
      "This is the 611 time's iteration. The loss is 4.8204712032488946. The k is 13.178134488359401, and b is -60.59870003806963\n",
      "This is the 612 time's iteration. The loss is 4.82026389064089. The k is 13.177154317279362, and b is -60.592465054159824\n",
      "This is the 613 time's iteration. The loss is 4.820056627885821. The k is 13.17617438190328, and b is -60.58623156959071\n",
      "This is the 614 time's iteration. The loss is 4.819849414971692. The k is 13.175194682174476, and b is -60.57999958400174\n",
      "This is the 615 time's iteration. The loss is 4.8196422518865205. The k is 13.174215218036284, and b is -60.573769097032454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 616 time's iteration. The loss is 4.819435138618332. The k is 13.173235989432051, and b is -60.56754010832247\n",
      "This is the 617 time's iteration. The loss is 4.81922807515513. The k is 13.172256996305137, and b is -60.5613126175115\n",
      "This is the 618 time's iteration. The loss is 4.819021061484949. The k is 13.171278238598916, and b is -60.55508662423934\n",
      "This is the 619 time's iteration. The loss is 4.818814097595817. The k is 13.170299716256775, and b is -60.54886212814588\n",
      "This is the 620 time's iteration. The loss is 4.81860718347576. The k is 13.169321429222117, and b is -60.54263912887108\n",
      "This is the 621 time's iteration. The loss is 4.81840031911281. The k is 13.168343377438358, and b is -60.536417626055005\n",
      "This is the 622 time's iteration. The loss is 4.8181935044949995. The k is 13.167365560848923, and b is -60.5301976193378\n",
      "This is the 623 time's iteration. The loss is 4.817986739610373. The k is 13.166387979397259, and b is -60.52397910835968\n",
      "This is the 624 time's iteration. The loss is 4.817780024446959. The k is 13.165410633026818, and b is -60.517762092760975\n",
      "This is the 625 time's iteration. The loss is 4.817573358992814. The k is 13.16443352168107, and b is -60.511546572182084\n",
      "This is the 626 time's iteration. The loss is 4.8173667432359775. The k is 13.163456645303501, and b is -60.505332546263496\n",
      "This is the 627 time's iteration. The loss is 4.8171601771645. The k is 13.162480003837606, and b is -60.49912001464578\n",
      "This is the 628 time's iteration. The loss is 4.816953660766433. The k is 13.161503597226893, and b is -60.492908976969616\n",
      "This is the 629 time's iteration. The loss is 4.816747194029834. The k is 13.16052742541489, and b is -60.48669943287574\n",
      "This is the 630 time's iteration. The loss is 4.81654077694275. The k is 13.159551488345132, and b is -60.480491382004985\n",
      "This is the 631 time's iteration. The loss is 4.816334409493264. The k is 13.158575785961169, and b is -60.474284823998275\n",
      "This is the 632 time's iteration. The loss is 4.816128091669415. The k is 13.157600318206569, and b is -60.468079758496614\n",
      "This is the 633 time's iteration. The loss is 4.815921823459282. The k is 13.156625085024906, and b is -60.4618761851411\n",
      "This is the 634 time's iteration. The loss is 4.815715604850934. The k is 13.155650086359774, and b is -60.45567410357292\n",
      "This is the 635 time's iteration. The loss is 4.8155094358324355. The k is 13.154675322154777, and b is -60.449473513433325\n",
      "This is the 636 time's iteration. The loss is 4.815303316391875. The k is 13.153700792353536, and b is -60.44327441436368\n",
      "This is the 637 time's iteration. The loss is 4.815097672493727. The k is 13.152726496899682, and b is -60.43707680600542\n",
      "This is the 638 time's iteration. The loss is 4.814894512301698. The k is 13.151752435736862, and b is -60.43088068800007\n",
      "This is the 639 time's iteration. The loss is 4.814691400964071. The k is 13.150778608808734, and b is -60.42468605998925\n",
      "This is the 640 time's iteration. The loss is 4.814488338469079. The k is 13.149805016058972, and b is -60.41849292161464\n",
      "This is the 641 time's iteration. The loss is 4.814285324804984. The k is 13.148831657431261, and b is -60.412301272518036\n",
      "This is the 642 time's iteration. The loss is 4.814082359960046. The k is 13.147858532869304, and b is -60.40611111234131\n",
      "This is the 643 time's iteration. The loss is 4.813879443922529. The k is 13.146885642316812, and b is -60.39992244072641\n",
      "This is the 644 time's iteration. The loss is 4.81367657668068. The k is 13.145912985717514, and b is -60.39373525731538\n",
      "This is the 645 time's iteration. The loss is 4.813473758222781. The k is 13.14494056301515, and b is -60.38754956175035\n",
      "This is the 646 time's iteration. The loss is 4.813270988537092. The k is 13.143968374153474, and b is -60.38136535367353\n",
      "This is the 647 time's iteration. The loss is 4.813068267611899. The k is 13.142996419076255, and b is -60.37518263272723\n",
      "This is the 648 time's iteration. The loss is 4.8128655954354596. The k is 13.142024697727273, and b is -60.369001398553834\n",
      "This is the 649 time's iteration. The loss is 4.81266297199606. The k is 13.141053210050323, and b is -60.36282165079581\n",
      "This is the 650 time's iteration. The loss is 4.8124620668702205. The k is 13.140081955989215, and b is -60.356643389095716\n",
      "This is the 651 time's iteration. The loss is 4.812263270977794. The k is 13.139110935487768, and b is -60.350466613096195\n",
      "This is the 652 time's iteration. The loss is 4.812064522890256. The k is 13.13814014848982, and b is -60.34429132243998\n",
      "This is the 653 time's iteration. The loss is 4.811865822596122. The k is 13.137169594939218, and b is -60.33811751676989\n",
      "This is the 654 time's iteration. The loss is 4.811667170083911. The k is 13.136199274779825, and b is -60.33194519572883\n",
      "This is the 655 time's iteration. The loss is 4.811468565342102. The k is 13.135229187955519, and b is -60.32577435895978\n",
      "This is the 656 time's iteration. The loss is 4.811270008359234. The k is 13.134259334410187, and b is -60.31960500610581\n",
      "This is the 657 time's iteration. The loss is 4.811071499123808. The k is 13.13328971408773, and b is -60.31343713681009\n",
      "This is the 658 time's iteration. The loss is 4.81087303762435. The k is 13.132320326932069, and b is -60.30727075071586\n",
      "This is the 659 time's iteration. The loss is 4.810674623849375. The k is 13.131351172887133, and b is -60.301105847466445\n",
      "This is the 660 time's iteration. The loss is 4.810476257787413. The k is 13.13038225189686, and b is -60.294942426705276\n",
      "This is the 661 time's iteration. The loss is 4.810277939426994. The k is 13.129413563905214, and b is -60.288780488075844\n",
      "This is the 662 time's iteration. The loss is 4.810079668756634. The k is 13.128445108856159, and b is -60.28262003122175\n",
      "This is the 663 time's iteration. The loss is 4.809881445764868. The k is 13.127476886693684, and b is -60.276461055786655\n",
      "This is the 664 time's iteration. The loss is 4.8096832704402335. The k is 13.126508897361784, and b is -60.27030356141432\n",
      "This is the 665 time's iteration. The loss is 4.809485142771268. The k is 13.125541140804469, and b is -60.2641475477486\n",
      "This is the 666 time's iteration. The loss is 4.809287062746517. The k is 13.124573616965764, and b is -60.25799301443342\n",
      "This is the 667 time's iteration. The loss is 4.80908903035451. The k is 13.123606325789705, and b is -60.25183996111279\n",
      "This is the 668 time's iteration. The loss is 4.808891045583805. The k is 13.122639267220347, and b is -60.24568838743083\n",
      "This is the 669 time's iteration. The loss is 4.808693108422945. The k is 13.121672441201751, and b is -60.239538293031714\n",
      "This is the 670 time's iteration. The loss is 4.808495218860482. The k is 13.120705847677996, and b is -60.23338967755972\n",
      "This is the 671 time's iteration. The loss is 4.808297376884965. The k is 13.119739486593176, and b is -60.227242540659205\n",
      "This is the 672 time's iteration. The loss is 4.808099582484969. The k is 13.11877335789139, and b is -60.22109688197462\n",
      "This is the 673 time's iteration. The loss is 4.807901835649032. The k is 13.117807461516762, and b is -60.21495270115048\n",
      "This is the 674 time's iteration. The loss is 4.807704136365725. The k is 13.11684179741342, and b is -60.208809997831416\n",
      "This is the 675 time's iteration. The loss is 4.807506484623614. The k is 13.115876365525512, and b is -60.202668771662125\n",
      "This is the 676 time's iteration. The loss is 4.80730888041126. The k is 13.114911165797196, and b is -60.19652902228739\n",
      "This is the 677 time's iteration. The loss is 4.807111323717235. The k is 13.113946198172643, and b is -60.19039074935209\n",
      "This is the 678 time's iteration. The loss is 4.806913814530126. The k is 13.11298146259604, and b is -60.18425395250117\n",
      "This is the 679 time's iteration. The loss is 4.806716352838499. The k is 13.112016959011582, and b is -60.17811863137969\n",
      "This is the 680 time's iteration. The loss is 4.806518938630919. The k is 13.111052687363486, and b is -60.17198478563276\n",
      "This is the 681 time's iteration. The loss is 4.806321571895992. The k is 13.110088647595976, and b is -60.1658524149056\n",
      "This is the 682 time's iteration. The loss is 4.806124252622283. The k is 13.10912483965329, and b is -60.15972151884351\n",
      "This is the 683 time's iteration. The loss is 4.805926980798392. The k is 13.108161263479682, and b is -60.153592097091874\n",
      "This is the 684 time's iteration. The loss is 4.805729756412899. The k is 13.107197919019418, and b is -60.14746414929616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 685 time's iteration. The loss is 4.805532579454403. The k is 13.106234806216776, and b is -60.14133767510193\n",
      "This is the 686 time's iteration. The loss is 4.805335449911496. The k is 13.105271925016051, and b is -60.135212674154815\n",
      "This is the 687 time's iteration. The loss is 4.805138367772772. The k is 13.104309275361548, and b is -60.12908914610054\n",
      "This is the 688 time's iteration. The loss is 4.804941333026844. The k is 13.103346857197586, and b is -60.122967090584915\n",
      "This is the 689 time's iteration. The loss is 4.8047443456623045. The k is 13.102384670468497, and b is -60.11684650725384\n",
      "This is the 690 time's iteration. The loss is 4.804547405667761. The k is 13.10142271511863, and b is -60.110727395753294\n",
      "This is the 691 time's iteration. The loss is 4.804350513031823. The k is 13.100460991092344, and b is -60.10460975572934\n",
      "This is the 692 time's iteration. The loss is 4.8041536677431065. The k is 13.09949949833401, and b is -60.098493586828134\n",
      "This is the 693 time's iteration. The loss is 4.80395686979022. The k is 13.098538236788018, and b is -60.092378888695904\n",
      "This is the 694 time's iteration. The loss is 4.80376011916178. The k is 13.097577206398764, and b is -60.086265660978974\n",
      "This is the 695 time's iteration. The loss is 4.803563415846409. The k is 13.096616407110664, and b is -60.080153903323755\n",
      "This is the 696 time's iteration. The loss is 4.8033667598327305. The k is 13.095655838868144, and b is -60.07404361537673\n",
      "This is the 697 time's iteration. The loss is 4.803170151109376. The k is 13.094695501615643, and b is -60.06793479678448\n",
      "This is the 698 time's iteration. The loss is 4.802973589664958. The k is 13.093735395297614, and b is -60.061827447193664\n",
      "This is the 699 time's iteration. The loss is 4.80277707548812. The k is 13.092775519858526, and b is -60.05572156625103\n",
      "This is the 700 time's iteration. The loss is 4.802580608567487. The k is 13.091815875242858, and b is -60.04961715360341\n",
      "This is the 701 time's iteration. The loss is 4.8023841888917. The k is 13.090856461395102, and b is -60.04351420889771\n",
      "This is the 702 time's iteration. The loss is 4.802187816449398. The k is 13.089897278259766, and b is -60.03741273178094\n",
      "This is the 703 time's iteration. The loss is 4.80199149122922. The k is 13.088938325781369, and b is -60.031312721900186\n",
      "This is the 704 time's iteration. The loss is 4.801795213219815. The k is 13.087979603904445, and b is -60.025214178902615\n",
      "This is the 705 time's iteration. The loss is 4.801598982409829. The k is 13.087021112573542, and b is -60.01911710243548\n",
      "This is the 706 time's iteration. The loss is 4.8014027987879. The k is 13.086062851733217, and b is -60.01302149214612\n",
      "This is the 707 time's iteration. The loss is 4.801206662342704. The k is 13.085104821328047, and b is -60.00692734768197\n",
      "This is the 708 time's iteration. The loss is 4.801010573062872. The k is 13.084147021302616, and b is -60.00083466869054\n",
      "This is the 709 time's iteration. The loss is 4.800814530937078. The k is 13.083189451601527, and b is -59.99474345481941\n",
      "This is the 710 time's iteration. The loss is 4.800618535953982. The k is 13.08223211216939, and b is -59.98865370571627\n",
      "This is the 711 time's iteration. The loss is 4.800422588102231. The k is 13.081275002950834, and b is -59.98256542102888\n",
      "This is the 712 time's iteration. The loss is 4.800226687370521. The k is 13.080318123890498, and b is -59.97647860040509\n",
      "This is the 713 time's iteration. The loss is 4.80003083374749. The k is 13.079361474933036, and b is -59.97039324349283\n",
      "This is the 714 time's iteration. The loss is 4.79983502722182. The k is 13.078405056023113, and b is -59.96430934994012\n",
      "This is the 715 time's iteration. The loss is 4.799639267782198. The k is 13.07744886710541, and b is -59.958226919395074\n",
      "This is the 716 time's iteration. The loss is 4.799443555417289. The k is 13.07649290812462, and b is -59.95214595150586\n",
      "This is the 717 time's iteration. The loss is 4.799247890115777. The k is 13.075537179025453, and b is -59.94606644592076\n",
      "This is the 718 time's iteration. The loss is 4.799052271866337. The k is 13.074581679752624, and b is -59.93998840228814\n",
      "This is the 719 time's iteration. The loss is 4.798856700657662. The k is 13.073626410250869, and b is -59.93391182025642\n",
      "This is the 720 time's iteration. The loss is 4.798661176478449. The k is 13.072671370464931, and b is -59.927836699474135\n",
      "This is the 721 time's iteration. The loss is 4.798465699317366. The k is 13.071716560339574, and b is -59.9217630395899\n",
      "This is the 722 time's iteration. The loss is 4.79827026916312. The k is 13.070761979819569, and b is -59.9156908402524\n",
      "This is the 723 time's iteration. The loss is 4.798074886004405. The k is 13.069807628849702, and b is -59.90962010111043\n",
      "This is the 724 time's iteration. The loss is 4.797879549829923. The k is 13.068853507374772, and b is -59.903550821812836\n",
      "This is the 725 time's iteration. The loss is 4.797684260628375. The k is 13.067899615339593, and b is -59.89748300200857\n",
      "This is the 726 time's iteration. The loss is 4.797489018388459. The k is 13.066945952688993, and b is -59.89141664134667\n",
      "This is the 727 time's iteration. The loss is 4.79729382309889. The k is 13.065992519367807, and b is -59.885351739476256\n",
      "This is the 728 time's iteration. The loss is 4.797098674748373. The k is 13.065039315320892, and b is -59.87928829604652\n",
      "This is the 729 time's iteration. The loss is 4.79690357332562. The k is 13.06408634049311, and b is -59.87322631070675\n",
      "This is the 730 time's iteration. The loss is 4.796708518819346. The k is 13.063133594829344, and b is -59.867165783106316\n",
      "This is the 731 time's iteration. The loss is 4.796513511218276. The k is 13.062181078274483, and b is -59.861106712894674\n",
      "This is the 732 time's iteration. The loss is 4.7963185505111205. The k is 13.061228790773434, and b is -59.855049099721356\n",
      "This is the 733 time's iteration. The loss is 4.796123636686609. The k is 13.060276732271117, and b is -59.84899294323599\n",
      "This is the 734 time's iteration. The loss is 4.795928769733469. The k is 13.059324902712461, and b is -59.842938243088284\n",
      "This is the 735 time's iteration. The loss is 4.795733949640421. The k is 13.058373302042414, and b is -59.83688499892803\n",
      "This is the 736 time's iteration. The loss is 4.7955391763962085. The k is 13.057421930205935, and b is -59.830833210405096\n",
      "This is the 737 time's iteration. The loss is 4.795344449989552. The k is 13.056470787147994, and b is -59.82478287716945\n",
      "This is the 738 time's iteration. The loss is 4.795149770409199. The k is 13.055519872813578, and b is -59.81873399887113\n",
      "This is the 739 time's iteration. The loss is 4.7949551376438855. The k is 13.054569187147685, and b is -59.81268657516027\n",
      "This is the 740 time's iteration. The loss is 4.794760551682353. The k is 13.053618730095325, and b is -59.80664060568708\n",
      "This is the 741 time's iteration. The loss is 4.7945660125133465. The k is 13.052668501601524, and b is -59.80059609010185\n",
      "This is the 742 time's iteration. The loss is 4.794371520125618. The k is 13.051718501611319, and b is -59.79455302805496\n",
      "This is the 743 time's iteration. The loss is 4.794177074507912. The k is 13.050768730069763, and b is -59.78851141919689\n",
      "This is the 744 time's iteration. The loss is 4.793982675648982. The k is 13.04981918692192, and b is -59.782471263178174\n",
      "This is the 745 time's iteration. The loss is 4.793788323537588. The k is 13.048869872112865, and b is -59.77643255964945\n",
      "This is the 746 time's iteration. The loss is 4.793594018162482. The k is 13.047920785587694, and b is -59.77039530826143\n",
      "This is the 747 time's iteration. The loss is 4.793399759512434. The k is 13.046971927291505, and b is -59.76435950866492\n",
      "This is the 748 time's iteration. The loss is 4.793205547576198. The k is 13.04602329716942, and b is -59.75832516051079\n",
      "This is the 749 time's iteration. The loss is 4.7930113823425495. The k is 13.045074895166568, and b is -59.75229226345003\n",
      "This is the 750 time's iteration. The loss is 4.792817263800255. The k is 13.044126721228091, and b is -59.74626081713368\n",
      "This is the 751 time's iteration. The loss is 4.7926231919380875. The k is 13.043178775299149, and b is -59.740230821212876\n",
      "This is the 752 time's iteration. The loss is 4.792429166744818. The k is 13.042231057324909, and b is -59.734202275338845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 753 time's iteration. The loss is 4.792235188209221. The k is 13.041283567250556, and b is -59.72817517916288\n",
      "This is the 754 time's iteration. The loss is 4.792041256320091. The k is 13.040336305021285, and b is -59.72214953233638\n",
      "This is the 755 time's iteration. The loss is 4.791847978620749. The k is 13.039389270582307, and b is -59.716125334510814\n",
      "This is the 756 time's iteration. The loss is 4.791655509238005. The k is 13.038442463878845, and b is -59.71010258533773\n",
      "This is the 757 time's iteration. The loss is 4.791463086138805. The k is 13.037495884856133, and b is -59.70408128446877\n",
      "This is the 758 time's iteration. The loss is 4.791270709312019. The k is 13.036549533459421, and b is -59.698061431555665\n",
      "This is the 759 time's iteration. The loss is 4.791078378746528. The k is 13.035603409633971, and b is -59.69204302625022\n",
      "This is the 760 time's iteration. The loss is 4.7908860944312. The k is 13.034657513325062, and b is -59.68602606820431\n",
      "This is the 761 time's iteration. The loss is 4.790693856354917. The k is 13.033711844477976, and b is -59.680010557069934\n",
      "This is the 762 time's iteration. The loss is 4.790501664506563. The k is 13.032766403038021, and b is -59.67399649249913\n",
      "This is the 763 time's iteration. The loss is 4.790309518875013. The k is 13.031821188950508, and b is -59.66798387414405\n",
      "This is the 764 time's iteration. The loss is 4.790117419449163. The k is 13.030876202160766, and b is -59.661972701656914\n",
      "This is the 765 time's iteration. The loss is 4.789925366217895. The k is 13.029931442614137, and b is -59.655962974690034\n",
      "This is the 766 time's iteration. The loss is 4.789733359170098. The k is 13.028986910255973, and b is -59.6499546928958\n",
      "This is the 767 time's iteration. The loss is 4.789541398294673. The k is 13.028042605031645, and b is -59.64394785592668\n",
      "This is the 768 time's iteration. The loss is 4.789349483580519. The k is 13.027098526886531, and b is -59.63794246343525\n",
      "This is the 769 time's iteration. The loss is 4.789157615016529. The k is 13.026154675766024, and b is -59.63193851507415\n",
      "This is the 770 time's iteration. The loss is 4.788965792591597. The k is 13.025211051615535, and b is -59.6259360104961\n",
      "This is the 771 time's iteration. The loss is 4.788774016294652. The k is 13.02426765438048, and b is -59.61993494935391\n",
      "This is the 772 time's iteration. The loss is 4.788582286114582. The k is 13.023324484006295, and b is -59.61393533130048\n",
      "This is the 773 time's iteration. The loss is 4.788390602040301. The k is 13.022381540438424, and b is -59.607937155988786\n",
      "This is the 774 time's iteration. The loss is 4.788198964060719. The k is 13.021438823622328, and b is -59.60194042307188\n",
      "This is the 775 time's iteration. The loss is 4.788007372164763. The k is 13.020496333503477, and b is -59.595945132202914\n",
      "This is the 776 time's iteration. The loss is 4.7878158263413395. The k is 13.019554070027358, and b is -59.58995128303511\n",
      "This is the 777 time's iteration. The loss is 4.787624326579369. The k is 13.01861203313947, and b is -59.58395887522179\n",
      "This is the 778 time's iteration. The loss is 4.78743386522183. The k is 13.017670222785325, and b is -59.57796790841634\n",
      "This is the 779 time's iteration. The loss is 4.78724418112446. The k is 13.016728638910449, and b is -59.57197838227224\n",
      "This is the 780 time's iteration. The loss is 4.787054542640853. The k is 13.015787281460378, and b is -59.56599029644305\n",
      "This is the 781 time's iteration. The loss is 4.786864949760032. The k is 13.014846150380665, and b is -59.56000365058241\n",
      "This is the 782 time's iteration. The loss is 4.786675402471052. The k is 13.013905245616872, and b is -59.55401844434405\n",
      "This is the 783 time's iteration. The loss is 4.7864859007629255. The k is 13.012964567114578, and b is -59.548034677381786\n",
      "This is the 784 time's iteration. The loss is 4.786296444624712. The k is 13.01202411481937, and b is -59.542052349349504\n",
      "This is the 785 time's iteration. The loss is 4.786107034045447. The k is 13.011083888676856, and b is -59.53607145990119\n",
      "This is the 786 time's iteration. The loss is 4.785917669014163. The k is 13.010143888632651, and b is -59.53009200869089\n",
      "This is the 787 time's iteration. The loss is 4.785728349519925. The k is 13.009204114632384, and b is -59.524113995372765\n",
      "This is the 788 time's iteration. The loss is 4.7855390755517675. The k is 13.008264566621698, and b is -59.51813741960103\n",
      "This is the 789 time's iteration. The loss is 4.785349847098761. The k is 13.00732524454625, and b is -59.51216228103001\n",
      "This is the 790 time's iteration. The loss is 4.785160664149943. The k is 13.006386148351707, and b is -59.50618857931408\n",
      "This is the 791 time's iteration. The loss is 4.784971526694374. The k is 13.005447277983752, and b is -59.500216314107725\n",
      "This is the 792 time's iteration. The loss is 4.784782434721118. The k is 13.00450863338808, and b is -59.494245485065505\n",
      "This is the 793 time's iteration. The loss is 4.784593388219239. The k is 13.003570214510397, and b is -59.48827609184206\n",
      "This is the 794 time's iteration. The loss is 4.784404387177802. The k is 13.002632021296426, and b is -59.482308134092115\n",
      "This is the 795 time's iteration. The loss is 4.78421543158587. The k is 13.001694053691903, and b is -59.476341611470474\n",
      "This is the 796 time's iteration. The loss is 4.784026521432517. The k is 13.00075631164257, and b is -59.470376523632034\n",
      "This is the 797 time's iteration. The loss is 4.783837656706816. The k is 12.99981879509419, and b is -59.46441287023177\n",
      "This is the 798 time's iteration. The loss is 4.783648837397844. The k is 12.99888150399254, and b is -59.45845065092474\n",
      "This is the 799 time's iteration. The loss is 4.783460063494673. The k is 12.997944438283401, and b is -59.45248986536608\n",
      "This is the 800 time's iteration. The loss is 4.783271334986402. The k is 12.997007597912575, and b is -59.44653051321102\n",
      "This is the 801 time's iteration. The loss is 4.783082651862095. The k is 12.996070982825872, and b is -59.44057259411486\n",
      "This is the 802 time's iteration. The loss is 4.782894014110845. The k is 12.99513459296912, and b is -59.43461610773299\n",
      "This is the 803 time's iteration. The loss is 4.782705421721739. The k is 12.994198428288156, and b is -59.42866105372089\n",
      "This is the 804 time's iteration. The loss is 4.782517275893561. The k is 12.993262488728831, and b is -59.422707431734096\n",
      "This is the 805 time's iteration. The loss is 4.7823318850510175. The k is 12.99232677423701, and b is -59.41675524142826\n",
      "This is the 806 time's iteration. The loss is 4.782146538789826. The k is 12.991391284758572, and b is -59.410804482459106\n",
      "This is the 807 time's iteration. The loss is 4.781961237099272. The k is 12.990456020239408, and b is -59.404855154482426\n",
      "This is the 808 time's iteration. The loss is 4.781775979968634. The k is 12.989520980625418, and b is -59.398907257154114\n",
      "This is the 809 time's iteration. The loss is 4.781590767387195. The k is 12.988586165862522, and b is -59.39296079013014\n",
      "This is the 810 time's iteration. The loss is 4.781405599344244. The k is 12.987651575896647, and b is -59.38701575306655\n",
      "This is the 811 time's iteration. The loss is 4.781220475829082. The k is 12.986717210673737, and b is -59.38107214561948\n",
      "This is the 812 time's iteration. The loss is 4.781035396830985. The k is 12.985783070139748, and b is -59.37512996744514\n",
      "This is the 813 time's iteration. The loss is 4.780850362339249. The k is 12.984849154240646, and b is -59.36918921819984\n",
      "This is the 814 time's iteration. The loss is 4.780665372343174. The k is 12.983915462922415, and b is -59.36324989753996\n",
      "This is the 815 time's iteration. The loss is 4.78048042683207. The k is 12.982981996131048, and b is -59.35731200512196\n",
      "This is the 816 time's iteration. The loss is 4.780295525795234. The k is 12.982048753812554, and b is -59.35137554060239\n",
      "This is the 817 time's iteration. The loss is 4.7801106692219655. The k is 12.981115735912953, and b is -59.34544050363788\n",
      "This is the 818 time's iteration. The loss is 4.779925857101563. The k is 12.980182942378278, and b is -59.339506893885144\n",
      "This is the 819 time's iteration. The loss is 4.779741089423369. The k is 12.979250373154574, and b is -59.33357471100098\n",
      "This is the 820 time's iteration. The loss is 4.7795563661766645. The k is 12.978318028187903, and b is -59.327643954642255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 821 time's iteration. The loss is 4.77937168735078. The k is 12.977385907424338, and b is -59.32171462446594\n",
      "This is the 822 time's iteration. The loss is 4.779187052935034. The k is 12.976454010809963, and b is -59.315786720129076\n",
      "This is the 823 time's iteration. The loss is 4.779002462918741. The k is 12.975522338290874, and b is -59.30986024128878\n",
      "This is the 824 time's iteration. The loss is 4.77881791729123. The k is 12.974590889813186, and b is -59.30393518760227\n",
      "This is the 825 time's iteration. The loss is 4.778633416041821. The k is 12.973659665323021, and b is -59.298011558726834\n",
      "This is the 826 time's iteration. The loss is 4.778448959159848. The k is 12.97272866476652, and b is -59.29208935431984\n",
      "This is the 827 time's iteration. The loss is 4.778264546634628. The k is 12.971797888089828, and b is -59.28616857403875\n",
      "This is the 828 time's iteration. The loss is 4.7780801784555145. The k is 12.97086733523911, and b is -59.28024921754109\n",
      "This is the 829 time's iteration. The loss is 4.777895854611836. The k is 12.969937006160542, and b is -59.27433128448449\n",
      "This is the 830 time's iteration. The loss is 4.777711575092931. The k is 12.969006900800315, and b is -59.26841477452665\n",
      "This is the 831 time's iteration. The loss is 4.777527339888134. The k is 12.96807701910463, and b is -59.262499687325345\n",
      "This is the 832 time's iteration. The loss is 4.777343148986793. The k is 12.9671473610197, and b is -59.25658602253845\n",
      "This is the 833 time's iteration. The loss is 4.7771590023782595. The k is 12.966217926491753, and b is -59.25067377982391\n",
      "This is the 834 time's iteration. The loss is 4.776974900051872. The k is 12.96528871546703, and b is -59.244762958839765\n",
      "This is the 835 time's iteration. The loss is 4.77679084199699. The k is 12.964359727891788, and b is -59.23885355924412\n",
      "This is the 836 time's iteration. The loss is 4.776606828202967. The k is 12.963430963712291, and b is -59.23294558069517\n",
      "This is the 837 time's iteration. The loss is 4.776422858659155. The k is 12.962502422874818, and b is -59.227039022851194\n",
      "This is the 838 time's iteration. The loss is 4.776238933354918. The k is 12.961574105325662, and b is -59.221133885370556\n",
      "This is the 839 time's iteration. The loss is 4.776055052279616. The k is 12.960646011011127, and b is -59.21523016791169\n",
      "This is the 840 time's iteration. The loss is 4.775871215422615. The k is 12.959718139877534, and b is -59.20932787013313\n",
      "This is the 841 time's iteration. The loss is 4.775687422773269. The k is 12.958790491871213, and b is -59.203426991693476\n",
      "This is the 842 time's iteration. The loss is 4.775503674320963. The k is 12.957863066938506, and b is -59.197527532251414\n",
      "This is the 843 time's iteration. The loss is 4.775319970055066. The k is 12.956935865025773, and b is -59.19162949146572\n",
      "This is the 844 time's iteration. The loss is 4.775136309964947. The k is 12.956008886079383, and b is -59.18573286899524\n",
      "This is the 845 time's iteration. The loss is 4.774952694039985. The k is 12.955082130045717, and b is -59.17983766449891\n",
      "This is the 846 time's iteration. The loss is 4.7747691222695625. The k is 12.954155596871175, and b is -59.17394387763576\n",
      "This is the 847 time's iteration. The loss is 4.774585594643057. The k is 12.953229286502161, and b is -59.16805150806487\n",
      "This is the 848 time's iteration. The loss is 4.774402111149856. The k is 12.952303198885097, and b is -59.16216055544543\n",
      "This is the 849 time's iteration. The loss is 4.774218671779347. The k is 12.95137733396642, and b is -59.1562710194367\n",
      "This is the 850 time's iteration. The loss is 4.774035276520911. The k is 12.950451691692576, and b is -59.15038289969803\n",
      "This is the 851 time's iteration. The loss is 4.773851925363958. The k is 12.949526272010024, and b is -59.14449619588884\n",
      "This is the 852 time's iteration. The loss is 4.773668618297869. The k is 12.94860107486524, and b is -59.13861090766864\n",
      "This is the 853 time's iteration. The loss is 4.773485355312044. The k is 12.947676100204704, and b is -59.13272703469702\n",
      "This is the 854 time's iteration. The loss is 4.773302136395886. The k is 12.94675134797492, and b is -59.12684457663365\n",
      "This is the 855 time's iteration. The loss is 4.773118961538796. The k is 12.945826818122399, and b is -59.12096353313829\n",
      "This is the 856 time's iteration. The loss is 4.772935830730175. The k is 12.944902510593664, and b is -59.11508390387077\n",
      "This is the 857 time's iteration. The loss is 4.772752743959438. The k is 12.943978425335253, and b is -59.10920568849101\n",
      "This is the 858 time's iteration. The loss is 4.772569701215985. The k is 12.943054562293716, and b is -59.10332888665901\n",
      "This is the 859 time's iteration. The loss is 4.772386702489237. The k is 12.942130921415615, and b is -59.09745349803484\n",
      "This is the 860 time's iteration. The loss is 4.772203747768613. The k is 12.941207502647526, and b is -59.09157952227867\n",
      "This is the 861 time's iteration. The loss is 4.77202083704352. The k is 12.940284305936038, and b is -59.08570695905075\n",
      "This is the 862 time's iteration. The loss is 4.771837970303387. The k is 12.939361331227754, and b is -59.07983580801141\n",
      "This is the 863 time's iteration. The loss is 4.771655147537627. The k is 12.938438578469286, and b is -59.07396606882104\n",
      "This is the 864 time's iteration. The loss is 4.771472368735673. The k is 12.937516047607264, and b is -59.06809774114015\n",
      "This is the 865 time's iteration. The loss is 4.771289633886958. The k is 12.936593738588327, and b is -59.062230824629296\n",
      "This is the 866 time's iteration. The loss is 4.771106942980898. The k is 12.935671651359126, and b is -59.05636531894913\n",
      "This is the 867 time's iteration. The loss is 4.770924296006934. The k is 12.93474978586633, and b is -59.0505012237604\n",
      "This is the 868 time's iteration. The loss is 4.7707416929545055. The k is 12.933828142056614, and b is -59.04463853872391\n",
      "This is the 869 time's iteration. The loss is 4.770559133813046. The k is 12.932906719876671, and b is -59.03877726350056\n",
      "This is the 870 time's iteration. The loss is 4.770376618571991. The k is 12.931985519273207, and b is -59.03291739775133\n",
      "This is the 871 time's iteration. The loss is 4.770194147220796. The k is 12.931064540192937, and b is -59.027058941137284\n",
      "This is the 872 time's iteration. The loss is 4.770011719748896. The k is 12.93014378258259, and b is -59.02120189331956\n",
      "This is the 873 time's iteration. The loss is 4.769829336145745. The k is 12.92922324638891, and b is -59.01534625395938\n",
      "This is the 874 time's iteration. The loss is 4.769646996400786. The k is 12.928302931558653, and b is -59.00949202271805\n",
      "This is the 875 time's iteration. The loss is 4.769464700503486. The k is 12.927382838038586, and b is -59.00363919925696\n",
      "This is the 876 time's iteration. The loss is 4.769282448443287. The k is 12.926462965775489, and b is -58.99778778323758\n",
      "This is the 877 time's iteration. The loss is 4.769100240209658. The k is 12.925543314716158, and b is -58.99193777432145\n",
      "This is the 878 time's iteration. The loss is 4.768918075792056. The k is 12.924623884807401, and b is -58.98608917217021\n",
      "This is the 879 time's iteration. The loss is 4.768735955179941. The k is 12.923704675996033, and b is -58.98024197644557\n",
      "This is the 880 time's iteration. The loss is 4.768553878362782. The k is 12.92278568822889, and b is -58.974396186809315\n",
      "This is the 881 time's iteration. The loss is 4.768371845330046. The k is 12.921866921452816, and b is -58.96855180292333\n",
      "This is the 882 time's iteration. The loss is 4.768189856071202. The k is 12.920948375614667, and b is -58.962708824449564\n",
      "This is the 883 time's iteration. The loss is 4.768007910575739. The k is 12.920030050661314, and b is -58.95686725105006\n",
      "This is the 884 time's iteration. The loss is 4.767826008833113. The k is 12.919111946539644, and b is -58.951027082386936\n",
      "This is the 885 time's iteration. The loss is 4.767644150832814. The k is 12.918194063196548, and b is -58.94518831812239\n",
      "This is the 886 time's iteration. The loss is 4.767462336564317. The k is 12.917276400578938, and b is -58.9393509579187\n",
      "This is the 887 time's iteration. The loss is 4.767280566017109. The k is 12.916358958633735, and b is -58.93351500143823\n",
      "This is the 888 time's iteration. The loss is 4.767098839180678. The k is 12.915441737307873, and b is -58.92768044834343\n",
      "This is the 889 time's iteration. The loss is 4.766917156044506. The k is 12.9145247365483, and b is -58.92184729829682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 890 time's iteration. The loss is 4.766735516598091. The k is 12.913607956301975, and b is -58.916015550961\n",
      "This is the 891 time's iteration. The loss is 4.766553920830929. The k is 12.912691396515871, and b is -58.910185205998665\n",
      "This is the 892 time's iteration. The loss is 4.766372368732509. The k is 12.911775057136973, and b is -58.90435626307258\n",
      "This is the 893 time's iteration. The loss is 4.7661908602923315. The k is 12.91085893811228, and b is -58.898528721845594\n",
      "This is the 894 time's iteration. The loss is 4.766009395499904. The k is 12.909943039388802, and b is -58.892702581980636\n",
      "This is the 895 time's iteration. The loss is 4.765827974344726. The k is 12.909027360913564, and b is -58.886877843140724\n",
      "This is the 896 time's iteration. The loss is 4.7656465968163015. The k is 12.908111902633602, and b is -58.881054504988946\n",
      "This is the 897 time's iteration. The loss is 4.765465262904141. The k is 12.907196664495963, and b is -58.87523256718847\n",
      "This is the 898 time's iteration. The loss is 4.76528397259776. The k is 12.906281646447711, and b is -58.869412029402554\n",
      "This is the 899 time's iteration. The loss is 4.765102725886662. The k is 12.905366848435921, and b is -58.86359289129454\n",
      "This is the 900 time's iteration. The loss is 4.764921522760376. The k is 12.904452270407681, and b is -58.857775152527836\n",
      "This is the 901 time's iteration. The loss is 4.7647403632084195. The k is 12.903537912310089, and b is -58.85195881276594\n",
      "This is the 902 time's iteration. The loss is 4.764559247220307. The k is 12.902623774090257, and b is -58.84614387167244\n",
      "This is the 903 time's iteration. The loss is 4.764378174785565. The k is 12.901709855695312, and b is -58.84033032891098\n",
      "This is the 904 time's iteration. The loss is 4.764197145893723. The k is 12.900796157072394, and b is -58.83451818414531\n",
      "This is the 905 time's iteration. The loss is 4.764016160534306. The k is 12.89988267816865, and b is -58.82870743703925\n",
      "This is the 906 time's iteration. The loss is 4.763835218696846. The k is 12.898969418931248, and b is -58.822898087256696\n",
      "This is the 907 time's iteration. The loss is 4.763654320370887. The k is 12.89805637930736, and b is -58.81709013446164\n",
      "This is the 908 time's iteration. The loss is 4.763473465545952. The k is 12.897143559244178, and b is -58.81128357831813\n",
      "This is the 909 time's iteration. The loss is 4.7632926542115825. The k is 12.896230958688903, and b is -58.805478418490324\n",
      "This is the 910 time's iteration. The loss is 4.763111886357338. The k is 12.895318577588748, and b is -58.799674654642445\n",
      "This is the 911 time's iteration. The loss is 4.762931161972737. The k is 12.894406415890943, and b is -58.79387228643879\n",
      "This is the 912 time's iteration. The loss is 4.7627504810473384. The k is 12.893494473542725, and b is -58.78807131354375\n",
      "This is the 913 time's iteration. The loss is 4.762569843570697. The k is 12.892582750491348, and b is -58.7822717356218\n",
      "This is the 914 time's iteration. The loss is 4.762389249532353. The k is 12.891671246684076, and b is -58.77647355233747\n",
      "This is the 915 time's iteration. The loss is 4.762208698921865. The k is 12.89075996206819, and b is -58.770676763355404\n",
      "This is the 916 time's iteration. The loss is 4.762028191728789. The k is 12.889848896590976, and b is -58.764881368340305\n",
      "This is the 917 time's iteration. The loss is 4.761847727942694. The k is 12.88893805019974, and b is -58.75908736695697\n",
      "This is the 918 time's iteration. The loss is 4.761667307553137. The k is 12.888027422841798, and b is -58.753294758870254\n",
      "This is the 919 time's iteration. The loss is 4.761486930549669. The k is 12.887117014464478, and b is -58.74750354374512\n",
      "This is the 920 time's iteration. The loss is 4.761306596921876. The k is 12.88620682501512, and b is -58.7417137212466\n",
      "This is the 921 time's iteration. The loss is 4.7611263066593095. The k is 12.885296854441082, and b is -58.7359252910398\n",
      "This is the 922 time's iteration. The loss is 4.760946059751554. The k is 12.884387102689727, and b is -58.730138252789914\n",
      "This is the 923 time's iteration. The loss is 4.760765856188187. The k is 12.883477569708434, and b is -58.72435260616221\n",
      "This is the 924 time's iteration. The loss is 4.760585695958774. The k is 12.882568255444596, and b is -58.71856835082205\n",
      "This is the 925 time's iteration. The loss is 4.760405579052899. The k is 12.881659159845617, and b is -58.71278548643487\n",
      "This is the 926 time's iteration. The loss is 4.760225505460141. The k is 12.880750282858914, and b is -58.707004012666175\n",
      "This is the 927 time's iteration. The loss is 4.760045475170095. The k is 12.879841624431918, and b is -58.701223929181566\n",
      "This is the 928 time's iteration. The loss is 4.759865488172337. The k is 12.878933184512071, and b is -58.695445235646716\n",
      "This is the 929 time's iteration. The loss is 4.759685544456453. The k is 12.878024963046828, and b is -58.68966793172738\n",
      "This is the 930 time's iteration. The loss is 4.759505644012049. The k is 12.877116959983656, and b is -58.68389201708939\n",
      "This is the 931 time's iteration. The loss is 4.75932578682871. The k is 12.876209175270038, and b is -58.67811749139867\n",
      "This is the 932 time's iteration. The loss is 4.759145972896031. The k is 12.875301608853462, and b is -58.67234435432122\n",
      "This is the 933 time's iteration. The loss is 4.758966202203622. The k is 12.874394260681438, and b is -58.6665726055231\n",
      "This is the 934 time's iteration. The loss is 4.7587864747410755. The k is 12.873487130701482, and b is -58.660802244670485\n",
      "This is the 935 time's iteration. The loss is 4.7586067904980025. The k is 12.872580218861126, and b is -58.6550332714296\n",
      "This is the 936 time's iteration. The loss is 4.758427149464. The k is 12.871673525107912, and b is -58.64926568546677\n",
      "This is the 937 time's iteration. The loss is 4.7582475516286875. The k is 12.870767049389398, and b is -58.6434994864484\n",
      "This is the 938 time's iteration. The loss is 4.758067996981672. The k is 12.869860791653153, and b is -58.637734674040956\n",
      "This is the 939 time's iteration. The loss is 4.757888485512568. The k is 12.868954751846756, and b is -58.631971247911\n",
      "This is the 940 time's iteration. The loss is 4.757709017210996. The k is 12.868048929917803, and b is -58.62620920772517\n",
      "This is the 941 time's iteration. The loss is 4.757529592066569. The k is 12.867143325813899, and b is -58.62044855315018\n",
      "This is the 942 time's iteration. The loss is 4.757350210068914. The k is 12.866237939482664, and b is -58.61468928385285\n",
      "This is the 943 time's iteration. The loss is 4.757170871207661. The k is 12.86533277087173, and b is -58.60893139950004\n",
      "This is the 944 time's iteration. The loss is 4.756991575472416. The k is 12.86442781992874, and b is -58.603174899758706\n",
      "This is the 945 time's iteration. The loss is 4.7568123228528325. The k is 12.86352308660135, and b is -58.5974197842959\n",
      "This is the 946 time's iteration. The loss is 4.7566331133385304. The k is 12.862618570837235, and b is -58.591666052778734\n",
      "This is the 947 time's iteration. The loss is 4.756453946919144. The k is 12.86171427258407, and b is -58.585913704874415\n",
      "This is the 948 time's iteration. The loss is 4.756274823584313. The k is 12.860810191789554, and b is -58.580162740250216\n",
      "This is the 949 time's iteration. The loss is 4.756095743323674. The k is 12.859906328401394, and b is -58.5744131585735\n",
      "This is the 950 time's iteration. The loss is 4.755916706126876. The k is 12.859002682367308, and b is -58.5686649595117\n",
      "This is the 951 time's iteration. The loss is 4.755737711983556. The k is 12.85809925363503, and b is -58.56291814273234\n",
      "This is the 952 time's iteration. The loss is 4.755558760883355. The k is 12.857196042152305, and b is -58.55717270790302\n",
      "This is the 953 time's iteration. The loss is 4.755379852815939. The k is 12.856293047866888, and b is -58.55142865469142\n",
      "This is the 954 time's iteration. The loss is 4.755200987770952. The k is 12.85539027072655, and b is -58.5456859827653\n",
      "This is the 955 time's iteration. The loss is 4.755022165738042. The k is 12.854487710679077, and b is -58.53994469179249\n",
      "This is the 956 time's iteration. The loss is 4.754843386706875. The k is 12.85358536767226, and b is -58.53420478144092\n",
      "This is the 957 time's iteration. The loss is 4.754664650667099. The k is 12.852683241653908, and b is -58.52846625137858\n",
      "This is the 958 time's iteration. The loss is 4.754485957608391. The k is 12.851781332571843, and b is -58.522729101273555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 959 time's iteration. The loss is 4.754307307520401. The k is 12.850879640373893, and b is -58.516993330794\n",
      "This is the 960 time's iteration. The loss is 4.754128700392805. The k is 12.84997816500791, and b is -58.51125893960816\n",
      "This is the 961 time's iteration. The loss is 4.753950136215272. The k is 12.849076906421748, and b is -58.50552592738435\n",
      "This is the 962 time's iteration. The loss is 4.7537716149774605. The k is 12.848175864563279, and b is -58.49979429379096\n",
      "This is the 963 time's iteration. The loss is 4.75359313666907. The k is 12.847275039380385, and b is -58.49406403849648\n",
      "This is the 964 time's iteration. The loss is 4.753414701279754. The k is 12.846374430820962, and b is -58.48833516116946\n",
      "This is the 965 time's iteration. The loss is 4.753236308799202. The k is 12.845474038832917, and b is -58.48260766147854\n",
      "This is the 966 time's iteration. The loss is 4.753057959217097. The k is 12.844573863364172, and b is -58.47688153909243\n",
      "This is the 967 time's iteration. The loss is 4.752879652523114. The k is 12.843673904362658, and b is -58.47115679367994\n",
      "This is the 968 time's iteration. The loss is 4.752701388706947. The k is 12.842774161776324, and b is -58.46543342490993\n",
      "This is the 969 time's iteration. The loss is 4.752523167758279. The k is 12.841874635553127, and b is -58.45971143245137\n",
      "This is the 970 time's iteration. The loss is 4.752344989666813. The k is 12.840975325641036, and b is -58.453990815973285\n",
      "This is the 971 time's iteration. The loss is 4.752166854422232. The k is 12.840076231988036, and b is -58.448271575144794\n",
      "This is the 972 time's iteration. The loss is 4.7519887620142365. The k is 12.839177354542121, and b is -58.44255370963509\n",
      "This is the 973 time's iteration. The loss is 4.751810712432525. The k is 12.8382786932513, and b is -58.43683721911345\n",
      "This is the 974 time's iteration. The loss is 4.75163270566681. The k is 12.837380248063594, and b is -58.43112210324922\n",
      "This is the 975 time's iteration. The loss is 4.7514547417067705. The k is 12.836482018927036, and b is -58.425408361711845\n",
      "This is the 976 time's iteration. The loss is 4.7512768205421345. The k is 12.835584005789672, and b is -58.41969599417083\n",
      "This is the 977 time's iteration. The loss is 4.751098942162608. The k is 12.83468620859956, and b is -58.41398500029577\n",
      "This is the 978 time's iteration. The loss is 4.750921106557889. The k is 12.83378862730477, and b is -58.408275379756326\n",
      "This is the 979 time's iteration. The loss is 4.750743313717699. The k is 12.832891261853385, and b is -58.40256713222226\n",
      "This is the 980 time's iteration. The loss is 4.75056556363176. The k is 12.831994112193502, and b is -58.3968602573634\n",
      "This is the 981 time's iteration. The loss is 4.75038785628979. The k is 12.831097178273229, and b is -58.39115475484966\n",
      "This is the 982 time's iteration. The loss is 4.750210191681497. The k is 12.830200460040686, and b is -58.385450624351016\n",
      "This is the 983 time's iteration. The loss is 4.7500325697966215. The k is 12.829303957444006, and b is -58.379747865537546\n",
      "This is the 984 time's iteration. The loss is 4.749854990624887. The k is 12.828407670431334, and b is -58.3740464780794\n",
      "This is the 985 time's iteration. The loss is 4.749677454156017. The k is 12.82751159895083, and b is -58.3683464616468\n",
      "This is the 986 time's iteration. The loss is 4.749499960379738. The k is 12.826615742950661, and b is -58.36264781591006\n",
      "This is the 987 time's iteration. The loss is 4.749322509285791. The k is 12.825720102379014, and b is -58.35695054053955\n",
      "This is the 988 time's iteration. The loss is 4.749145100863903. The k is 12.824824677184083, and b is -58.35125463520575\n",
      "This is the 989 time's iteration. The loss is 4.748967735103834. The k is 12.823929467314073, and b is -58.345560099579195\n",
      "This is the 990 time's iteration. The loss is 4.748790411995299. The k is 12.82303447271721, and b is -58.33986693333051\n",
      "This is the 991 time's iteration. The loss is 4.748613131528058. The k is 12.822139693341722, and b is -58.334175136130405\n",
      "This is the 992 time's iteration. The loss is 4.748435893691854. The k is 12.821245129135855, and b is -58.32848470764966\n",
      "This is the 993 time's iteration. The loss is 4.748258698476431. The k is 12.82035078004787, and b is -58.322795647559126\n",
      "This is the 994 time's iteration. The loss is 4.748081545871545. The k is 12.819456646026032, and b is -58.31710795552975\n",
      "This is the 995 time's iteration. The loss is 4.747904435866946. The k is 12.818562727018627, and b is -58.31142163123255\n",
      "This is the 996 time's iteration. The loss is 4.747727368452388. The k is 12.817669022973949, and b is -58.305736674338625\n",
      "This is the 997 time's iteration. The loss is 4.747550343617634. The k is 12.816775533840305, and b is -58.300053084519156\n",
      "This is the 998 time's iteration. The loss is 4.747373361352449. The k is 12.815882259566017, and b is -58.294370861445394\n",
      "This is the 999 time's iteration. The loss is 4.747196421646579. The k is 12.814989200099415, and b is -58.28869000478868\n",
      "This is the 1000 time's iteration. The loss is 4.747019524489805. The k is 12.814096355388845, and b is -58.28301051422042\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "k = random.random() * 100 - 100\n",
    "b = random.random() * 100 - 100\n",
    "\n",
    "learning_rate = 1e-2\n",
    "iteration_time = 1000\n",
    "losses = []\n",
    "\n",
    "X = x[:, 5]\n",
    "print(type(list(X)))\n",
    "print(type(list(y)))\n",
    "for i in range(iteration_time):\n",
    "    fitting_value = [linear_function(k, b, x) for x in X]\n",
    "    \n",
    "    loss_now = loss(list(y), fitting_value)\n",
    "    losses.append(loss_now)\n",
    "    \n",
    "    print('This is the {0} time\\'s iteration. The loss is {1}. The k is {2}, and b is {3}'.format(i + 1, loss_now, k, b))\n",
    "    \n",
    "    k = k - partial_derivative_for_k(X, y, fitting_value) * learning_rate\n",
    "    b = b - partial_derivative_for_b(y, fitting_value) * learning_rate\n",
    "    \n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T07:00:06.858031Z",
     "start_time": "2019-10-21T07:00:06.594655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x122b0bf60>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAWl0lEQVR4nO3dfZCd5Xnf8e+lZbEXiivAWAXBRgxhlJqYl3QHTEk9BEzwCwbC8FrioR3G6h9uawdXtmSYATqmkFET8B+pUzUkoWPMa8QBjItMsaknHluN5AVkWVaDMQYOGJQW1cRszbJc/WPPgpB2V+flec7Lc76fGWb3POfluXd2+O2t676f64nMRJJUTUt6PQBJUnkMeUmqMENekirMkJekCjPkJanC9uv1AHb33ve+N1esWNHrYUjSQNmyZcvfZeZh8z3XVyG/YsUKNm/e3OthSNJAiYifLfSc5RpJqjBDXpIqzJCXpAoz5CWpwgx5SaqwvtpdI0nDpDZZZ93GHbywa4ojlo6x+uyVnH/S8kLPYchLUg/UJuus3bCVqekZAOq7pli7YStAoUFvyEtSF83N3uu7pvZ6bmp6hnUbdxjykjSIrqlt5fbvP8tid/F4YZ7w74QLr5LUBbXJ+j4DHuCIpWOFnteQl6QuWLdxxz4Dfmx0hNVnryz0vJZrJKkL9lWGWe7uGkkaXEcsHZt3sTWAmy85sfBwn2O5RpK6YPXZKxkbHXnHsQAu/+B4aQEPzuQlqSvmgrzsi5/2ZMhLUpecf9Ly0kN9T5ZrJKnCDHlJqrBCyjUR8QzwKjADvJGZExFxCHAXsAJ4Brg4M18p4nySpOYUOZP/ncw8MTMnGo/XAI9m5rHAo43HkqQuKnPh9Tzg9Mb3twGPAV8o8XySVIputAQuS1Ehn8A3IyKB/5yZ64Flmfli4/mfA8vme2NErAJWAYyPjxc0HEkqxp5NxcpqCVyWokL+tzOzHhHvAx6JiB/v/mRmZuMPwF4afxDWA0xMTOyrtYMkdUVtss71D27jldem93qujJbAZSmkJp+Z9cbXl4H7gJOBlyLicIDG15eLOJcklW3uhh7zBfycolsCl6XjmXxEHAgsycxXG9//LvDvgQeAK4CbGl/v7/RcklSmxW7osaeiWwKXpYhyzTLgvoiY+7yvZebDEfE3wN0RcSXwM+DiAs4lSYWqTdb54oYneW36zabfE1B4S+CydBzymfk0cMI8x/83cGanny9JZalN1rnq7sd5s4XVwG40FSuSvWskDa11G3e0FPBLx0a57tzjBibgwZCXNMSaXTwt64Ye3WDISxpaC93IY87Y6Ag3XvCBgQz3OTYokzS0Vp+9kiUx/3MH7j/4AQ/O5CUNsbkA3313TQRcfso4Xzr/A70cWmEMeUmV1Gy/mV7cyKObDHlJlTJfO4JB6zdTJGvykiqjNlln9b1PLNpvZtgY8pIq4/oHtzE9s/DG90HpN1MkQ15SZSzWUAwGp99MkQx5SUNhbHRkYPrNFMmFV0kDZ8/F1bl2A0vHRtk1tfdsPqASe97b4Uxe0kCZb3F119Q0q+95gnNOOJzRPa5uGl0S3HzJiUMZ8OBMXtIAaKbP+/Sbybd/vJN1F50wsPdjLYMhL6mv7XmP1cW8sGuq8hc3tcqQl9SXapN1Vt/zOC3cy2Mod8/siyEvqe+0czOP0SUxlLtn9sWFV0l9pTZZ53N3P9HyzTzWXXSCZZp5OJOX1Ddqk3XWbtjKTDaX8FXo9142Q15S31i3cQdT0zNNvXaQ79bUTYWFfESMAJuBemaeExFHA3cChwJbgE9m5utFnU9SNVxT28odm55revZ+2jGHcPunTi15VNVR5Ez+M8B24D2Nx38I3JyZd0bEnwJXAl8p8HySBlQz+973VLWbeXRLISEfEUcCHwduAK6KiADOAP554yW3AddhyEtDb67u3mxZxrp7Z4qayd8CfB44qPH4UGBXZr7RePw8MO9vKCJWAasAxsfHCxqOpH40t3OmmdJMgFesFqDjkI+Ic4CXM3NLRJze6vszcz2wHmBiYqKFTVOSBkkrO2dGIvjJjR/rwqiqr4iZ/GnAuRHxMeDdzNbkvwwsjYj9GrP5I4F6AeeSNKBa2Tlz2SlHlTya4dFxyGfmWmAtQGMm/+8y8/KIuAe4kNkdNlcA93d6LkmDY88baTezyDoSwWWnHOXiaoHK3Cf/BeDOiPgSMAncWuK5JPWJ2mSd6x7Y9o6+7vVdUwTM22RsJII/utirVctSaMhn5mPAY43vnwZOLvLzJfWv2mSdL254ktcW6CiWsFfQu3OmfPaukdSx2Y6RTywY8HOS2StVo/HVgC+fbQ0kdeTy//I9vvuT/9PUa5cvHeO7a84oeUTanSEvqW1n/fFj/O3Lv2zqtcN6I+1eM+QltWy+xdXFHHzAKNd+4jhLMz1gyEtq2jW1rXxt07Mt9Xr//Q/ab6aXDHlJTWml9g7O3vuFIS9pUa3cSHuO7YD7hyEvaV7t3Egb4Nj3HWjA9xFDXtJeapN1PnvX4y29x/JMfzLkJb3DNbWtfPX7zzb9+rHRJdx4wfGGe58y5CUBc73eH2emheK7tff+Z8hLQ67VmTvAgfuPcMPv2ZJgEBjy0hBrJ+CdvQ8WQ14aQrXJOlfft5Vfvt7cTTzmGPCDx5CXhki74R7A5V65OpAMeWlItFOagdl9749cdXrxA1JXGPJSxdUm61x11+O0eE0TYN+ZKjDkpQprpRXw7qy9V4chL1VQO1esglsjq8iQlyqm1W6RcyzNVJMhL1XINbWtLQW8LQmqr+OQj4h3A98B3tX4vHsz89qIOBq4EzgU2AJ8MjNf7/R8kt6pNlnn+ge38cprzd2laY67ZoZDETP5XwFnZObfR8Qo8NcR8d+Aq4CbM/POiPhT4ErgKwWcT1JDuwurBvzwWNLpB+Ssv288HG38l8AZwL2N47cB53d6LklvO/7ah9sK+FsuOdGAHyKF1OQjYoTZksyvA38C/ATYlZlvNF7yPDBv0S8iVgGrAMbHx4sYjlRp7S6svuddIzx5/UdKGJH6WcczeYDMnMnME4EjgZOB32jhveszcyIzJw477LAihiNVUm2yzoo1D7W9c8aAH06F7q7JzF0R8W3gVGBpROzXmM0fCdSLPJc0TNqZvY+OBOsuPMGdM0Ou45l8RBwWEUsb348BZwHbgW8DFzZedgVwf6fnkobNNbWtbc3eRwIDXkAxM/nDgdsadfklwN2Z+fWI+BFwZ0R8CZgEbi3gXNLQcOeMitBxyGfmk8BJ8xx/mtn6vKQWnXLDI7z0auuXldxyyYnO3vUOXvEq9ZF2Z++2JNBCDHmpD7TbUGzZQfuz6eqzShiRqsKQl3rMdsAqkyEv9Ui7d2oK4GZr72qSIS/1wPHXPswvftXafVYtzagdhrzURe3O3g14tcuQl7pkxZqH2nqfO2fUCUNeKlk7pRmYrb3/9KaPFz8gDRVDXipJu6UZcPau4hjyUgmOXvMQ2cb7bAesohnyUoHa7fUOzt5VDkNeKki7tXcvalKZDHmpQ+22JAB4xoVVlcyQlzpgQzH1O0NeaoOzdw0KQ15qUbs7Z9z3rl4w5KUmue9dg8iQl5rQ7s4Zw129ZshLi/iNq7/B/5tppzhjwKs/GPLSAtptKLZfwFM3WntXf+g45CPiKOC/AsuABNZn5pcj4hDgLmAF8AxwcWa+0un5pLK1W5oZXQLrLvJmHuovRczk3wA+l5k/iIiDgC0R8QjwL4BHM/OmiFgDrAG+UMD5pFK4LVJV1HHIZ+aLwIuN71+NiO3AcuA84PTGy24DHsOQV59q96ImMODV3wqtyUfECuAkYBOwrPEHAODnzJZz5nvPKmAVwPj4eJHDkfapk4ZidozUICgs5CPiHwB/BXw2M38REW89l5kZEfNuUcjM9cB6gImJifa2MUhtaHdhFZy9a3AsKeJDImKU2YC/PTM3NA6/FBGHN54/HHi5iHNJnapN1tsO+NOOOcSA10ApYndNALcC2zPzj3d76gHgCuCmxtf7Oz2X1Kl2WxKAs3cNpiLKNacBnwS2RsTc1oQvMhvud0fElcDPgIsLOJfUlk4WVr2oSYOsiN01f81s76X5nNnp50udaveq1WPfdyCPXHV68QOSusgrXlVZv772Id5oszZzyyVe1KRqMORVSbYkkGYZ8qqUTtoBO3tXFRnyqoTaZJ0/uOvxtnbOeFGTqsyQ18Cz54y0MENeA63d8ow7ZzQsDHkNpE56zlh71zAx5DVwTrnhEV569fWW32e4axgZ8hoY7S6uurCqYWbIayC0W56xJYGGnSGvvtZuzxkvapJmGfLqW+22JXDnjPQ2Q159p91tke8eCX58w8dKGJE0uAx59Y3aZJ3P3/sEr7fRMXLZQfuz6eqzShiVNNgMefWF2mSdz93zBDNvthbwpx1zCLd/6tSSRiUNPkNefeH6B7e1FPDO3KXmGPLqmXa3RRrwUvMMeXVdJ7V3971LrTHk1VXttiSw9i61x5BX1xx/7cP84lczLb3HmbvUGUNepWtn3/vypWOsPnulDcWkDhUS8hHx58A5wMuZ+ZuNY4cAdwErgGeAizPzlSLOp8HRTsDbLVIqzpKCPucvgT3b/K0BHs3MY4FHG481JK6pbeWYtd9oKeBHlxjwUtEKmcln5nciYsUeh88DTm98fxvwGPCFIs6n/lWbrLP6nseZfrO191l7l8pR1Ex+Pssy88XG9z8Hls33oohYFRGbI2Lzzp07SxyOyjYb8E8Y8FIf6crCa2ZmRMy7KToz1wPrASYmJtroOah+sW7jDqZbuGrVbZFS+coM+Zci4vDMfDEiDgdeLvFc6oHaZJ11G3fwwq4pjlg6Rn3XVFPvO2B0Cf/hguOtvUtdUGbIPwBcAdzU+Hp/iedSl9Um66zdsJWp6dl97/VdUwQsems+yzJS9xVSk4+IO4DvASsj4vmIuJLZcD8rIv4W+HDjsSriuge2vRXwcxYL+NOOOcSAl3qgqN01ly3w1JlFfL76S22yzq6p6QWfXzo2+tbzBx8wyrWfOM7SjNQjXvGqpuxef18SseDrli8d47trzujiyCQtxpDXPu1Zf5/JhQszq89e2a1hSWqCIa8Fzc3em901c/ABo5ZlpD5jyGsv7fSbGRsd4dpPHFfSiCS1y5DXW2qTdb644Ulea/KS1ZEI3szkCDtGSn3LkBfQ+ux9bHSEGy/4gMEu9TlDfsjVJutc98C2RbdE7sle79LgMOSHWKs30g7gZlsBSwOlzC6U6mOtBjzA5R8cN+ClAeNMfgjVJustBfzY6BJutKGYNJAM+SG0buOOpl43siT4o4tOMNylAWa5Zgi90MTFTQcfMGrASxXgTH4ILdb73Rt5SNXiTH4IrT57JWOjI3sdN+Cl6nEmP4TmSjC739XJfe9SNRnyQ+r8k5Yb6tIQsFwjSRXmTH6A1SbrXP/gNl55bbYlwdKxUa4717swSXqbIT+gapN1Vt/7BNMzb9/AY9fUNKvveQLAoJcEWK4ZWOs27nhHwM+ZfjObvthJUvWVHvIR8ZGI2BERT0XEmrLPNywWu6CpmYudJA2HUkM+IkaAPwE+CrwfuCwi3l/mOYfFEUvH2npO0nApuyZ/MvBUZj4NEBF3AucBPyr5vJUyd6/V3fe0rz575V41eYDRJeHNtCW9pexyzXLgud0eP9849paIWBURmyNi886dO0sezuCpTdZZu2Er9V1TJFDfNcXaDVsBWHfhCRx8wOhbr106Nso6+81I2k3Pd9dk5npgPcDExMTeK4lDbt3GHUxNz7zj2NT0DOs27uC7a84w0CUtquyZfB04arfHRzaOqUkLLaK6uCqpGWWH/N8Ax0bE0RGxP3Ap8EDJ56yUhRZRXVyV1IxSQz4z3wD+NbAR2A7cnZnbyjznoKlN1jntpm9x9JqHOO2mb1GbfOc/dObrGDk2OuLiqqSmlF6Tz8xvAN8o+zyDaG5Rda7mvvui6lyt3Y6RkjrR84XXYbbYouruIW7HSEntsq1BD7moKqlshnwPuagqqWyGfA+5qCqpbNbke8hFVUllM+R7zEVVSWWyXCNJFeZMviDX1LZyx6bnmMlkJILLTjmKL53/gV4PS9KQM+QLcE1tK1/9/rNvPZ7JfOuxQS+plyzXFOCOTc+1dFySusWQL8BMzt8heaHjktQthnwBRiJaOi5J3WLIF+CyU45q6bgkdYsLrwWYW1x1d42kfhPZR3XjiYmJ3Lx5c6+HIUkDJSK2ZObEfM9ZrpGkCjPkJanCrMk31CbrNgqTVDmGPM3dhk+SBpHlGha/DZ8kDTJDHm/DJ6m6Ogr5iLgoIrZFxJsRMbHHc2sj4qmI2BERZ3c2zHJ5Gz5JVdXpTP6HwAXAd3Y/GBHvBy4FjgM+AvyniBjZ++39wdvwSaqqjhZeM3M7QOzdo+U84M7M/BXw04h4CjgZ+F4n5yuLt+GTVFVl7a5ZDnx/t8fPN47tJSJWAasAxsfHSxrOvnkbPklVtM+Qj4j/DvyjeZ66OjPv73QAmbkeWA+zbQ06/TxJ0tv2GfKZ+eE2PrcO7N6C8cjGMUlSF5W1hfIB4NKIeFdEHA0cC/zPks4lSVpAp1sofy8ingdOBR6KiI0AmbkNuBv4EfAw8OnMnFn4kyRJZeh0d819wH0LPHcDcEMnn98s+85I0vwGvneNfWckaWED39bAvjOStLCBD3n7zkjSwgY+5O07I0kLG/iQt++MJC1s4Bde7TsjSQsb+JAH+85I0kIGvlwjSVqYIS9JFWbIS1KFGfKSVGGGvCRVWGT2z306ImIn8LNej2Mf3gv8Xa8H0SXD8rP6c1bLMP6cv5aZh833or4K+UEQEZszc6LX4+iGYflZ/TmrxZ/znSzXSFKFGfKSVGGGfOvW93oAXTQsP6s/Z7X4c+7GmrwkVZgzeUmqMENekirMkG9RRIxExGREfL3XYylLRDwTEVsj4vGI2Nzr8ZQlIpZGxL0R8eOI2B4Rp/Z6TEWLiJWN3+Pcf7+IiM/2elxliIg/iIhtEfHDiLgjIt7d6zGVISI+0/gZtzXzu6xEq+Eu+wywHXhPrwdSst/JzKpfUPJl4OHMvDAi9gcO6PWAipaZO4ATYXaCAtSB+3o6qBJExHLg3wLvz8ypiLgbuBT4y54OrGAR8ZvAp4CTgdeBhyPi65n51ELvcSbfgog4Evg48Ge9Hos6ExH/EPgQcCtAZr6embt6O6rSnQn8JDP7/arydu0HjEXEfsz+wX6hx+Mpwz8GNmXma5n5BvA/gAsWe4Mh35pbgM8Db/Z6ICVL4JsRsSUiVvV6MCU5GtgJ/EWj/PZnEXFgrwdVskuBO3o9iDJkZh34j8CzwIvA/83Mb/Z2VKX4IfDPIuLQiDgA+Bhw1GJvMOSbFBHnAC9n5pZej6ULfjszfwv4KPDpiPhQrwdUgv2A3wK+kpknAb8E1vR2SOVplKPOBe7p9VjKEBEHA+cx+8f7CODAiPj93o6qeJm5HfhD4JvAw8DjwMxi7zHkm3cacG5EPAPcCZwREV/t7ZDK0ZgVkZkvM1u/Pbm3IyrF88Dzmbmp8fheZkO/qj4K/CAzX+r1QEryYeCnmbkzM6eBDcA/7fGYSpGZt2bmP8nMDwGvAP9rsdcb8k3KzLWZeWRmrmD2n73fyszKzRQi4sCIOGjue+B3mf0nYqVk5s+B5yJiZePQmcCPejiksl1GRUs1Dc8CH4yIAyIimP19bu/xmEoREe9rfB1nth7/tcVe7+4a7WkZcN/s/yfsB3wtMx/u7ZBK82+A2xuljKeBf9nj8ZSi8cf6LOBf9XosZcnMTRFxL/AD4A1gkuq2N/iriDgUmAY+va8NA7Y1kKQKs1wjSRVmyEtShRnyklRhhrwkVZghL0kVZshLUoUZ8pJUYf8fhCRnqtjNCFMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, linear_function(best_k, best_b, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary for changing L2-norm to L1-norm\n",
    "Finally it shows convergence reaching a loss of 4.74. However, the convergence loss is related to the randomlt selected initial k and b."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
