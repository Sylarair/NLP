{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangqin/anaconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, inputs=[]):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "\n",
    "        self.value = None\n",
    "        self.gradients = {}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {self:0}\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(outputnode, graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "#         print(n.outputs)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "# feed_dict = {\n",
    "#     X: X_,\n",
    "#     y: y_,\n",
    "#     W1: W1_,\n",
    "#     b1: b1_,\n",
    "#     W2: W2_,\n",
    "#     b2: b2_\n",
    "# }\n",
    "# topological_sort(feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 200.702\n",
      "Epoch: 101, Loss: 6.822\n",
      "Epoch: 201, Loss: 4.840\n",
      "Epoch: 301, Loss: 4.556\n",
      "Epoch: 401, Loss: 4.094\n",
      "Epoch: 501, Loss: 4.060\n",
      "Epoch: 601, Loss: 3.765\n",
      "Epoch: 701, Loss: 5.129\n",
      "Epoch: 801, Loss: 4.391\n",
      "Epoch: 901, Loss: 3.705\n",
      "Epoch: 1001, Loss: 3.497\n",
      "Epoch: 1101, Loss: 3.677\n",
      "Epoch: 1201, Loss: 3.989\n",
      "Epoch: 1301, Loss: 3.790\n",
      "Epoch: 1401, Loss: 3.223\n",
      "Epoch: 1501, Loss: 3.189\n",
      "Epoch: 1601, Loss: 3.871\n",
      "Epoch: 1701, Loss: 3.151\n",
      "Epoch: 1801, Loss: 2.782\n",
      "Epoch: 1901, Loss: 3.336\n",
      "Epoch: 2001, Loss: 2.836\n",
      "Epoch: 2101, Loss: 3.037\n",
      "Epoch: 2201, Loss: 3.052\n",
      "Epoch: 2301, Loss: 3.183\n",
      "Epoch: 2401, Loss: 3.121\n",
      "Epoch: 2501, Loss: 2.851\n",
      "Epoch: 2601, Loss: 3.008\n",
      "Epoch: 2701, Loss: 2.448\n",
      "Epoch: 2801, Loss: 2.954\n",
      "Epoch: 2901, Loss: 2.783\n",
      "Epoch: 3001, Loss: 2.799\n",
      "Epoch: 3101, Loss: 2.695\n",
      "Epoch: 3201, Loss: 2.567\n",
      "Epoch: 3301, Loss: 3.115\n",
      "Epoch: 3401, Loss: 2.515\n",
      "Epoch: 3501, Loss: 2.648\n",
      "Epoch: 3601, Loss: 3.185\n",
      "Epoch: 3701, Loss: 2.511\n",
      "Epoch: 3801, Loss: 2.796\n",
      "Epoch: 3901, Loss: 2.582\n",
      "Epoch: 4001, Loss: 2.938\n",
      "Epoch: 4101, Loss: 2.863\n",
      "Epoch: 4201, Loss: 2.557\n",
      "Epoch: 4301, Loss: 2.641\n",
      "Epoch: 4401, Loss: 2.790\n",
      "Epoch: 4501, Loss: 2.730\n",
      "Epoch: 4601, Loss: 2.502\n",
      "Epoch: 4701, Loss: 2.633\n",
      "Epoch: 4801, Loss: 2.584\n",
      "Epoch: 4901, Loss: 2.620\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "# print(feed_dict)\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "# print(graph)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: A neuron firstly computes linear weighted values with inputs and weights (plus constant values), then a activation function will take the value to perform a non-linear transformation to get an output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Without non-linear functions, there are only linear functions in neural networks, which would lead to a condition that we can in fact use a linear function to finish whole computation and the number of hidden layers has nothing to do with the final outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: It's used to measure the difference between transformed values and actual values in the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Loss = -\\sum{ylog\\hat{y} + (1-y)log(1-\\hat{y})} $$\n",
    "    (Suppose: y is the actual value; y_hat is the transformed value.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommend using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Both sigmoid and tanh functions have advantages that they are continous and derivable in the real number region, while their disadvantages are that their decent rate can be very slow when the x is very large or small, that would lead to slow convergence. \n",
    "\n",
    "    Due to such disadvantages, the ReLU is used because it has a constant derivate, although it's not deriavted in the 0 and it's derivate is 0 when x is less than 0. And Leaky ReLU further amends shortcomings, and also have a constant derivate in the region from negative infinite to 0. Both of ReLU and Leaky ReLU make the convergence rate stable and not slow.\n",
    "\n",
    "    Taken all of that, I recommend sigmoid or tanh when the input x is small, while it's Leaky ReLU when x is large. In fact, the Leaky ReLU is used more universally due to its better performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: If we use zero initialization for all parameters, then during the parameters update, we will get same parameters for the neurons in the same hidden layer, which means that the large numbers of neurons are meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Yes, I can. When it comes to multiple classification, we use softmax function to compute the probability of a thing belonging to one type, then we consider the type with the largest probability is the true type. In the function, we first use the exponential function to transform probability, then we normalize these transformed probability by dividing the sum of transformed probability, then use the following formula to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ S_j = \\frac{e^{aj}}{\\sum{e^{ak}}} $$\n",
    "$$ L = -\\sum\\limits_{j=1}^{m}{y_jlogS_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangqin/anaconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()\n",
    "digits.data[1]#.shape\n",
    "# digits['DESCR']\n",
    "# digits.column_names\n",
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADSCAYAAAB0FBqGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPaUlEQVR4nO3df2xW93XH8c8ZlJQki3GARQtkmCgTqtWIH7GybN0CtKFKu6mgSYlaqRNEk2BSNgGaNNhfIf+BNE3wxzQxJYstrU0FaYunqWpDFMM2acpmB9MlpUwETIE1P1CCm23R0rCzPx5HIpO/5/q5tp9zmd8vCRVynsf3+Mu9Hy4Pp99r7i4AQOf9QnYDADBXEcAAkIQABoAkBDAAJCGAASDJ/HZevGTJEu/p6Wn7IO+9915Yv3z5crF2xx13FGvLly8v1ubNm1fd2CTGxsZ09epVm+rr665JlbNnzxZr169fL9buvvvuYm3RokW1+xkZGbnq7kun8trZWpP333+/WHvjjTeKtYULFxZrq1atqt1PO2si1V+XN998M6xfuXKlWFuwYEGx1tvbW6zd7NdPdI1cuHChWLvvvvtmvBepfK60FcA9PT0aHh5u++BHjx4N63v27CnWNm3aVKzt37+/WOvu7q5ubBJ9fX1tvb7umlTZsGFDsXbt2rVi7emnny7WNm/eXLsfM7s41dfO1pqcOHGiWNuyZUuxtmbNmlpfs0o7ayLVX5cDBw6E9b179xZry5YtK9ZefvnlYu1mv36ia2Tbtm3F2rFjx2a8F6l8rvARBAAkIYABIAkBDABJCGAASEIAA0CStqYg6oqmHKR4LCQaYbvzzjuLtSNHjoTHfOyxx8J6tmhk7OTJk8Xa0NBQsTadKYhOGB0dDesbN24s1rq6uoq1sbGxui11TDTJUHUuHz58uFjbsWNHsTYyMlKsPfLII+Exm66/v79Yi6ZiOo07YABIQgADQBICGACSEMAAkIQABoAkBDAAJJmxMbRopCUaM5PinazuvffeYi3aqCfqR8ofQ6sauaq7SUyTRmzaVbURyurVq4u1aDOeaIOipti+fXuxVjXG+cADDxRrK1euLNZu5lGzaLMdKR5D27VrV7E2nZHFOru6cQcMAEkIYABIQgADQBICGACSEMAAkIQABoAkBDAAJJmxOeBo28h169aF741mfSPR/GMTHDx4sFjbt29f+N7x8fFax4we5tl00XymFM9ZRu9t+jacUnwNnD9/PnxvNGcfzfpG12zdh3J2SjTnK8XzvNFDOaPzqOqp4lXX9GS4AwaAJAQwACQhgAEgCQEMAEkIYABIQgADQJKOjKFF20bO1jGbMEYTjbREozBS/f6rtunLFvUXje1J1dtVllSNLDVd1Zjmu+++W6xFY2hR7aWXXgqP2Ynra3BwsFjbvXt3+N6tW7fWOuahQ4eKteeee67W14xwBwwASQhgAEhCAANAEgIYAJIQwACQhAAGgCQzNoYWjaVUPaE4Eo2aDQ8PF2uPP/547WPezKKnLTfhicnRjlHRCFCVaEStaherm1107UXjZDt27CjWDhw4EB5z//791Y1NU1dXV62aJA0MDBRrVU8kL4mevF0Xd8AAkIQABoAkBDAAJCGAASAJAQwASQhgAEgyY2No0Y5N0biYJB09erRWLbJnz55a78PsinaBO3HiRPje06dPF2vRiFD0UM4nnngiPGYTHui5d+/esF73wZvHjx8v1powxhk9YLZq179o1Cz6utEuarMxzsgdMAAkIYABIAkBDABJCGAASEIAA0ASAhgAkhDAAJCkI3PAVVvbRTO7fX19xdp0trnMVjVTGM2fRk+LjWZpq57E3AnRlphV2wRG9Wiby2i9enp6wmM2YQ646gnE27dvr/V1o1nfw4cP1/qaTRFdX+Pj48Vap68R7oABIAkBDABJCGAASEIAA0ASAhgAkhDAAJDE3H3qLzZ7R9LF2WunEVa4+9KpvniOrInUxrqwJpObI+vCmkxu0nVpK4ABADOHjyAAIAkBDABJCGAASEIAA0ASAhgAkhDAAJCEAAaAJAQwACQhgAEgSWMD2MweNbOzZnbOzPZm95PNzP7azN42s9eye2kKM7vHzIbM7Edm9rqZ7czuKZuZfdrM/tnMTk+sydPZPTWFmc0zs1Nm9nfZvXyskQFsZvMk/YWkL0nqlfQ1M+vN7Spdv6RHs5tomI8k/bG790p6SNKTnCf6b0mfd/fVktZIetTMHkruqSl2SjqT3cSNGhnAkh6UdM7dz7v7h5K+JSn/4VyJ3P3vJb2b3UeTuPtP3f3ViZ+/r9bFtSy3q1ze8h8Tv/zUxI85v+GLmS2X9NuSnsnu5UZNDeBlki7d8OvLmuMXFmJm1iNpraRXcjvJN/FX7VFJb0s67u5zfk0kHZT0J5L+J7uRGzU1gIEpM7PbJX1b0i53/1l2P9nc/bq7r5G0XNKDZvbZ7J4ymdnvSHrb3Rv3GPWmBvAVSffc8OvlE/8N+AQz+5Ra4fsNd/9Odj9N4u7XJA2Jfzv4nKSvmNmYWh9nft7M/ia3pZamBvC/SPpVM1tpZgskfVXS3yb3hIYxM5P0rKQz7v7n2f00gZktNbNFEz9fKGmTpB/ndpXL3f/U3Ze7e49aWfKyu389uS1JDQ1gd/9I0h9K+oFa/7ByxN1fz+0ql5k9L+mfJK0ys8tm9vvZPTXA5yT9nlp3NKMTP76c3VSyX5Y0ZGY/VOtG5ri7N2bsCp/EEzEAIEkj74ABYC4ggAEgCQEMAEkIYABIQgADQBICGACSEMAAkIQABoAkBDAAJCGAASAJAQwASQhgAEhCAANAEgIYAJIQwACQhAAGgCQEMAAkIYABIAkBDABJCGAASEIAA0ASAhgAkhDAAJCEAAaAJAQwACQhgAEgCQEMAEkIYABIQgADQBICGACSEMAAkIQABoAkBDAAJCGAASAJAQwASQhgAEhCAANAEgIYAJIQwACQhAAGgCQEMAAkmd/Oi5csWeI9PT1tH+Ts2bNh/ZZbbinW6hxvOsbGxnT16lWb6uvrrkmVaM2uX79erPX29s54L5I0MjJy1d2XTuW1ddfkrbfeCuvR933t2rVi7YMPPijW5s2bFx7z/vvvL9ZGR0envCZS/XW5dOlSWI++98WLFxdrd911V7FWtS4lnbp+zp07F9ajc2XVqlVtH2+6StdPWwHc09Oj4eHhtg++YcOGyq9b0t/f3/bxpqOvr6+t19ddkyrRmkUX3Gz0IklmdnGqr627JgcPHgzr0fd97NixYu306dPF2u233x4ec2hoqFjr7u6e8ppI9ddl165dYT363rdt21br6y5atKiyr8l06vrZsmVLWI/OlRMnTrR9vOkqXT98BAEASQhgAEhCAANAEgIYAJIQwACQpK0piLrGxsbC+smTJ4u1gYGBYm3FihW1j5ltcHAwrEdr8tRTT810OzeF6F/mowmKqBb9a3nVMTtldHS09nujKaJoGiBjUuD/iq7hqusnYlaeklu9enWxNp3fhxLugAEgCQEMAEkIYABIQgADQBICGACSEMAAkKQjY2hVozwXL5b3NOnq6irW6m5YM5WeZtt0RsmqNiK5WVVtOhPZt29fsRaNMzVh3KrKmjVrwnrdzayia6BqXao22JoJVddwZP369cVatF6dPh+4AwaAJAQwACQhgAEgCQEMAEkIYABIQgADQBICGACSdGQOuOqpp9FDE8fHx4u1aD4ye863StWMY7QtXtVcaJPN1haIVQ/0LIkeaCnFD7XslKoe1q5dW6xFM9DRNdLpp5HPdA/R72s0Rz+d2eM6uAMGgCQEMAAkIYABIAkBDABJCGAASEIAA0CSjoyhVY36RONH0ZNId+/eXbelaW19OBOqxl2iEZxo5CoasWn6aFHVU2frjqlF518ntlWcrumMRkVP175w4UKx1oRzJRqTi8Y0Jam7u7tY27lzZ7EWnYNVT1qvs2bcAQNAEgIYAJIQwACQhAAGgCQEMAAkIYABIElHxtCqzMYoUNXISLaqkZVofCgaS4pG806dOhUesxO7rEXfd9W4opnVeu/NMGoWjT9t3LgxfG/0hO3oOohGFqt+L7LH1KpGFqN63fO8anS1as0mwx0wACQhgAEgCQEMAEkIYABIQgADQBICGACSdGQMbXBwMKx3dXUVa/v27at1zGjEpgmqHrQYjZNFI0DR2FHVmEz2wz6rxnyi82T9+vUz3U5HRb+n0fctxesWnQ/Rwzz7+/vDY9a9LjslOpej9Yq+7zpjZlW4AwaAJAQwACQhgAEgCQEMAEkIYABIQgADQBICGACSdGQOeGhoKKwfOnSo1tfdunVrsdb0LQir5oCj+c1oVjH6vps+G1311OOBgYFiLXqC7s0g6r/qXI6eABzNEG/evLlYy35qeJWq/qLtKKPtXKNzcDbm5LkDBoAkBDAAJCGAASAJAQwASQhgAEhCAANAEnP3qb/Y7B1JF2evnUZY4e5Lp/riObImUhvrwppMbo6sC2syuUnXpa0ABgDMHD6CAIAkBDAAJCGAASAJAQwASQhgAEhCAANAEgIYAJIQwACQhAAGgCSNDWAzGzOzfzWzUTMbzu6nCcxskZm9YGY/NrMzZvbr2T1lMrNVE+fHxz9+ZmbNfpRDB5jZbjN73cxeM7PnzezT2T01gZntnFiT15tynjT2/4psZmOS+tz9anYvTWFmA5L+wd2fMbMFkm519/LzVeYQM5sn6YqkX3P3ubC3wKTMbJmkf5TU6+4fmNkRSd9z9/7cznKZ2WclfUvSg5I+lPR9SX/g7ucy+2rsHTA+ycy6JD0s6VlJcvcPCd9P+IKkN+Zy+N5gvqSFZjZf0q2S/j25nyb4jKRX3P2/3P0jSScl/W5yT40OYJf0opmNmNn27GYaYKWkdyQ9Z2anzOwZM7stu6kG+aqk57ObyObuVyT9maSfSPqppHF3fzG3q0Z4TdJvmdliM7tV0pcl3ZPcU6MD+DfdfZ2kL0l60swezm4o2XxJ6yT9pbuvlfSfkvbmttQMEx/HfEXS0exesplZt6TNav2Bfbek28zs67ld5XP3M5IOSHpRrY8fRiVdT21KDQ7giT/J5e5vS/quWp/dzGWXJV1291cmfv2CWoGM1h/Sr7r7W9mNNMAjki64+zvu/nNJ35H0G8k9NYK7P+vuD7j7w5Lek/Rv2T01MoDN7DYz+8WPfy7pi2r9FWLOcvc3JV0ys1UT/+kLkn6U2FKTfE18/PCxn0h6yMxuNTNT6zw5k9xTI5jZL03876+o9fnvN3M7av21tonukvTd1vmj+ZK+6e7fz22pEf5I0jcm/sp9XtITyf2km/gDepOkHdm9NIG7v2JmL0h6VdJHkk5J+qvcrhrj22a2WNLPJT3ZhH/EbuwYGgD8f9fIjyAAYC4ggAEgCQEMAEkIYABIQgADQBICGACSEMAAkOR/AUYpZiaT03DXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap='gray_r')#plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 3, 4, 0, 5, 3, 5, 4, 2, 3, 8, 4, 0, 5, 7, 4, 1, 7, 1, 7, 2, 3,\n",
       "       0, 9, 8, 2, 5, 6, 4, 1, 6, 4, 5, 5, 4, 8, 1, 1, 6, 5, 9, 3, 4, 7,\n",
       "       8, 7, 5, 0, 1, 3, 4, 4, 7, 8, 1, 2, 6, 0, 0, 5, 4, 7, 1, 6, 1, 2,\n",
       "       1, 2, 5, 1, 3, 8, 8, 6, 8, 6, 8, 4, 4, 3, 0, 6, 4, 5, 8, 0, 6, 5,\n",
       "       0, 7, 1, 2, 9, 1, 4, 8, 6, 9, 2, 0, 7, 4, 6, 5, 6, 4, 0, 2, 2, 6,\n",
       "       9, 9, 5, 2, 7, 6, 4, 0, 0, 6, 2, 1, 7, 3, 5, 9, 5, 2, 4, 5, 5, 1,\n",
       "       2, 5, 7, 6, 2, 3, 3, 9, 2, 9, 0, 7, 6, 7, 1, 7, 6, 7, 1, 2, 5, 8,\n",
       "       3, 9, 0, 1, 3, 8, 5, 2, 5, 0, 6, 5, 6, 9, 0, 7, 5, 5, 9, 6, 5, 4,\n",
       "       1, 9, 7, 3, 3, 2, 2, 5, 4, 5, 7, 9, 8, 7, 1, 3, 3, 0, 8, 5, 2, 7,\n",
       "       4, 8, 7, 5, 3, 3, 5, 0, 7, 6, 4, 4, 1, 8, 2, 7, 6, 9, 7, 6, 1, 9,\n",
       "       8, 7, 4, 1, 4, 9, 3, 6, 2, 3, 4, 1, 3, 9, 2, 3, 8, 7, 9, 0, 1, 8,\n",
       "       8, 3, 7, 0, 7, 2, 2, 7, 3, 2, 9, 2, 2, 0, 9, 9, 8, 6, 3, 0, 6, 8,\n",
       "       3, 9, 3, 0, 0, 4, 4, 8, 2, 6, 9, 8, 0, 5, 7, 4, 2, 2, 3, 1, 5, 8,\n",
       "       2, 7, 5, 0, 7, 9, 0, 2, 1, 7, 6, 9, 5, 7, 8, 1, 9, 8, 5, 5, 6, 0,\n",
       "       2, 6, 7, 2, 3, 5, 8, 3, 3, 9, 3, 2, 5, 4, 0, 2, 1, 5, 6, 6, 0, 6,\n",
       "       6, 2, 1, 5, 5, 3, 1, 7, 6, 5, 6, 2, 9, 8, 7, 0, 6, 2, 9, 1, 0, 0,\n",
       "       2, 6, 1, 7, 3, 9, 2, 4, 0, 1, 6, 7, 2, 8, 9, 6, 3, 5, 6, 5, 4, 3,\n",
       "       8, 0, 7, 7, 4, 7, 2, 4, 2, 3, 3, 6, 3, 8, 0, 7, 6, 1, 7, 3, 9, 1,\n",
       "       2, 3, 3, 8, 1, 5, 6, 4, 9, 5, 8, 7, 1, 5, 0, 0, 9, 1, 3, 2, 5, 7,\n",
       "       0, 5, 0, 8, 0, 4, 1, 1, 8, 4, 3, 9, 2, 6, 9, 7, 1, 5, 3, 1, 7, 2,\n",
       "       7, 4, 5, 9, 7, 1, 1, 8, 5, 8])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "# print(X_train.shape)\n",
    "# X_test.shape\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wangqin/Downloads/jupyters_and_slides/jupyters_and_slides-master/2019-autumn/Lecture_NLP/4th\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = 1. / (1 + np.exp(-np.array(z, dtype=float))) # using np.array to assure z \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44392646],\n",
       "       [-0.340871  ],\n",
       "       [-0.6857288 ],\n",
       "       [-0.9484141 ],\n",
       "       [ 0.83237931],\n",
       "       [-0.1339836 ],\n",
       "       [ 1.27991369],\n",
       "       [-1.76297707],\n",
       "       [-0.45382971],\n",
       "       [ 2.22644414]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random innitialize the parameters\n",
    "# import random\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.randn(dim, 1)\n",
    "    b = np.random.ranf()\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b\n",
    "\n",
    "## test function\n",
    "initialize_parameters(10)[0]#.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dw': array([[ 0.00000000e+00],\n",
       "         [ 1.21928442e+00],\n",
       "         [ 1.05162804e+01],\n",
       "         [ 2.51670020e+01],\n",
       "         [ 8.25876159e+00],\n",
       "         [-1.61014817e+01],\n",
       "         [-1.05928762e+01],\n",
       "         [-1.45276286e+00],\n",
       "         [ 1.20044476e-05],\n",
       "         [ 5.51934469e+00],\n",
       "         [ 1.87755144e+01],\n",
       "         [ 2.08568671e+01],\n",
       "         [ 1.31361391e+01],\n",
       "         [ 5.00090434e+00],\n",
       "         [-8.21118989e+00],\n",
       "         [-1.48242263e+00],\n",
       "         [ 1.56309241e-02],\n",
       "         [ 8.22298307e+00],\n",
       "         [ 1.38821962e+01],\n",
       "         [ 2.39787262e+00],\n",
       "         [ 1.04148591e+01],\n",
       "         [ 1.20393561e+01],\n",
       "         [ 9.82237692e-01],\n",
       "         [-2.96790303e-01],\n",
       "         [ 1.64558614e-02],\n",
       "         [ 9.53022430e+00],\n",
       "         [ 4.60975523e+00],\n",
       "         [-2.05337481e+01],\n",
       "         [-7.59672254e+00],\n",
       "         [-1.28781496e+00],\n",
       "         [ 8.11444703e+00],\n",
       "         [ 1.56248438e-02],\n",
       "         [ 0.00000000e+00],\n",
       "         [ 1.69174044e+01],\n",
       "         [ 2.10144055e+01],\n",
       "         [ 8.19188475e+00],\n",
       "         [ 3.28422648e+00],\n",
       "         [-7.92733822e+00],\n",
       "         [ 5.53796936e+00],\n",
       "         [ 0.00000000e+00],\n",
       "         [ 1.23256782e-02],\n",
       "         [ 1.22591388e+01],\n",
       "         [ 4.27703965e+01],\n",
       "         [ 3.00737290e+01],\n",
       "         [ 6.37126645e+00],\n",
       "         [-3.76195022e+00],\n",
       "         [-8.44211541e-02],\n",
       "         [-9.47333837e-02],\n",
       "         [ 1.87064154e-02],\n",
       "         [ 3.50830258e+00],\n",
       "         [ 3.80984986e+01],\n",
       "         [ 3.35310694e+01],\n",
       "         [ 2.02974082e+01],\n",
       "         [ 8.14519352e+00],\n",
       "         [ 3.40365716e+00],\n",
       "         [ 3.20937502e-01],\n",
       "         [ 5.87288113e-11],\n",
       "         [ 1.47391893e+00],\n",
       "         [ 1.21896207e+01],\n",
       "         [ 2.33266792e+01],\n",
       "         [ 1.34837090e+01],\n",
       "         [ 5.11868085e+00],\n",
       "         [ 6.85042327e+00],\n",
       "         [ 1.52852384e+00]]), 'db': 1.2566021066857371}, 368343.6499963927)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "#     X = (X - np.nanmean(X, axis=0)) / np.nanstd(X, axis=0)\n",
    "#     return X\n",
    "    A = sigmoid(np.dot(X, w) + b) #np.array(b, dtype=float))\n",
    "#     A = np.array\n",
    "\n",
    "    A_2 = A\n",
    "    for i in range(A_2.shape[0]): ## adjust value to avoid log0\n",
    "        if A_2[i][0] == 1:\n",
    "            A[i][0] = 0.999999999\n",
    "        elif A_2[i][0] == 0:\n",
    "            A[i][0] = 1e-10\n",
    "    \n",
    "#     return A.shape#[0][0]\n",
    "#     return A.shape\n",
    "#     return np.log(A)\n",
    "#     return np.dot(X, w) + b\n",
    "#     cost = (-1. / m) * (np.dot(Y.T, np.log(A)) + np.dot(1. - Y.T, np.log(1. - A)))\n",
    "    cost = (-1. / m) * np.sum(np.dot(np.log(A), Y.T) + np.dot(np.log(1. - A), 1. - Y.T))\n",
    "#     return (A - Y).shape\n",
    "    dw = (1 / m) * np.dot(X.T, np.subtract(A, Y))\n",
    "    db = (1 / m) * np.sum(np.subtract(A, Y))\n",
    "#     return dw.shape\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost\n",
    "\n",
    "## test function\n",
    "np.random.seed(1000) # set seed to maintain same random results\n",
    "\n",
    "n = X_train.shape[0]\n",
    "m = X_train.shape[1]\n",
    "\n",
    "init_parameters = initialize_parameters(m)\n",
    "init_w = init_parameters[0]\n",
    "init_b = init_parameters[1]\n",
    "# print(X_train.shape)\n",
    "# print(y_train.reshape(n,-1).shape)\n",
    "# print(init_w.shape)\n",
    "# print(init_b)\n",
    "propagate(init_w, init_b, X_train, y_train.reshape(n,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "## test function\n",
    "np.random.seed(1000) # set seed to maintain same random results\n",
    "\n",
    "n = X_train.shape[0]\n",
    "m = X_train.shape[1]\n",
    "\n",
    "init_parameters = initialize_parameters(m)\n",
    "init_w = init_parameters[0]\n",
    "init_b = init_parameters[1]\n",
    "last_param = optimize(init_w, init_b, X_train, y_train.reshape(n, -1), 10000, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'w': array([[-0.8044583 ],\n",
      "       [ 0.58969986],\n",
      "       [ 0.01610277],\n",
      "       [-0.00307443],\n",
      "       [ 0.1621523 ],\n",
      "       [ 0.25901514],\n",
      "       [ 0.48034742],\n",
      "       [-0.53765849],\n",
      "       [ 1.06289233],\n",
      "       [-0.36072765],\n",
      "       [ 0.16550428],\n",
      "       [ 0.15090017],\n",
      "       [-0.0434767 ],\n",
      "       [-0.13859495],\n",
      "       [-0.13504183],\n",
      "       [ 0.42322342],\n",
      "       [-0.31224882],\n",
      "       [ 0.14200552],\n",
      "       [ 0.39764745],\n",
      "       [-0.11068416],\n",
      "       [-0.53039696],\n",
      "       [-0.18674716],\n",
      "       [-0.10369256],\n",
      "       [-0.23166795],\n",
      "       [ 0.13900929],\n",
      "       [-0.41429934],\n",
      "       [ 0.07605646],\n",
      "       [ 0.3999035 ],\n",
      "       [ 0.16229895],\n",
      "       [ 0.37883345],\n",
      "       [-0.35309989],\n",
      "       [-0.10711935],\n",
      "       [-0.32692468],\n",
      "       [-0.59863348],\n",
      "       [-0.06144516],\n",
      "       [ 0.37792472],\n",
      "       [-0.20530803],\n",
      "       [-0.06437969],\n",
      "       [ 0.13340462],\n",
      "       [ 0.86726082],\n",
      "       [-1.04557125],\n",
      "       [-0.13066489],\n",
      "       [ 0.22693992],\n",
      "       [-0.1054221 ],\n",
      "       [ 0.26138114],\n",
      "       [ 0.12517333],\n",
      "       [ 0.09063508],\n",
      "       [-0.1060376 ],\n",
      "       [-1.11165912],\n",
      "       [-0.00773019],\n",
      "       [ 0.05806421],\n",
      "       [-0.18090737],\n",
      "       [-0.66547607],\n",
      "       [-0.04340754],\n",
      "       [ 0.24664663],\n",
      "       [ 0.02264008],\n",
      "       [-0.83534588],\n",
      "       [-0.88832269],\n",
      "       [ 0.09227077],\n",
      "       [-0.12629085],\n",
      "       [-0.38590213],\n",
      "       [-0.14198242],\n",
      "       [-0.21048917],\n",
      "       [-0.36809622]]), 'b': 0.8220783461988392}, {'dw': array([[ 0.00000000e+00],\n",
      "       [-3.92331650e-02],\n",
      "       [-3.30978489e-01],\n",
      "       [-5.64519111e-01],\n",
      "       [-6.89877344e-01],\n",
      "       [ 5.05299769e-01],\n",
      "       [ 4.22688963e-01],\n",
      "       [ 9.35736534e-02],\n",
      "       [-5.14058045e-02],\n",
      "       [-2.45113802e-01],\n",
      "       [ 2.45366149e-01],\n",
      "       [-1.00286957e+00],\n",
      "       [-1.64588642e+00],\n",
      "       [-3.28686763e-01],\n",
      "       [ 2.30817761e-01],\n",
      "       [ 1.47724458e-02],\n",
      "       [ 1.69223316e-02],\n",
      "       [-1.07245105e-01],\n",
      "       [ 3.60399387e-01],\n",
      "       [-1.35318700e+00],\n",
      "       [-1.84466902e+00],\n",
      "       [-7.04103359e-01],\n",
      "       [-2.61041313e-01],\n",
      "       [ 7.41240928e-02],\n",
      "       [ 1.63864239e-02],\n",
      "       [-3.44421891e-01],\n",
      "       [ 5.86867561e-01],\n",
      "       [ 4.92234424e-01],\n",
      "       [-1.53574064e-01],\n",
      "       [-2.57419614e-01],\n",
      "       [-5.40573870e-01],\n",
      "       [ 1.35052018e-02],\n",
      "       [ 0.00000000e+00],\n",
      "       [-7.75731458e-01],\n",
      "       [ 7.52822385e-02],\n",
      "       [ 8.70626268e-01],\n",
      "       [ 3.20319371e-01],\n",
      "       [-1.61163238e-01],\n",
      "       [-4.22283954e-01],\n",
      "       [ 0.00000000e+00],\n",
      "       [ 3.44778062e-06],\n",
      "       [-5.81530016e-01],\n",
      "       [-9.79298399e-01],\n",
      "       [-5.33568524e-01],\n",
      "       [-8.15407486e-01],\n",
      "       [-6.49318302e-01],\n",
      "       [ 1.13922819e-01],\n",
      "       [-1.04599558e-03],\n",
      "       [ 1.78973297e-08],\n",
      "       [-2.07511096e-01],\n",
      "       [-6.12416598e-01],\n",
      "       [-6.61431358e-01],\n",
      "       [-2.41800576e+00],\n",
      "       [-1.19464295e+00],\n",
      "       [ 3.71757113e-01],\n",
      "       [-7.79872549e-02],\n",
      "       [ 5.55024190e-09],\n",
      "       [-5.97689705e-02],\n",
      "       [-4.02304008e-01],\n",
      "       [-6.88532953e-01],\n",
      "       [-1.37293962e+00],\n",
      "       [-1.04883228e-01],\n",
      "       [ 1.80586191e-01],\n",
      "       [-3.53660654e-02]]), 'db': -0.05160367101273991}, [368343.6499963927, 272073.0661693379, 254279.36345171373, 231138.4829797353, 210140.94337324373, 189544.40639760657, 171288.64492139485, 156517.2217949822, 142790.3012074596, 129991.96195077668, 118329.26914353122, 107848.66119287252, 114021.30696010584, 114120.79111021348, 126958.80630256649, 107835.19969733257, 138826.00395626153, 108788.30856832444, 107357.69575727051, 112729.12881190992, 104045.96364535869, 145410.9772537299, 109196.61485059076, 102004.57465203856, 127574.93675529731, 107927.3279314039, 101277.07540351186, 125552.0099994744, 107830.93498044416, 101177.7085698349, 131511.4934904728, 108466.96789177791, 101576.63371747395, 148244.42198695018, 109666.36844484035, 102418.45824888366, 151291.89028699545, 111389.62697657992, 103650.88618113054, 108006.5864130258, 113745.11136512343, 105249.07792437283, 100212.73418126735, 118920.60990548, 107254.67109358124, 100865.25665234575, 145769.06593525843, 109740.70189160986, 102617.06643039973, 125036.73534930318, 112865.4015493401, 104833.90280513876, 100097.7040969697, 119253.32437737955, 107536.00153926219, 101134.79003305294, 157129.0049178539, 110850.7694300575, 103527.8072806113, 103055.5459004356, 115324.64897573885, 106463.20176431438, 100481.48428680586, 144886.10919721506, 110045.0895378119, 103028.30746649696, 106614.33808542689, 114688.5172259563, 106220.26486978124, 100374.71465969984, 144057.0475836412, 110123.85704721349, 103151.76566597709, 104346.78091022576, 115304.59251155987, 106647.90918523625, 100690.71608662017, 154194.9401092248, 110940.69999265565, 103800.2442375136, 100760.81352434572, 117618.5276257493, 107641.48584456866, 101429.0211217528, 156708.2955352624, 112410.54819646165, 104902.90422484907, 99865.68877280905, 127339.58197674822, 109166.62678755449, 102585.70199229289, 110989.06483200051, 114662.29666237842, 106458.5005011058, 100659.6190654628, 154989.7177781309, 111246.4721684391, 104144.273454836, 100112.34952178862, 120558.92685465039])\n"
     ]
    }
   ],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[1],1)\n",
    "    \n",
    "    A = sigmoid(np.dot(X, w) + b)\n",
    "    \n",
    "    for i in range(A.shape[0]):\n",
    "        if A[i][0] > 0.5:\n",
    "            Y_prediction[0][i] = 1\n",
    "        else:\n",
    "            Y_prediction[0][i] = 0\n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "# print(len(last_param))\n",
    "print(last_param)\n",
    "trained_weights = last_param[0]['w']\n",
    "trained_b = last_param[0]['b']\n",
    "# print(X_test.shape)\n",
    "testset_predicted_value = predict(trained_weights, trained_b, X_test)\n",
    "# testset_predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testset_accuracy(predicted_value, actual_value):\n",
    "    m = actual_value.shape[0]\n",
    "    predicted_value_2 = predicted_value\n",
    "    for i in range(predicted_value_2.shape[0]): ## adjust value to avoid log0\n",
    "        if predicted_value_2[i][0] == 1:\n",
    "            predicted_value[i][0] = 0.999999999\n",
    "        elif predicted_value_2[i][0] == 0:\n",
    "            predicted_value[i][0] = 1e-10\n",
    "    cost = (-1. / m) * np.sum(np.dot(np.log(predicted_value), actual_value.T) + np.dot(np.log(1. - predicted_value), 1. - actual_value.T))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction_accuracy(predicted_value, actual_value):\n",
    "    assert(predicted_value.shape[0] == actual_value.shape[0])\n",
    "    \n",
    "    total_record_number = actual_value.shape[0]\n",
    "    correct_number = 0\n",
    "    for i in range(total_record_number):\n",
    "        if actual_value[i][0] == predicted_value[i][0]:\n",
    "            correct_number += 1\n",
    "            \n",
    "    return correct_number / total_record_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "\n",
    "    init_parameters = initialize_parameters(m)\n",
    "    init_w = init_parameters[0]\n",
    "    init_b = init_parameters[1]\n",
    "    last_param = optimize(init_w, init_b, X_train, y_train.reshape(n, -1), num_iterations, learning_rate)\n",
    "    \n",
    "    trained_weights = last_param[0]['w']\n",
    "    trained_b = last_param[0]['b']\n",
    "    training_predicted_value = predict(trained_weights, trained_b, X_train).T\n",
    "#     return training_predicted_value.shape\n",
    "#     return y_train.reshape(-1,1).shape\n",
    "    training_accuracy = compute_prediction_accuracy(training_predicted_value, y_train.reshape(-1,1))\n",
    "    # print(X_test.shape)\n",
    "    testset_predicted_value = predict(trained_weights, trained_b, X_test).T\n",
    "    test_accuracy = compute_prediction_accuracy(testset_predicted_value, y_test.reshape(-1,1))\n",
    "    \n",
    "    cost = last_param[2]#[-1] # the last training cost\n",
    "    \n",
    "    if print_cost:\n",
    "        d = {\"w\": trained_weights, \"b\": trained_b, \"training_accuracy\": training_accuracy,\"test_accuracy\":test_accuracy,\"cost\": cost}\n",
    "    else:\n",
    "        d = {\"w\": trained_weights, \"b\": trained_b, \"training_accuracy\": training_accuracy,\"test_accuracy\":test_accuracy}\n",
    "\n",
    "    \n",
    "    return d\n",
    "\n",
    "n_iterations = 1000\n",
    "learning_rate = 1e-3\n",
    "result_1000 = model(X_train, y_train.reshape(-1, 1), X_test, y_test.reshape(-1, 1), n_iterations, learning_rate, print_cost = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': array([[ 0.42132889],\n",
       "        [-1.28081478],\n",
       "        [ 0.77113898],\n",
       "        [ 0.00351706],\n",
       "        [ 0.22872438],\n",
       "        [ 0.28325745],\n",
       "        [ 0.22697886],\n",
       "        [-0.9656317 ],\n",
       "        [-0.12077264],\n",
       "        [ 0.29642998],\n",
       "        [ 0.13468248],\n",
       "        [ 0.24506098],\n",
       "        [-0.06992258],\n",
       "        [-0.18512856],\n",
       "        [ 0.35655071],\n",
       "        [ 0.02120948],\n",
       "        [ 0.50294182],\n",
       "        [-0.33418629],\n",
       "        [ 0.51193183],\n",
       "        [-0.1321378 ],\n",
       "        [-0.48802661],\n",
       "        [-0.11495714],\n",
       "        [-0.41353363],\n",
       "        [ 2.0942584 ],\n",
       "        [ 0.98829657],\n",
       "        [-0.04897782],\n",
       "        [ 0.04145517],\n",
       "        [ 0.39655676],\n",
       "        [ 0.07923262],\n",
       "        [ 0.39791471],\n",
       "        [-0.24137106],\n",
       "        [ 1.17774853],\n",
       "        [ 2.57702967],\n",
       "        [-0.83823174],\n",
       "        [-0.0983902 ],\n",
       "        [ 0.50190449],\n",
       "        [-0.32613033],\n",
       "        [ 0.07047939],\n",
       "        [-0.37254127],\n",
       "        [-0.35557729],\n",
       "        [-0.43938129],\n",
       "        [ 0.32439332],\n",
       "        [ 0.19332788],\n",
       "        [-0.19001827],\n",
       "        [ 0.36730289],\n",
       "        [-0.03610295],\n",
       "        [ 0.60824628],\n",
       "        [-0.22626651],\n",
       "        [ 0.40482124],\n",
       "        [-0.28773045],\n",
       "        [ 0.08957373],\n",
       "        [-0.08029367],\n",
       "        [-0.83377841],\n",
       "        [ 0.07108349],\n",
       "        [-0.15439862],\n",
       "        [ 0.30848971],\n",
       "        [-0.11065138],\n",
       "        [-0.18482974],\n",
       "        [-0.59733092],\n",
       "        [-0.27608518],\n",
       "        [-0.43913967],\n",
       "        [-0.18460967],\n",
       "        [-0.1473816 ],\n",
       "        [ 0.34200721]]),\n",
       " 'b': 0.6114304762046254,\n",
       " 'training_accuracy': 0.8819599109131403,\n",
       " 'test_accuracy': 0.8733333333333333,\n",
       " 'cost': [610053.1836537265,\n",
       "  185753.07654194816,\n",
       "  184178.58263272658,\n",
       "  168560.78312238143,\n",
       "  152088.63097432134,\n",
       "  137394.47752246453,\n",
       "  125097.59971796344,\n",
       "  114712.98285986863,\n",
       "  105797.11932589785,\n",
       "  143178.89111080076]}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11f647cc0>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_axis = range(1, 1001, 100)\n",
    "Y_axis = result_1000['cost']\n",
    "plt.plot(X_axis, Y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 28 20:52:43 2019\n",
      "1000\n",
      "Mon Oct 28 20:52:56 2019\n",
      "2000\n",
      "Mon Oct 28 20:53:21 2019\n",
      "3000\n",
      "Mon Oct 28 20:54:00 2019\n",
      "4000\n",
      "Mon Oct 28 20:54:51 2019\n",
      "5000\n",
      "Mon Oct 28 20:55:53 2019\n",
      "6000\n",
      "Mon Oct 28 20:57:10 2019\n",
      "7000\n",
      "Mon Oct 28 20:58:43 2019\n",
      "8000\n",
      "Mon Oct 28 21:00:27 2019\n",
      "9000\n",
      "Mon Oct 28 21:02:24 2019\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "result_set = defaultdict(list)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "for i in range(1000, 10001, 1000): # iterating 1000, 2000, ..., 10000\n",
    "    n_iterations = i\n",
    "    print(time.ctime())\n",
    "    print(n_iterations)\n",
    "    result = model(X_train, y_train.reshape(-1, 1), X_test, y_test.reshape(-1, 1), n_iterations, learning_rate, print_cost = True)\n",
    "    result_set[i] = result\n",
    "\n",
    "result_set = dict(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11fed9240>]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_axis = list(result_set.keys())\n",
    "Y_axis = [i['test_accuracy'] for i in result_set.values()]\n",
    "# print(X_axis)\n",
    "# print(Y_axis)\n",
    "## interation_num vs test_accuracy\n",
    "plt.plot(X_axis, Y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digits (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
